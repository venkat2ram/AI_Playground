{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular_Playground_Series_Aug_2021_practice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPyMueEAAiBWJeZu0bMITZ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkat2ram/AI_Playground/blob/main/Tabular_Playground_Series_Aug_2021_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rl7qXxUfKnE"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_HzVy6DfZN6"
      },
      "source": [
        "dataset = pd.read_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\train.csv')\n",
        "pred_dataset=pd.read_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\test.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQrIpGD3fnSY"
      },
      "source": [
        "dataset.drop('id',inplace=True,axis=1)\n",
        "pred_dataset.drop('id',inplace=True,axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "ELmm9x_C5Hqu",
        "outputId": "867f9681-650d-4ce7-9047-76dfe6fb1b3a"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f0   f1        f2        f3          f4        f5        f6  \\\n",
              "0 -0.002350   59  0.766739 -1.350460     42.2727  16.68570   30.3599   \n",
              "1  0.784462  145 -0.463845 -0.530421  27324.9000   3.47545  160.4980   \n",
              "2  0.317816   19 -0.432571 -0.382644   1383.2600  19.71290   31.1026   \n",
              "3  0.210753   17 -0.616454  0.946362   -119.2530   4.08235  185.2570   \n",
              "4  0.439671   20  0.968126 -0.092546     74.3020  12.30650   72.1860   \n",
              "\n",
              "         f7         f8       f9  ...        f91        f92      f93       f94  \\\n",
              "0  1.267300   0.392007  1.09101  ...  -42.43990  26.854000  1.45751  0.696161   \n",
              "1  0.828007   3.735860  1.28138  ... -184.13200   7.901370  1.70644 -0.494699   \n",
              "2 -0.515354  34.430800  1.24210  ...    7.43721  37.218100  3.25339  0.337934   \n",
              "3  1.383310 -47.521400  1.09130  ...    9.66778   0.626942  1.49425  0.517513   \n",
              "4 -0.233964  24.399100  1.10151  ...  290.65700  15.604300  1.73557 -0.476668   \n",
              "\n",
              "         f95       f96       f97      f98       f99  loss  \n",
              "0   0.941764  1.828470  0.924090  2.29658  10.48980    15  \n",
              "1  -2.058300  0.819184  0.439152  2.36470   1.14383     3  \n",
              "2   0.615037  2.216760  0.745268  1.69679  12.30550     6  \n",
              "3 -10.222100  2.627310  0.617270  1.45645  10.02880     2  \n",
              "4   1.390190  2.195740  0.826987  1.78485   7.07197     1  \n",
              "\n",
              "[5 rows x 101 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.002350</td>\n",
              "      <td>59</td>\n",
              "      <td>0.766739</td>\n",
              "      <td>-1.350460</td>\n",
              "      <td>42.2727</td>\n",
              "      <td>16.68570</td>\n",
              "      <td>30.3599</td>\n",
              "      <td>1.267300</td>\n",
              "      <td>0.392007</td>\n",
              "      <td>1.09101</td>\n",
              "      <td>...</td>\n",
              "      <td>-42.43990</td>\n",
              "      <td>26.854000</td>\n",
              "      <td>1.45751</td>\n",
              "      <td>0.696161</td>\n",
              "      <td>0.941764</td>\n",
              "      <td>1.828470</td>\n",
              "      <td>0.924090</td>\n",
              "      <td>2.29658</td>\n",
              "      <td>10.48980</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.784462</td>\n",
              "      <td>145</td>\n",
              "      <td>-0.463845</td>\n",
              "      <td>-0.530421</td>\n",
              "      <td>27324.9000</td>\n",
              "      <td>3.47545</td>\n",
              "      <td>160.4980</td>\n",
              "      <td>0.828007</td>\n",
              "      <td>3.735860</td>\n",
              "      <td>1.28138</td>\n",
              "      <td>...</td>\n",
              "      <td>-184.13200</td>\n",
              "      <td>7.901370</td>\n",
              "      <td>1.70644</td>\n",
              "      <td>-0.494699</td>\n",
              "      <td>-2.058300</td>\n",
              "      <td>0.819184</td>\n",
              "      <td>0.439152</td>\n",
              "      <td>2.36470</td>\n",
              "      <td>1.14383</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.317816</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.432571</td>\n",
              "      <td>-0.382644</td>\n",
              "      <td>1383.2600</td>\n",
              "      <td>19.71290</td>\n",
              "      <td>31.1026</td>\n",
              "      <td>-0.515354</td>\n",
              "      <td>34.430800</td>\n",
              "      <td>1.24210</td>\n",
              "      <td>...</td>\n",
              "      <td>7.43721</td>\n",
              "      <td>37.218100</td>\n",
              "      <td>3.25339</td>\n",
              "      <td>0.337934</td>\n",
              "      <td>0.615037</td>\n",
              "      <td>2.216760</td>\n",
              "      <td>0.745268</td>\n",
              "      <td>1.69679</td>\n",
              "      <td>12.30550</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.210753</td>\n",
              "      <td>17</td>\n",
              "      <td>-0.616454</td>\n",
              "      <td>0.946362</td>\n",
              "      <td>-119.2530</td>\n",
              "      <td>4.08235</td>\n",
              "      <td>185.2570</td>\n",
              "      <td>1.383310</td>\n",
              "      <td>-47.521400</td>\n",
              "      <td>1.09130</td>\n",
              "      <td>...</td>\n",
              "      <td>9.66778</td>\n",
              "      <td>0.626942</td>\n",
              "      <td>1.49425</td>\n",
              "      <td>0.517513</td>\n",
              "      <td>-10.222100</td>\n",
              "      <td>2.627310</td>\n",
              "      <td>0.617270</td>\n",
              "      <td>1.45645</td>\n",
              "      <td>10.02880</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.439671</td>\n",
              "      <td>20</td>\n",
              "      <td>0.968126</td>\n",
              "      <td>-0.092546</td>\n",
              "      <td>74.3020</td>\n",
              "      <td>12.30650</td>\n",
              "      <td>72.1860</td>\n",
              "      <td>-0.233964</td>\n",
              "      <td>24.399100</td>\n",
              "      <td>1.10151</td>\n",
              "      <td>...</td>\n",
              "      <td>290.65700</td>\n",
              "      <td>15.604300</td>\n",
              "      <td>1.73557</td>\n",
              "      <td>-0.476668</td>\n",
              "      <td>1.390190</td>\n",
              "      <td>2.195740</td>\n",
              "      <td>0.826987</td>\n",
              "      <td>1.78485</td>\n",
              "      <td>7.07197</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "3m4HsYgY5wHu",
        "outputId": "86ccd5b5-4681-475f-9f9c-679b8e53976a"
      },
      "source": [
        "pred_dataset.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f0   f1        f2        f3          f4        f5         f6  \\\n",
              "0  0.812665   15 -1.239120 -0.893251    295.5770  15.87120   23.04360   \n",
              "1  0.190344  131 -0.501361  0.801921     64.8866   3.09703  344.80500   \n",
              "2  0.919671   19 -0.057382  0.901419  11961.2000  16.39650  273.24000   \n",
              "3  0.860985   19 -0.549509  0.471799   7501.6000   2.80698   71.08170   \n",
              "4  0.313229   89  0.588509  0.167705   2931.2600   4.34986    1.57187   \n",
              "\n",
              "         f7         f8       f9  ...       f90        f91       f92      f93  \\\n",
              "0  0.942256  29.898000  1.11394  ...  0.446389   -422.332  -1.44630  1.69075   \n",
              "1  0.807194  38.421900  1.09695  ...  0.377179  10352.200  21.06270  1.84351   \n",
              "2 -0.003300  37.940000  1.15222  ...  0.990140   3224.020  -2.25287  1.55100   \n",
              "3  0.792136   0.395235  1.20157  ...  1.396880   9689.760  14.77150  1.41390   \n",
              "4  1.118300   7.754630  1.16807  ...  0.862502   2693.350  44.18050  1.58020   \n",
              "\n",
              "        f94        f95      f96       f97       f98       f99  \n",
              "0  1.059300  -3.010570  1.94664  0.529470  1.386950   8.78767  \n",
              "1  0.251895   4.440570  1.90309  0.248534  0.863881  11.79390  \n",
              "2 -0.559157  17.838600  1.83385  0.931796  2.336870   9.05400  \n",
              "3  0.329272   0.802437  2.23251  0.893348  1.359470   4.84833  \n",
              "4 -0.191021  26.253000  2.68238  0.361923  1.532800   3.70660  \n",
              "\n",
              "[5 rows x 100 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.812665</td>\n",
              "      <td>15</td>\n",
              "      <td>-1.239120</td>\n",
              "      <td>-0.893251</td>\n",
              "      <td>295.5770</td>\n",
              "      <td>15.87120</td>\n",
              "      <td>23.04360</td>\n",
              "      <td>0.942256</td>\n",
              "      <td>29.898000</td>\n",
              "      <td>1.11394</td>\n",
              "      <td>...</td>\n",
              "      <td>0.446389</td>\n",
              "      <td>-422.332</td>\n",
              "      <td>-1.44630</td>\n",
              "      <td>1.69075</td>\n",
              "      <td>1.059300</td>\n",
              "      <td>-3.010570</td>\n",
              "      <td>1.94664</td>\n",
              "      <td>0.529470</td>\n",
              "      <td>1.386950</td>\n",
              "      <td>8.78767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.190344</td>\n",
              "      <td>131</td>\n",
              "      <td>-0.501361</td>\n",
              "      <td>0.801921</td>\n",
              "      <td>64.8866</td>\n",
              "      <td>3.09703</td>\n",
              "      <td>344.80500</td>\n",
              "      <td>0.807194</td>\n",
              "      <td>38.421900</td>\n",
              "      <td>1.09695</td>\n",
              "      <td>...</td>\n",
              "      <td>0.377179</td>\n",
              "      <td>10352.200</td>\n",
              "      <td>21.06270</td>\n",
              "      <td>1.84351</td>\n",
              "      <td>0.251895</td>\n",
              "      <td>4.440570</td>\n",
              "      <td>1.90309</td>\n",
              "      <td>0.248534</td>\n",
              "      <td>0.863881</td>\n",
              "      <td>11.79390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.919671</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.057382</td>\n",
              "      <td>0.901419</td>\n",
              "      <td>11961.2000</td>\n",
              "      <td>16.39650</td>\n",
              "      <td>273.24000</td>\n",
              "      <td>-0.003300</td>\n",
              "      <td>37.940000</td>\n",
              "      <td>1.15222</td>\n",
              "      <td>...</td>\n",
              "      <td>0.990140</td>\n",
              "      <td>3224.020</td>\n",
              "      <td>-2.25287</td>\n",
              "      <td>1.55100</td>\n",
              "      <td>-0.559157</td>\n",
              "      <td>17.838600</td>\n",
              "      <td>1.83385</td>\n",
              "      <td>0.931796</td>\n",
              "      <td>2.336870</td>\n",
              "      <td>9.05400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.860985</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.549509</td>\n",
              "      <td>0.471799</td>\n",
              "      <td>7501.6000</td>\n",
              "      <td>2.80698</td>\n",
              "      <td>71.08170</td>\n",
              "      <td>0.792136</td>\n",
              "      <td>0.395235</td>\n",
              "      <td>1.20157</td>\n",
              "      <td>...</td>\n",
              "      <td>1.396880</td>\n",
              "      <td>9689.760</td>\n",
              "      <td>14.77150</td>\n",
              "      <td>1.41390</td>\n",
              "      <td>0.329272</td>\n",
              "      <td>0.802437</td>\n",
              "      <td>2.23251</td>\n",
              "      <td>0.893348</td>\n",
              "      <td>1.359470</td>\n",
              "      <td>4.84833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.313229</td>\n",
              "      <td>89</td>\n",
              "      <td>0.588509</td>\n",
              "      <td>0.167705</td>\n",
              "      <td>2931.2600</td>\n",
              "      <td>4.34986</td>\n",
              "      <td>1.57187</td>\n",
              "      <td>1.118300</td>\n",
              "      <td>7.754630</td>\n",
              "      <td>1.16807</td>\n",
              "      <td>...</td>\n",
              "      <td>0.862502</td>\n",
              "      <td>2693.350</td>\n",
              "      <td>44.18050</td>\n",
              "      <td>1.58020</td>\n",
              "      <td>-0.191021</td>\n",
              "      <td>26.253000</td>\n",
              "      <td>2.68238</td>\n",
              "      <td>0.361923</td>\n",
              "      <td>1.532800</td>\n",
              "      <td>3.70660</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbsWNwhW5Ijy"
      },
      "source": [
        "dependent_columns=pred_dataset.columns"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5lhOMEA6ncx"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler((0,1))\n",
        "scaler.fit(dataset[dependent_columns])\n",
        "dataset[dependent_columns]=scaler.transform(dataset[dependent_columns])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "GVS0ZJGm72_7",
        "outputId": "693e7a92-b24e-455b-805f-ee831473efe6"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              f0        f1        f2        f3        f4        f5        f6  \\\n",
              "0       0.058636  0.262069  0.490389  0.039642  0.167960  0.520648  0.072185   \n",
              "1       0.748009  0.558621  0.420724  0.299497  0.768412  0.176353  0.203889   \n",
              "2       0.339152  0.124138  0.422494  0.346325  0.197473  0.599544  0.072937   \n",
              "3       0.245348  0.117241  0.412084  0.767463  0.164405  0.192171  0.228946   \n",
              "4       0.445917  0.127586  0.501790  0.438252  0.168665  0.406514  0.114515   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "249995  0.870249  0.079310  0.484528  0.450112  0.167175  0.118774  0.082395   \n",
              "249996  0.274089  0.082759  0.415447  0.890131  0.165821  0.544036  0.256987   \n",
              "249997  0.101018  0.241379  0.473186  0.690674  0.212432  0.465809  0.053025   \n",
              "249998  0.916993  0.100000  0.390208  0.650222  0.271376  0.375093  0.045322   \n",
              "249999  0.274682  0.234483  0.488270  0.244940  0.171375  0.581590  0.084303   \n",
              "\n",
              "              f7        f8        f9  ...       f91       f92       f93  \\\n",
              "0       0.663569  0.519427  0.201658  ...  0.188769  0.368139  0.261862   \n",
              "1       0.609690  0.522879  0.446219  ...  0.186655  0.142436  0.308433   \n",
              "2       0.444931  0.554563  0.395758  ...  0.189513  0.491562  0.597841   \n",
              "3       0.677797  0.469969  0.202030  ...  0.189547  0.055806  0.268735   \n",
              "4       0.479443  0.544208  0.215147  ...  0.193739  0.234168  0.313882   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "249995  0.612679  0.545368  0.242677  ...  0.241886  0.269060  0.254725   \n",
              "249996  0.360378  0.386499  0.333118  ...  0.218431  0.280349  0.288654   \n",
              "249997  0.445611  0.602446  0.346594  ...  0.188978  0.179434  0.290540   \n",
              "249998  0.583712  0.641758  0.732520  ...  0.185292  0.310163  0.580464   \n",
              "249999  0.565191  0.399904  0.260418  ...  0.269773  0.507717  0.666021   \n",
              "\n",
              "             f94       f95       f96       f97       f98       f99  loss  \n",
              "0       0.683482  0.379244  0.441330  0.835195  0.572721  0.325062    15  \n",
              "1       0.381462  0.334849  0.290870  0.394403  0.585975  0.056975     3  \n",
              "2       0.592630  0.374409  0.499214  0.672652  0.456020  0.377146     6  \n",
              "3       0.638174  0.214041  0.560417  0.556307  0.409258  0.311839     2  \n",
              "4       0.386035  0.385880  0.496081  0.746932  0.473154  0.227023     1  \n",
              "...          ...       ...       ...       ...       ...       ...   ...  \n",
              "249995  0.660997  0.438570  0.594125  0.869547  0.333680  0.065403    11  \n",
              "249996  0.467646  0.449557  0.466087  0.686884  0.403699  0.059352     5  \n",
              "249997  0.417925  0.779458  0.319717  0.750155  0.321512  0.037225     1  \n",
              "249998  0.702730  0.403668  0.755847  0.629169  0.313121  0.236888    10  \n",
              "249999  0.368713  0.350692  0.447806  0.406551  0.307880  0.853506     7  \n",
              "\n",
              "[250000 rows x 101 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.058636</td>\n",
              "      <td>0.262069</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.039642</td>\n",
              "      <td>0.167960</td>\n",
              "      <td>0.520648</td>\n",
              "      <td>0.072185</td>\n",
              "      <td>0.663569</td>\n",
              "      <td>0.519427</td>\n",
              "      <td>0.201658</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188769</td>\n",
              "      <td>0.368139</td>\n",
              "      <td>0.261862</td>\n",
              "      <td>0.683482</td>\n",
              "      <td>0.379244</td>\n",
              "      <td>0.441330</td>\n",
              "      <td>0.835195</td>\n",
              "      <td>0.572721</td>\n",
              "      <td>0.325062</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.748009</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.420724</td>\n",
              "      <td>0.299497</td>\n",
              "      <td>0.768412</td>\n",
              "      <td>0.176353</td>\n",
              "      <td>0.203889</td>\n",
              "      <td>0.609690</td>\n",
              "      <td>0.522879</td>\n",
              "      <td>0.446219</td>\n",
              "      <td>...</td>\n",
              "      <td>0.186655</td>\n",
              "      <td>0.142436</td>\n",
              "      <td>0.308433</td>\n",
              "      <td>0.381462</td>\n",
              "      <td>0.334849</td>\n",
              "      <td>0.290870</td>\n",
              "      <td>0.394403</td>\n",
              "      <td>0.585975</td>\n",
              "      <td>0.056975</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.339152</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.422494</td>\n",
              "      <td>0.346325</td>\n",
              "      <td>0.197473</td>\n",
              "      <td>0.599544</td>\n",
              "      <td>0.072937</td>\n",
              "      <td>0.444931</td>\n",
              "      <td>0.554563</td>\n",
              "      <td>0.395758</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189513</td>\n",
              "      <td>0.491562</td>\n",
              "      <td>0.597841</td>\n",
              "      <td>0.592630</td>\n",
              "      <td>0.374409</td>\n",
              "      <td>0.499214</td>\n",
              "      <td>0.672652</td>\n",
              "      <td>0.456020</td>\n",
              "      <td>0.377146</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.245348</td>\n",
              "      <td>0.117241</td>\n",
              "      <td>0.412084</td>\n",
              "      <td>0.767463</td>\n",
              "      <td>0.164405</td>\n",
              "      <td>0.192171</td>\n",
              "      <td>0.228946</td>\n",
              "      <td>0.677797</td>\n",
              "      <td>0.469969</td>\n",
              "      <td>0.202030</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189547</td>\n",
              "      <td>0.055806</td>\n",
              "      <td>0.268735</td>\n",
              "      <td>0.638174</td>\n",
              "      <td>0.214041</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.556307</td>\n",
              "      <td>0.409258</td>\n",
              "      <td>0.311839</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.445917</td>\n",
              "      <td>0.127586</td>\n",
              "      <td>0.501790</td>\n",
              "      <td>0.438252</td>\n",
              "      <td>0.168665</td>\n",
              "      <td>0.406514</td>\n",
              "      <td>0.114515</td>\n",
              "      <td>0.479443</td>\n",
              "      <td>0.544208</td>\n",
              "      <td>0.215147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.193739</td>\n",
              "      <td>0.234168</td>\n",
              "      <td>0.313882</td>\n",
              "      <td>0.386035</td>\n",
              "      <td>0.385880</td>\n",
              "      <td>0.496081</td>\n",
              "      <td>0.746932</td>\n",
              "      <td>0.473154</td>\n",
              "      <td>0.227023</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249995</th>\n",
              "      <td>0.870249</td>\n",
              "      <td>0.079310</td>\n",
              "      <td>0.484528</td>\n",
              "      <td>0.450112</td>\n",
              "      <td>0.167175</td>\n",
              "      <td>0.118774</td>\n",
              "      <td>0.082395</td>\n",
              "      <td>0.612679</td>\n",
              "      <td>0.545368</td>\n",
              "      <td>0.242677</td>\n",
              "      <td>...</td>\n",
              "      <td>0.241886</td>\n",
              "      <td>0.269060</td>\n",
              "      <td>0.254725</td>\n",
              "      <td>0.660997</td>\n",
              "      <td>0.438570</td>\n",
              "      <td>0.594125</td>\n",
              "      <td>0.869547</td>\n",
              "      <td>0.333680</td>\n",
              "      <td>0.065403</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249996</th>\n",
              "      <td>0.274089</td>\n",
              "      <td>0.082759</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.890131</td>\n",
              "      <td>0.165821</td>\n",
              "      <td>0.544036</td>\n",
              "      <td>0.256987</td>\n",
              "      <td>0.360378</td>\n",
              "      <td>0.386499</td>\n",
              "      <td>0.333118</td>\n",
              "      <td>...</td>\n",
              "      <td>0.218431</td>\n",
              "      <td>0.280349</td>\n",
              "      <td>0.288654</td>\n",
              "      <td>0.467646</td>\n",
              "      <td>0.449557</td>\n",
              "      <td>0.466087</td>\n",
              "      <td>0.686884</td>\n",
              "      <td>0.403699</td>\n",
              "      <td>0.059352</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249997</th>\n",
              "      <td>0.101018</td>\n",
              "      <td>0.241379</td>\n",
              "      <td>0.473186</td>\n",
              "      <td>0.690674</td>\n",
              "      <td>0.212432</td>\n",
              "      <td>0.465809</td>\n",
              "      <td>0.053025</td>\n",
              "      <td>0.445611</td>\n",
              "      <td>0.602446</td>\n",
              "      <td>0.346594</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188978</td>\n",
              "      <td>0.179434</td>\n",
              "      <td>0.290540</td>\n",
              "      <td>0.417925</td>\n",
              "      <td>0.779458</td>\n",
              "      <td>0.319717</td>\n",
              "      <td>0.750155</td>\n",
              "      <td>0.321512</td>\n",
              "      <td>0.037225</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249998</th>\n",
              "      <td>0.916993</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.390208</td>\n",
              "      <td>0.650222</td>\n",
              "      <td>0.271376</td>\n",
              "      <td>0.375093</td>\n",
              "      <td>0.045322</td>\n",
              "      <td>0.583712</td>\n",
              "      <td>0.641758</td>\n",
              "      <td>0.732520</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185292</td>\n",
              "      <td>0.310163</td>\n",
              "      <td>0.580464</td>\n",
              "      <td>0.702730</td>\n",
              "      <td>0.403668</td>\n",
              "      <td>0.755847</td>\n",
              "      <td>0.629169</td>\n",
              "      <td>0.313121</td>\n",
              "      <td>0.236888</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249999</th>\n",
              "      <td>0.274682</td>\n",
              "      <td>0.234483</td>\n",
              "      <td>0.488270</td>\n",
              "      <td>0.244940</td>\n",
              "      <td>0.171375</td>\n",
              "      <td>0.581590</td>\n",
              "      <td>0.084303</td>\n",
              "      <td>0.565191</td>\n",
              "      <td>0.399904</td>\n",
              "      <td>0.260418</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269773</td>\n",
              "      <td>0.507717</td>\n",
              "      <td>0.666021</td>\n",
              "      <td>0.368713</td>\n",
              "      <td>0.350692</td>\n",
              "      <td>0.447806</td>\n",
              "      <td>0.406551</td>\n",
              "      <td>0.307880</td>\n",
              "      <td>0.853506</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250000 rows × 101 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z8R4hGm51Tb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(dataset[dependent_columns],dataset['loss'],test_size=0.25,random_state=True,shuffle=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds5pYPGO8Qzs"
      },
      "source": [
        "import optuna.integration.xgboost as xgb"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIEQoVUwALn3"
      },
      "source": [
        "import optuna.integration.lightgbm as lgbo"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW91z3kOBRd5"
      },
      "source": [
        "lgb_train=lgbo.Dataset(X_train,y_train)\n",
        "lgb_valid=lgbo.Dataset(X_test,y_test)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SogcyeOLBswh",
        "outputId": "660a602d-e177-44f9-b636-22ab760e25fa"
      },
      "source": [
        "params={'objective':'mean_squared_error','metric': 'rmse'}\n",
        "model=lgbo.train(params,lgb_train,valid_sets=lgb_valid,verbose_eval=False, num_boost_round=100, early_stopping_rounds=5)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-12 11:48:26,125]\u001b[0m A new study created in memory with name: no-name-f04687c9-2925-42b6-9879-0459478dbec9\u001b[0m\n",
            "\n",
            "\n",
            "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: inf:   0%|                                                          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039506 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:   0%|                                                     | 0/7 [00:02<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  14%|######4                                      | 1/7 [00:02<00:13,  2.17s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:28,313]\u001b[0m Trial 0 finished with value: 7.853125557823134 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 7.853125557823134.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  14%|######4                                      | 1/7 [00:02<00:13,  2.17s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061715 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  14%|######4                                      | 1/7 [00:05<00:13,  2.17s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  29%|############8                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:31,275]\u001b[0m Trial 1 finished with value: 7.854144341847006 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 7.853125557823134.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  29%|############8                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052526 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  29%|############8                                | 2/7 [00:07<00:13,  2.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  43%|###################2                         | 3/7 [00:07<00:09,  2.47s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:33,548]\u001b[0m Trial 2 finished with value: 7.856669702201823 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 7.853125557823134.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  43%|###################2                         | 3/7 [00:07<00:09,  2.47s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045904 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  43%|###################2                         | 3/7 [00:10<00:09,  2.47s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  57%|#########################7                   | 4/7 [00:10<00:07,  2.56s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:36,239]\u001b[0m Trial 3 finished with value: 7.855080967360817 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 7.853125557823134.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  57%|#########################7                   | 4/7 [00:10<00:07,  2.56s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039894 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  57%|#########################7                   | 4/7 [00:12<00:07,  2.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  71%|################################1            | 5/7 [00:12<00:04,  2.47s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:38,549]\u001b[0m Trial 4 finished with value: 7.854447217013653 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 7.853125557823134.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction, val_score: 7.853126:  71%|################################1            | 5/7 [00:12<00:04,  2.47s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046067 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction, val_score: 7.852776:  71%|################################1            | 5/7 [00:15<00:04,  2.47s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: 7.852776:  86%|######################################5      | 6/7 [00:15<00:02,  2.56s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:41,283]\u001b[0m Trial 5 finished with value: 7.852775542519601 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 5 with value: 7.852775542519601.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction, val_score: 7.852776:  86%|######################################5      | 6/7 [00:15<00:02,  2.56s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044651 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction, val_score: 7.852699:  86%|######################################5      | 6/7 [00:18<00:02,  2.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction, val_score: 7.852699: 100%|#############################################| 7/7 [00:18<00:00,  2.69s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:44,253]\u001b[0m Trial 6 finished with value: 7.852699291692991 and parameters: {'feature_fraction': 1.0}. Best is trial 6 with value: 7.852699291692991.\u001b[0m\n",
            "feature_fraction, val_score: 7.852699: 100%|#############################################| 7/7 [00:18<00:00,  2.59s/it]\n",
            "\n",
            "\n",
            "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:   0%|                                                          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045229 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:   0%|                                                          | 0/20 [00:02<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:   5%|##5                                               | 1/20 [00:02<00:51,  2.70s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:46,951]\u001b[0m Trial 7 finished with value: 7.8818672173110595 and parameters: {'num_leaves': 247}. Best is trial 7 with value: 7.8818672173110595.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:   5%|##5                                               | 1/20 [00:02<00:51,  2.70s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062213 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:   5%|##5                                               | 1/20 [00:06<00:51,  2.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  10%|#####                                             | 2/20 [00:06<00:55,  3.10s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:50,330]\u001b[0m Trial 8 finished with value: 7.881696910719886 and parameters: {'num_leaves': 233}. Best is trial 8 with value: 7.881696910719886.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  10%|#####                                             | 2/20 [00:06<00:55,  3.10s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045289 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  10%|#####                                             | 2/20 [00:08<00:55,  3.10s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  15%|#######5                                          | 3/20 [00:08<00:50,  2.99s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:53,188]\u001b[0m Trial 9 finished with value: 7.868018785609841 and parameters: {'num_leaves': 120}. Best is trial 9 with value: 7.868018785609841.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  15%|#######5                                          | 3/20 [00:08<00:50,  2.99s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045802 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  15%|#######5                                          | 3/20 [00:12<00:50,  2.99s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  20%|##########                                        | 4/20 [00:12<00:48,  3.03s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:56,290]\u001b[0m Trial 10 finished with value: 7.8676466455897724 and parameters: {'num_leaves': 97}. Best is trial 10 with value: 7.8676466455897724.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  20%|##########                                        | 4/20 [00:12<00:48,  3.03s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046103 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  20%|##########                                        | 4/20 [00:14<00:48,  3.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  25%|############5                                     | 5/20 [00:14<00:43,  2.90s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:48:58,940]\u001b[0m Trial 11 finished with value: 7.855572005947477 and parameters: {'num_leaves': 19}. Best is trial 11 with value: 7.855572005947477.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  25%|############5                                     | 5/20 [00:14<00:43,  2.90s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046824 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  25%|############5                                     | 5/20 [00:17<00:43,  2.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  30%|###############                                   | 6/20 [00:17<00:39,  2.85s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:01,696]\u001b[0m Trial 12 finished with value: 7.875000231051736 and parameters: {'num_leaves': 173}. Best is trial 11 with value: 7.855572005947477.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  30%|###############                                   | 6/20 [00:17<00:39,  2.85s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045210 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  30%|###############                                   | 6/20 [00:20<00:39,  2.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  35%|#################5                                | 7/20 [00:20<00:37,  2.90s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:04,692]\u001b[0m Trial 13 finished with value: 7.85438176638821 and parameters: {'num_leaves': 35}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  35%|#################5                                | 7/20 [00:20<00:37,  2.90s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046921 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  35%|#################5                                | 7/20 [00:23<00:37,  2.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  40%|####################                              | 8/20 [00:23<00:36,  3.08s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:08,171]\u001b[0m Trial 14 finished with value: 7.880478000204028 and parameters: {'num_leaves': 248}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  40%|####################                              | 8/20 [00:23<00:36,  3.08s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053969 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  40%|####################                              | 8/20 [00:27<00:36,  3.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  45%|######################5                           | 9/20 [00:27<00:35,  3.26s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:11,829]\u001b[0m Trial 15 finished with value: 7.868806962745433 and parameters: {'num_leaves': 134}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  45%|######################5                           | 9/20 [00:27<00:35,  3.26s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080013 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  45%|######################5                           | 9/20 [00:31<00:35,  3.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  50%|########################5                        | 10/20 [00:31<00:34,  3.46s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:15,726]\u001b[0m Trial 16 finished with value: 7.855721317954771 and parameters: {'num_leaves': 32}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  50%|########################5                        | 10/20 [00:31<00:34,  3.46s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050035 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  50%|########################5                        | 10/20 [00:34<00:34,  3.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  55%|##########################9                      | 11/20 [00:34<00:30,  3.43s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:19,081]\u001b[0m Trial 17 finished with value: 7.859975530982821 and parameters: {'num_leaves': 55}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  55%|##########################9                      | 11/20 [00:34<00:30,  3.43s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045969 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  55%|##########################9                      | 11/20 [00:37<00:30,  3.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  60%|#############################4                   | 12/20 [00:37<00:24,  3.10s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:21,454]\u001b[0m Trial 18 finished with value: 7.866227782092597 and parameters: {'num_leaves': 7}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  60%|#############################4                   | 12/20 [00:37<00:24,  3.10s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047081 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  60%|#############################4                   | 12/20 [00:40<00:24,  3.10s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  65%|###############################8                 | 13/20 [00:40<00:22,  3.17s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:24,783]\u001b[0m Trial 19 finished with value: 7.859879485893543 and parameters: {'num_leaves': 66}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  65%|###############################8                 | 13/20 [00:40<00:22,  3.17s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047874 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  65%|###############################8                 | 13/20 [00:43<00:22,  3.17s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  70%|##################################3              | 14/20 [00:43<00:17,  2.97s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:27,281]\u001b[0m Trial 20 finished with value: 7.856590810983974 and parameters: {'num_leaves': 13}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  70%|##################################3              | 14/20 [00:43<00:17,  2.97s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046878 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  70%|##################################3              | 14/20 [00:46<00:17,  2.97s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  75%|####################################7            | 15/20 [00:46<00:15,  3.06s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:30,560]\u001b[0m Trial 21 finished with value: 7.856186948751007 and parameters: {'num_leaves': 58}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  75%|####################################7            | 15/20 [00:46<00:15,  3.06s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051428 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  75%|####################################7            | 15/20 [00:49<00:15,  3.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  80%|#######################################2         | 16/20 [00:49<00:12,  3.02s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:33,479]\u001b[0m Trial 22 finished with value: 7.863349094323208 and parameters: {'num_leaves': 90}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  80%|#######################################2         | 16/20 [00:49<00:12,  3.02s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053368 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  80%|#######################################2         | 16/20 [00:52<00:12,  3.02s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  85%|#########################################6       | 17/20 [00:52<00:09,  3.21s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:37,138]\u001b[0m Trial 23 finished with value: 7.872661014063622 and parameters: {'num_leaves': 163}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  85%|#########################################6       | 17/20 [00:52<00:09,  3.21s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047346 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  85%|#########################################6       | 17/20 [00:56<00:09,  3.21s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  90%|############################################1    | 18/20 [00:56<00:06,  3.32s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:40,723]\u001b[0m Trial 24 finished with value: 7.855721317954771 and parameters: {'num_leaves': 32}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  90%|############################################1    | 18/20 [00:56<00:06,  3.32s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057287 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  90%|############################################1    | 18/20 [00:59<00:06,  3.32s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699:  95%|##############################################5  | 19/20 [00:59<00:03,  3.09s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:43,279]\u001b[0m Trial 25 finished with value: 7.873190677249141 and parameters: {'num_leaves': 5}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  95%|##############################################5  | 19/20 [00:59<00:03,  3.09s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044899 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "num_leaves, val_score: 7.852699:  95%|##############################################5  | 19/20 [01:01<00:03,  3.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "num_leaves, val_score: 7.852699: 100%|#################################################| 20/20 [01:01<00:00,  3.05s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:46,239]\u001b[0m Trial 26 finished with value: 7.864843023190774 and parameters: {'num_leaves': 92}. Best is trial 13 with value: 7.85438176638821.\u001b[0m\n",
            "num_leaves, val_score: 7.852699: 100%|#################################################| 20/20 [01:01<00:00,  3.10s/it]\n",
            "\n",
            "\n",
            "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852699:   0%|                                                             | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047366 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852699:   0%|                                                             | 0/10 [00:03<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852699:  10%|#####3                                               | 1/10 [00:03<00:29,  3.32s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:49,579]\u001b[0m Trial 27 finished with value: 7.853606563418363 and parameters: {'bagging_fraction': 0.8321823986113218, 'bagging_freq': 6}. Best is trial 27 with value: 7.853606563418363.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852699:  10%|#####3                                               | 1/10 [00:03<00:29,  3.32s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065142 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852699:  10%|#####3                                               | 1/10 [00:06<00:29,  3.32s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852699:  20%|##########6                                          | 2/10 [00:06<00:24,  3.02s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:52,394]\u001b[0m Trial 28 finished with value: 7.85926865713813 and parameters: {'bagging_fraction': 0.6851827909188535, 'bagging_freq': 1}. Best is trial 27 with value: 7.853606563418363.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852699:  20%|##########6                                          | 2/10 [00:06<00:24,  3.02s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047727 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852699:  20%|##########6                                          | 2/10 [00:09<00:24,  3.02s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852699:  30%|###############9                                     | 3/10 [00:09<00:21,  3.06s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:55,501]\u001b[0m Trial 29 finished with value: 7.855166028009144 and parameters: {'bagging_fraction': 0.737331670689518, 'bagging_freq': 2}. Best is trial 27 with value: 7.853606563418363.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852699:  30%|###############9                                     | 3/10 [00:09<00:21,  3.06s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079394 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852699:  30%|###############9                                     | 3/10 [00:12<00:21,  3.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852699:  40%|#####################2                               | 4/10 [00:12<00:19,  3.28s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:49:59,118]\u001b[0m Trial 30 finished with value: 7.8582336000805935 and parameters: {'bagging_fraction': 0.6462777228906789, 'bagging_freq': 4}. Best is trial 27 with value: 7.853606563418363.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852699:  40%|#####################2                               | 4/10 [00:12<00:19,  3.28s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046443 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  40%|#####################2                               | 4/10 [00:15<00:19,  3.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852179:  50%|##########################5                          | 5/10 [00:15<00:16,  3.21s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:02,210]\u001b[0m Trial 31 finished with value: 7.8521794068886885 and parameters: {'bagging_fraction': 0.8935797972262962, 'bagging_freq': 7}. Best is trial 31 with value: 7.8521794068886885.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  50%|##########################5                          | 5/10 [00:15<00:16,  3.21s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045726 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  50%|##########################5                          | 5/10 [00:19<00:16,  3.21s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852179:  60%|###############################8                     | 6/10 [00:19<00:12,  3.20s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:05,375]\u001b[0m Trial 32 finished with value: 7.857051352339924 and parameters: {'bagging_fraction': 0.8724678925958009, 'bagging_freq': 1}. Best is trial 31 with value: 7.8521794068886885.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  60%|###############################8                     | 6/10 [00:19<00:12,  3.20s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045760 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  60%|###############################8                     | 6/10 [00:22<00:12,  3.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852179:  70%|#####################################                | 7/10 [00:22<00:09,  3.19s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:08,560]\u001b[0m Trial 33 finished with value: 7.85789063956366 and parameters: {'bagging_fraction': 0.7454978555625202, 'bagging_freq': 3}. Best is trial 31 with value: 7.8521794068886885.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  70%|#####################################                | 7/10 [00:22<00:09,  3.19s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056228 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  70%|#####################################                | 7/10 [00:24<00:09,  3.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.852179:  80%|##########################################4          | 8/10 [00:24<00:05,  2.94s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:10,957]\u001b[0m Trial 34 finished with value: 7.8639915632608375 and parameters: {'bagging_fraction': 0.5615311238996794, 'bagging_freq': 1}. Best is trial 31 with value: 7.8521794068886885.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.852179:  80%|##########################################4          | 8/10 [00:24<00:05,  2.94s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046199 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.851372:  80%|##########################################4          | 8/10 [00:27<00:05,  2.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.851372:  90%|###############################################7     | 9/10 [00:27<00:02,  2.93s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:13,868]\u001b[0m Trial 35 finished with value: 7.851371581729893 and parameters: {'bagging_fraction': 0.8412551174105269, 'bagging_freq': 4}. Best is trial 35 with value: 7.851371581729893.\u001b[0m\n",
            "\n",
            "\n",
            "bagging, val_score: 7.851372:  90%|###############################################7     | 9/10 [00:27<00:02,  2.93s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049049 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "bagging, val_score: 7.851372:  90%|###############################################7     | 9/10 [00:30<00:02,  2.93s/it]\u001b[A\u001b[A\n",
            "\n",
            "bagging, val_score: 7.851372: 100%|####################################################| 10/10 [00:30<00:00,  2.87s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:16,608]\u001b[0m Trial 36 finished with value: 7.856896425288025 and parameters: {'bagging_fraction': 0.7243548220029739, 'bagging_freq': 7}. Best is trial 35 with value: 7.851371581729893.\u001b[0m\n",
            "bagging, val_score: 7.851372: 100%|####################################################| 10/10 [00:30<00:00,  3.04s/it]\n",
            "\n",
            "\n",
            "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.851372:   0%|                                              | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055964 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.851372:   0%|                                              | 0/3 [00:02<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.851372:  33%|############6                         | 1/3 [00:02<00:05,  2.75s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:19,366]\u001b[0m Trial 37 finished with value: 7.853082209639239 and parameters: {'feature_fraction': 0.92}. Best is trial 37 with value: 7.853082209639239.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.851372:  33%|############6                         | 1/3 [00:02<00:05,  2.75s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049361 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.850935:  33%|############6                         | 1/3 [00:05<00:05,  2.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.850935:  67%|#########################3            | 2/3 [00:05<00:02,  2.85s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:22,290]\u001b[0m Trial 38 finished with value: 7.850935267773864 and parameters: {'feature_fraction': 0.9840000000000001}. Best is trial 38 with value: 7.850935267773864.\u001b[0m\n",
            "\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.850935:  67%|#########################3            | 2/3 [00:05<00:02,  2.85s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047417 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.850935:  67%|#########################3            | 2/3 [00:08<00:02,  2.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "feature_fraction_stage2, val_score: 7.850935: 100%|######################################| 3/3 [00:08<00:00,  2.84s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:25,133]\u001b[0m Trial 39 finished with value: 7.856626188451317 and parameters: {'feature_fraction': 0.9520000000000001}. Best is trial 38 with value: 7.850935267773864.\u001b[0m\n",
            "feature_fraction_stage2, val_score: 7.850935: 100%|######################################| 3/3 [00:08<00:00,  2.84s/it]\n",
            "\n",
            "\n",
            "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:   0%|                                              | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048080 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:   0%|                                              | 0/20 [00:03<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:   5%|#9                                    | 1/20 [00:03<00:59,  3.13s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:28,275]\u001b[0m Trial 40 finished with value: 7.8509351672531364 and parameters: {'lambda_l1': 0.00439193722195977, 'lambda_l2': 3.579555414349541e-05}. Best is trial 40 with value: 7.8509351672531364.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:   5%|#9                                    | 1/20 [00:03<00:59,  3.13s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046285 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:   5%|#9                                    | 1/20 [00:06<00:59,  3.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  10%|###8                                  | 2/20 [00:06<00:55,  3.09s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:31,343]\u001b[0m Trial 41 finished with value: 7.850934605204465 and parameters: {'lambda_l1': 1.074643012313494e-06, 'lambda_l2': 0.0027218351871671894}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  10%|###8                                  | 2/20 [00:06<00:55,  3.09s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049262 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  10%|###8                                  | 2/20 [00:09<00:55,  3.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  15%|#####7                                | 3/20 [00:09<00:51,  3.05s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:34,332]\u001b[0m Trial 42 finished with value: 7.852573343210221 and parameters: {'lambda_l1': 0.40279842405499955, 'lambda_l2': 0.0010688712819802593}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  15%|#####7                                | 3/20 [00:09<00:51,  3.05s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064386 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  15%|#####7                                | 3/20 [00:12<00:51,  3.05s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  20%|#######6                              | 4/20 [00:12<00:50,  3.14s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:37,618]\u001b[0m Trial 43 finished with value: 7.850935188641628 and parameters: {'lambda_l1': 0.0037617578405650764, 'lambda_l2': 1.9472333009141995e-06}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  20%|#######6                              | 4/20 [00:12<00:50,  3.14s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058026 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  20%|#######6                              | 4/20 [00:15<00:50,  3.14s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  25%|#########5                            | 5/20 [00:15<00:46,  3.11s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:40,664]\u001b[0m Trial 44 finished with value: 7.850935257509082 and parameters: {'lambda_l1': 0.00046294410026288785, 'lambda_l2': 2.3603724001497725e-06}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  25%|#########5                            | 5/20 [00:15<00:46,  3.11s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054677 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  25%|#########5                            | 5/20 [00:18<00:46,  3.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  30%|###########4                          | 6/20 [00:18<00:43,  3.11s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:43,772]\u001b[0m Trial 45 finished with value: 7.850935256556294 and parameters: {'lambda_l1': 1.547551491598396e-08, 'lambda_l2': 4.6141230411029555e-05}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  30%|###########4                          | 6/20 [00:18<00:43,  3.11s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051414 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  30%|###########4                          | 6/20 [00:21<00:43,  3.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  35%|#############3                        | 7/20 [00:21<00:40,  3.09s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:46,821]\u001b[0m Trial 46 finished with value: 7.850935262422507 and parameters: {'lambda_l1': 0.00024264170033994962, 'lambda_l2': 1.1486774290838028e-06}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  35%|#############3                        | 7/20 [00:21<00:40,  3.09s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048759 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  35%|#############3                        | 7/20 [00:24<00:40,  3.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  40%|###############2                      | 8/20 [00:24<00:36,  3.08s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:49,889]\u001b[0m Trial 47 finished with value: 7.850935187474733 and parameters: {'lambda_l1': 0.0038397302486350418, 'lambda_l2': 6.170379235846248e-08}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  40%|###############2                      | 8/20 [00:24<00:36,  3.08s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045993 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  40%|###############2                      | 8/20 [00:27<00:36,  3.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  45%|#################1                    | 9/20 [00:27<00:34,  3.10s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:53,036]\u001b[0m Trial 48 finished with value: 7.851127419495375 and parameters: {'lambda_l1': 7.650231368824229e-06, 'lambda_l2': 6.890220306770905}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  45%|#################1                    | 9/20 [00:27<00:34,  3.10s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045165 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  45%|#################1                    | 9/20 [00:30<00:34,  3.10s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  50%|##################5                  | 10/20 [00:30<00:30,  3.05s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:55,959]\u001b[0m Trial 49 finished with value: 7.853400461630609 and parameters: {'lambda_l1': 0.05203186158571123, 'lambda_l2': 0.004196246982012812}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  50%|##################5                  | 10/20 [00:30<00:30,  3.05s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046192 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  50%|##################5                  | 10/20 [00:33<00:30,  3.05s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  55%|####################3                | 11/20 [00:33<00:27,  3.04s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:50:58,983]\u001b[0m Trial 50 finished with value: 7.853058442273899 and parameters: {'lambda_l1': 1.1346594408811669e-07, 'lambda_l2': 0.10387424705176052}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  55%|####################3                | 11/20 [00:33<00:27,  3.04s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053922 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  55%|####################3                | 11/20 [00:36<00:27,  3.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  60%|######################2              | 12/20 [00:36<00:24,  3.00s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:01,906]\u001b[0m Trial 51 finished with value: 7.853397841588381 and parameters: {'lambda_l1': 3.093465113022842e-06, 'lambda_l2': 0.022146842631683112}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  60%|######################2              | 12/20 [00:36<00:24,  3.00s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045629 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  60%|######################2              | 12/20 [00:39<00:24,  3.00s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  65%|########################             | 13/20 [00:39<00:21,  3.01s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:04,941]\u001b[0m Trial 52 finished with value: 7.853914107970294 and parameters: {'lambda_l1': 7.712808364363206, 'lambda_l2': 5.1952545887315765e-05}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  65%|########################             | 13/20 [00:39<00:21,  3.01s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048844 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  65%|########################             | 13/20 [00:42<00:21,  3.01s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  70%|#########################9           | 14/20 [00:42<00:18,  3.01s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:07,955]\u001b[0m Trial 53 finished with value: 7.853466571497219 and parameters: {'lambda_l1': 3.3843734993698415e-06, 'lambda_l2': 0.26126232441132674}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  70%|#########################9           | 14/20 [00:42<00:18,  3.01s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046683 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  70%|#########################9           | 14/20 [00:45<00:18,  3.01s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  75%|###########################7         | 15/20 [00:45<00:15,  3.03s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:11,031]\u001b[0m Trial 54 finished with value: 7.850935243392944 and parameters: {'lambda_l1': 5.9307560499410055e-05, 'lambda_l2': 9.498973322359037e-05}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  75%|###########################7         | 15/20 [00:45<00:15,  3.03s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045069 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  75%|###########################7         | 15/20 [00:48<00:15,  3.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  80%|#############################6       | 16/20 [00:48<00:12,  3.04s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:14,079]\u001b[0m Trial 55 finished with value: 7.850935267763611 and parameters: {'lambda_l1': 2.824673206519907e-07, 'lambda_l2': 2.0956684172780866e-08}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  80%|#############################6       | 16/20 [00:48<00:12,  3.04s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045040 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  80%|#############################6       | 16/20 [00:51<00:12,  3.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  85%|###############################4     | 17/20 [00:51<00:09,  3.03s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:17,103]\u001b[0m Trial 56 finished with value: 7.853048454248738 and parameters: {'lambda_l1': 0.016916694985308577, 'lambda_l2': 0.000798461556660063}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  85%|###############################4     | 17/20 [00:51<00:09,  3.03s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045626 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  85%|###############################4     | 17/20 [00:55<00:09,  3.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  90%|#################################3   | 18/20 [00:55<00:06,  3.09s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:20,311]\u001b[0m Trial 57 finished with value: 7.850935261964577 and parameters: {'lambda_l1': 4.312418354410452e-05, 'lambda_l2': 2.0185908097789684e-05}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  90%|#################################3   | 18/20 [00:55<00:06,  3.09s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046277 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  90%|#################################3   | 18/20 [00:58<00:06,  3.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  95%|###################################1 | 19/20 [00:58<00:03,  3.11s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:23,473]\u001b[0m Trial 58 finished with value: 7.851171456009283 and parameters: {'lambda_l1': 0.2605726060646231, 'lambda_l2': 0.00584746324451046}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  95%|###################################1 | 19/20 [00:58<00:03,  3.11s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063476 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "regularization_factors, val_score: 7.850935:  95%|###################################1 | 19/20 [01:01<00:03,  3.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "regularization_factors, val_score: 7.850935: 100%|#####################################| 20/20 [01:01<00:00,  3.11s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:26,601]\u001b[0m Trial 59 finished with value: 7.8549677039088905 and parameters: {'lambda_l1': 9.775754136839118e-07, 'lambda_l2': 2.3665386159337287}. Best is trial 41 with value: 7.850934605204465.\u001b[0m\n",
            "regularization_factors, val_score: 7.850935: 100%|#####################################| 20/20 [01:01<00:00,  3.07s/it]\n",
            "\n",
            "\n",
            "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:   0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077523 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:   0%|                                                     | 0/5 [00:04<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  20%|#########                                    | 1/5 [00:04<00:16,  4.06s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:30,680]\u001b[0m Trial 60 finished with value: 7.855111996129399 and parameters: {'min_child_samples': 5}. Best is trial 60 with value: 7.855111996129399.\u001b[0m\n",
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  20%|#########                                    | 1/5 [00:04<00:16,  4.06s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044918 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  20%|#########                                    | 1/5 [00:07<00:16,  4.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  40%|##################                           | 2/5 [00:07<00:10,  3.48s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:33,744]\u001b[0m Trial 61 finished with value: 7.852829000088943 and parameters: {'min_child_samples': 25}. Best is trial 61 with value: 7.852829000088943.\u001b[0m\n",
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  40%|##################                           | 2/5 [00:07<00:10,  3.48s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048811 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  40%|##################                           | 2/5 [00:10<00:10,  3.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  60%|###########################                  | 3/5 [00:10<00:06,  3.42s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:37,089]\u001b[0m Trial 62 finished with value: 7.8584302789279015 and parameters: {'min_child_samples': 10}. Best is trial 61 with value: 7.852829000088943.\u001b[0m\n",
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  60%|###########################                  | 3/5 [00:10<00:06,  3.42s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046913 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  60%|###########################                  | 3/5 [00:14<00:06,  3.42s/it]\u001b[A\u001b[A\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  80%|####################################         | 4/5 [00:14<00:03,  3.81s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:41,501]\u001b[0m Trial 63 finished with value: 7.854050701524753 and parameters: {'min_child_samples': 50}. Best is trial 61 with value: 7.852829000088943.\u001b[0m\n",
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850935:  80%|####################################         | 4/5 [00:14<00:03,  3.81s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044396 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 187500, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850403:  80%|####################################         | 4/5 [00:18<00:03,  3.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "min_data_in_leaf, val_score: 7.850403: 100%|#############################################| 5/5 [00:18<00:00,  3.58s/it]\u001b[A\u001b[A\u001b[32m[I 2021-08-12 11:51:44,661]\u001b[0m Trial 64 finished with value: 7.850403497642293 and parameters: {'min_child_samples': 100}. Best is trial 64 with value: 7.850403497642293.\u001b[0m\n",
            "min_data_in_leaf, val_score: 7.850403: 100%|#############################################| 5/5 [00:18<00:00,  3.61s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti9sUrY9CZ9D"
      },
      "source": [
        "best_lgb_params =model.params\n",
        "best_lgb_params[\"learning_rate\"] = 0.003\n",
        "best_lgb_params[\"early_stopping_round\"] = 1000\n",
        "best_lgb_params[\"num_iterations\"] = 80000"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3J-N3TsDm7I",
        "outputId": "de7a64c3-0d31-45a1-fe7e-a8b0e4f99ac5"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "kfld=KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "\n",
        "models=[]\n",
        "\n",
        "for fold,(train_index,test_index) in enumerate(kfld.split(dataset[dependent_columns],dataset['loss'])):\n",
        "  lgb_train=lgb.Dataset(dataset[dependent_columns].loc[train_index],dataset['loss'].loc[train_index])\n",
        "  lgb_test=lgb.Dataset(dataset[dependent_columns].loc[test_index],dataset['loss'].loc[test_index])\n",
        "  _model=lgb.train(best_lgb_params,lgb_train,valid_sets=lgb_test,verbose_eval=100)\n",
        "  models.append(_model)\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\venka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "c:\\users\\venka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047877 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.827720\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.90666\n",
            "[200]\tvalid_0's rmse: 7.89766\n",
            "[300]\tvalid_0's rmse: 7.8908\n",
            "[400]\tvalid_0's rmse: 7.88498\n",
            "[500]\tvalid_0's rmse: 7.87994\n",
            "[600]\tvalid_0's rmse: 7.87553\n",
            "[700]\tvalid_0's rmse: 7.87161\n",
            "[800]\tvalid_0's rmse: 7.8682\n",
            "[900]\tvalid_0's rmse: 7.865\n",
            "[1000]\tvalid_0's rmse: 7.86199\n",
            "[1100]\tvalid_0's rmse: 7.85918\n",
            "[1200]\tvalid_0's rmse: 7.85658\n",
            "[1300]\tvalid_0's rmse: 7.85425\n",
            "[1400]\tvalid_0's rmse: 7.85206\n",
            "[1500]\tvalid_0's rmse: 7.85007\n",
            "[1600]\tvalid_0's rmse: 7.84817\n",
            "[1700]\tvalid_0's rmse: 7.84663\n",
            "[1800]\tvalid_0's rmse: 7.84492\n",
            "[1900]\tvalid_0's rmse: 7.8432\n",
            "[2000]\tvalid_0's rmse: 7.84155\n",
            "[2100]\tvalid_0's rmse: 7.8401\n",
            "[2200]\tvalid_0's rmse: 7.83886\n",
            "[2300]\tvalid_0's rmse: 7.83761\n",
            "[2400]\tvalid_0's rmse: 7.83652\n",
            "[2500]\tvalid_0's rmse: 7.83531\n",
            "[2600]\tvalid_0's rmse: 7.83431\n",
            "[2700]\tvalid_0's rmse: 7.83354\n",
            "[2800]\tvalid_0's rmse: 7.83249\n",
            "[2900]\tvalid_0's rmse: 7.83176\n",
            "[3000]\tvalid_0's rmse: 7.83104\n",
            "[3100]\tvalid_0's rmse: 7.83057\n",
            "[3200]\tvalid_0's rmse: 7.82995\n",
            "[3300]\tvalid_0's rmse: 7.82917\n",
            "[3400]\tvalid_0's rmse: 7.82864\n",
            "[3500]\tvalid_0's rmse: 7.82829\n",
            "[3600]\tvalid_0's rmse: 7.82774\n",
            "[3700]\tvalid_0's rmse: 7.82717\n",
            "[3800]\tvalid_0's rmse: 7.82686\n",
            "[3900]\tvalid_0's rmse: 7.82656\n",
            "[4000]\tvalid_0's rmse: 7.82612\n",
            "[4100]\tvalid_0's rmse: 7.82584\n",
            "[4200]\tvalid_0's rmse: 7.82546\n",
            "[4300]\tvalid_0's rmse: 7.82514\n",
            "[4400]\tvalid_0's rmse: 7.82474\n",
            "[4500]\tvalid_0's rmse: 7.82439\n",
            "[4600]\tvalid_0's rmse: 7.82421\n",
            "[4700]\tvalid_0's rmse: 7.82388\n",
            "[4800]\tvalid_0's rmse: 7.82367\n",
            "[4900]\tvalid_0's rmse: 7.82338\n",
            "[5000]\tvalid_0's rmse: 7.82293\n",
            "[5100]\tvalid_0's rmse: 7.82261\n",
            "[5200]\tvalid_0's rmse: 7.82238\n",
            "[5300]\tvalid_0's rmse: 7.82215\n",
            "[5400]\tvalid_0's rmse: 7.82177\n",
            "[5500]\tvalid_0's rmse: 7.82155\n",
            "[5600]\tvalid_0's rmse: 7.82138\n",
            "[5700]\tvalid_0's rmse: 7.82116\n",
            "[5800]\tvalid_0's rmse: 7.82106\n",
            "[5900]\tvalid_0's rmse: 7.82091\n",
            "[6000]\tvalid_0's rmse: 7.82079\n",
            "[6100]\tvalid_0's rmse: 7.82078\n",
            "[6200]\tvalid_0's rmse: 7.8205\n",
            "[6300]\tvalid_0's rmse: 7.82033\n",
            "[6400]\tvalid_0's rmse: 7.82018\n",
            "[6500]\tvalid_0's rmse: 7.82\n",
            "[6600]\tvalid_0's rmse: 7.81978\n",
            "[6700]\tvalid_0's rmse: 7.8197\n",
            "[6800]\tvalid_0's rmse: 7.81973\n",
            "[6900]\tvalid_0's rmse: 7.81972\n",
            "[7000]\tvalid_0's rmse: 7.81969\n",
            "[7100]\tvalid_0's rmse: 7.81947\n",
            "[7200]\tvalid_0's rmse: 7.81925\n",
            "[7300]\tvalid_0's rmse: 7.81914\n",
            "[7400]\tvalid_0's rmse: 7.81914\n",
            "[7500]\tvalid_0's rmse: 7.81891\n",
            "[7600]\tvalid_0's rmse: 7.81879\n",
            "[7700]\tvalid_0's rmse: 7.8186\n",
            "[7800]\tvalid_0's rmse: 7.81862\n",
            "[7900]\tvalid_0's rmse: 7.81854\n",
            "[8000]\tvalid_0's rmse: 7.81843\n",
            "[8100]\tvalid_0's rmse: 7.81833\n",
            "[8200]\tvalid_0's rmse: 7.81816\n",
            "[8300]\tvalid_0's rmse: 7.8179\n",
            "[8400]\tvalid_0's rmse: 7.81794\n",
            "[8500]\tvalid_0's rmse: 7.8179\n",
            "[8600]\tvalid_0's rmse: 7.81792\n",
            "[8700]\tvalid_0's rmse: 7.81773\n",
            "[8800]\tvalid_0's rmse: 7.81757\n",
            "[8900]\tvalid_0's rmse: 7.81742\n",
            "[9000]\tvalid_0's rmse: 7.81741\n",
            "[9100]\tvalid_0's rmse: 7.81728\n",
            "[9200]\tvalid_0's rmse: 7.81719\n",
            "[9300]\tvalid_0's rmse: 7.81729\n",
            "[9400]\tvalid_0's rmse: 7.81725\n",
            "[9500]\tvalid_0's rmse: 7.8172\n",
            "[9600]\tvalid_0's rmse: 7.81733\n",
            "[9700]\tvalid_0's rmse: 7.81725\n",
            "[9800]\tvalid_0's rmse: 7.81723\n",
            "[9900]\tvalid_0's rmse: 7.81723\n",
            "[10000]\tvalid_0's rmse: 7.81715\n",
            "[10100]\tvalid_0's rmse: 7.81702\n",
            "[10200]\tvalid_0's rmse: 7.817\n",
            "[10300]\tvalid_0's rmse: 7.81678\n",
            "[10400]\tvalid_0's rmse: 7.81664\n",
            "[10500]\tvalid_0's rmse: 7.81666\n",
            "[10600]\tvalid_0's rmse: 7.81664\n",
            "[10700]\tvalid_0's rmse: 7.81675\n",
            "[10800]\tvalid_0's rmse: 7.81673\n",
            "[10900]\tvalid_0's rmse: 7.81665\n",
            "[11000]\tvalid_0's rmse: 7.81663\n",
            "[11100]\tvalid_0's rmse: 7.81654\n",
            "[11200]\tvalid_0's rmse: 7.81669\n",
            "[11300]\tvalid_0's rmse: 7.81664\n",
            "[11400]\tvalid_0's rmse: 7.81665\n",
            "[11500]\tvalid_0's rmse: 7.81655\n",
            "[11600]\tvalid_0's rmse: 7.81654\n",
            "[11700]\tvalid_0's rmse: 7.8164\n",
            "[11800]\tvalid_0's rmse: 7.81643\n",
            "[11900]\tvalid_0's rmse: 7.81645\n",
            "[12000]\tvalid_0's rmse: 7.81655\n",
            "[12100]\tvalid_0's rmse: 7.81656\n",
            "[12200]\tvalid_0's rmse: 7.8167\n",
            "[12300]\tvalid_0's rmse: 7.81665\n",
            "[12400]\tvalid_0's rmse: 7.81663\n",
            "[12500]\tvalid_0's rmse: 7.8166\n",
            "[12600]\tvalid_0's rmse: 7.81667\n",
            "Early stopping, best iteration is:\n",
            "[11683]\tvalid_0's rmse: 7.81635\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053389 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.812385\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.95304\n",
            "[200]\tvalid_0's rmse: 7.94462\n",
            "[300]\tvalid_0's rmse: 7.93887\n",
            "[400]\tvalid_0's rmse: 7.93364\n",
            "[500]\tvalid_0's rmse: 7.92921\n",
            "[600]\tvalid_0's rmse: 7.9252\n",
            "[700]\tvalid_0's rmse: 7.92179\n",
            "[800]\tvalid_0's rmse: 7.91826\n",
            "[900]\tvalid_0's rmse: 7.91535\n",
            "[1000]\tvalid_0's rmse: 7.91249\n",
            "[1100]\tvalid_0's rmse: 7.90998\n",
            "[1200]\tvalid_0's rmse: 7.90772\n",
            "[1300]\tvalid_0's rmse: 7.90545\n",
            "[1400]\tvalid_0's rmse: 7.90358\n",
            "[1500]\tvalid_0's rmse: 7.90167\n",
            "[1600]\tvalid_0's rmse: 7.89972\n",
            "[1700]\tvalid_0's rmse: 7.89815\n",
            "[1800]\tvalid_0's rmse: 7.89652\n",
            "[1900]\tvalid_0's rmse: 7.8952\n",
            "[2000]\tvalid_0's rmse: 7.89388\n",
            "[2100]\tvalid_0's rmse: 7.89281\n",
            "[2200]\tvalid_0's rmse: 7.89171\n",
            "[2300]\tvalid_0's rmse: 7.89053\n",
            "[2400]\tvalid_0's rmse: 7.88951\n",
            "[2500]\tvalid_0's rmse: 7.88847\n",
            "[2600]\tvalid_0's rmse: 7.88765\n",
            "[2700]\tvalid_0's rmse: 7.88675\n",
            "[2800]\tvalid_0's rmse: 7.88608\n",
            "[2900]\tvalid_0's rmse: 7.88532\n",
            "[3000]\tvalid_0's rmse: 7.8846\n",
            "[3100]\tvalid_0's rmse: 7.88411\n",
            "[3200]\tvalid_0's rmse: 7.88362\n",
            "[3300]\tvalid_0's rmse: 7.88314\n",
            "[3400]\tvalid_0's rmse: 7.88276\n",
            "[3500]\tvalid_0's rmse: 7.88224\n",
            "[3600]\tvalid_0's rmse: 7.88186\n",
            "[3700]\tvalid_0's rmse: 7.88126\n",
            "[3800]\tvalid_0's rmse: 7.88067\n",
            "[3900]\tvalid_0's rmse: 7.88042\n",
            "[4000]\tvalid_0's rmse: 7.88009\n",
            "[4100]\tvalid_0's rmse: 7.87972\n",
            "[4200]\tvalid_0's rmse: 7.87938\n",
            "[4300]\tvalid_0's rmse: 7.8791\n",
            "[4400]\tvalid_0's rmse: 7.87877\n",
            "[4500]\tvalid_0's rmse: 7.87857\n",
            "[4600]\tvalid_0's rmse: 7.87831\n",
            "[4700]\tvalid_0's rmse: 7.87811\n",
            "[4800]\tvalid_0's rmse: 7.87794\n",
            "[4900]\tvalid_0's rmse: 7.8777\n",
            "[5000]\tvalid_0's rmse: 7.87752\n",
            "[5100]\tvalid_0's rmse: 7.87715\n",
            "[5200]\tvalid_0's rmse: 7.87689\n",
            "[5300]\tvalid_0's rmse: 7.87673\n",
            "[5400]\tvalid_0's rmse: 7.87646\n",
            "[5500]\tvalid_0's rmse: 7.87618\n",
            "[5600]\tvalid_0's rmse: 7.87594\n",
            "[5700]\tvalid_0's rmse: 7.8757\n",
            "[5800]\tvalid_0's rmse: 7.87548\n",
            "[5900]\tvalid_0's rmse: 7.87511\n",
            "[6000]\tvalid_0's rmse: 7.87494\n",
            "[6100]\tvalid_0's rmse: 7.87487\n",
            "[6200]\tvalid_0's rmse: 7.87467\n",
            "[6300]\tvalid_0's rmse: 7.87447\n",
            "[6400]\tvalid_0's rmse: 7.87424\n",
            "[6500]\tvalid_0's rmse: 7.87412\n",
            "[6600]\tvalid_0's rmse: 7.874\n",
            "[6700]\tvalid_0's rmse: 7.87396\n",
            "[6800]\tvalid_0's rmse: 7.87378\n",
            "[6900]\tvalid_0's rmse: 7.87366\n",
            "[7000]\tvalid_0's rmse: 7.87344\n",
            "[7100]\tvalid_0's rmse: 7.87331\n",
            "[7200]\tvalid_0's rmse: 7.87314\n",
            "[7300]\tvalid_0's rmse: 7.87311\n",
            "[7400]\tvalid_0's rmse: 7.87288\n",
            "[7500]\tvalid_0's rmse: 7.87278\n",
            "[7600]\tvalid_0's rmse: 7.87279\n",
            "[7700]\tvalid_0's rmse: 7.87276\n",
            "[7800]\tvalid_0's rmse: 7.87274\n",
            "[7900]\tvalid_0's rmse: 7.87268\n",
            "[8000]\tvalid_0's rmse: 7.87254\n",
            "[8100]\tvalid_0's rmse: 7.87246\n",
            "[8200]\tvalid_0's rmse: 7.87238\n",
            "[8300]\tvalid_0's rmse: 7.8723\n",
            "[8400]\tvalid_0's rmse: 7.87225\n",
            "[8500]\tvalid_0's rmse: 7.87213\n",
            "[8600]\tvalid_0's rmse: 7.87203\n",
            "[8700]\tvalid_0's rmse: 7.87193\n",
            "[8800]\tvalid_0's rmse: 7.87175\n",
            "[8900]\tvalid_0's rmse: 7.87166\n",
            "[9000]\tvalid_0's rmse: 7.8716\n",
            "[9100]\tvalid_0's rmse: 7.87161\n",
            "[9200]\tvalid_0's rmse: 7.87157\n",
            "[9300]\tvalid_0's rmse: 7.87161\n",
            "[9400]\tvalid_0's rmse: 7.87161\n",
            "[9500]\tvalid_0's rmse: 7.87156\n",
            "[9600]\tvalid_0's rmse: 7.87155\n",
            "[9700]\tvalid_0's rmse: 7.8714\n",
            "[9800]\tvalid_0's rmse: 7.87137\n",
            "[9900]\tvalid_0's rmse: 7.87117\n",
            "[10000]\tvalid_0's rmse: 7.87114\n",
            "[10100]\tvalid_0's rmse: 7.871\n",
            "[10200]\tvalid_0's rmse: 7.87099\n",
            "[10300]\tvalid_0's rmse: 7.87086\n",
            "[10400]\tvalid_0's rmse: 7.87071\n",
            "[10500]\tvalid_0's rmse: 7.8707\n",
            "[10600]\tvalid_0's rmse: 7.87054\n",
            "[10700]\tvalid_0's rmse: 7.8705\n",
            "[10800]\tvalid_0's rmse: 7.87048\n",
            "[10900]\tvalid_0's rmse: 7.87045\n",
            "[11000]\tvalid_0's rmse: 7.87051\n",
            "[11100]\tvalid_0's rmse: 7.8704\n",
            "[11200]\tvalid_0's rmse: 7.8703\n",
            "[11300]\tvalid_0's rmse: 7.87027\n",
            "[11400]\tvalid_0's rmse: 7.87013\n",
            "[11500]\tvalid_0's rmse: 7.86999\n",
            "[11600]\tvalid_0's rmse: 7.86993\n",
            "[11700]\tvalid_0's rmse: 7.86991\n",
            "[11800]\tvalid_0's rmse: 7.8699\n",
            "[11900]\tvalid_0's rmse: 7.86978\n",
            "[12000]\tvalid_0's rmse: 7.86965\n",
            "[12100]\tvalid_0's rmse: 7.86968\n",
            "[12200]\tvalid_0's rmse: 7.86967\n",
            "[12300]\tvalid_0's rmse: 7.86958\n",
            "[12400]\tvalid_0's rmse: 7.86954\n",
            "[12500]\tvalid_0's rmse: 7.86937\n",
            "[12600]\tvalid_0's rmse: 7.86935\n",
            "[12700]\tvalid_0's rmse: 7.86929\n",
            "[12800]\tvalid_0's rmse: 7.86925\n",
            "[12900]\tvalid_0's rmse: 7.8692\n",
            "[13000]\tvalid_0's rmse: 7.86915\n",
            "[13100]\tvalid_0's rmse: 7.86907\n",
            "[13200]\tvalid_0's rmse: 7.86894\n",
            "[13300]\tvalid_0's rmse: 7.86887\n",
            "[13400]\tvalid_0's rmse: 7.86897\n",
            "[13500]\tvalid_0's rmse: 7.86894\n",
            "[13600]\tvalid_0's rmse: 7.86882\n",
            "[13700]\tvalid_0's rmse: 7.86882\n",
            "[13800]\tvalid_0's rmse: 7.86877\n",
            "[13900]\tvalid_0's rmse: 7.86884\n",
            "[14000]\tvalid_0's rmse: 7.86874\n",
            "[14100]\tvalid_0's rmse: 7.86879\n",
            "[14200]\tvalid_0's rmse: 7.86867\n",
            "[14300]\tvalid_0's rmse: 7.86878\n",
            "[14400]\tvalid_0's rmse: 7.86881\n",
            "[14500]\tvalid_0's rmse: 7.86879\n",
            "[14600]\tvalid_0's rmse: 7.86858\n",
            "[14700]\tvalid_0's rmse: 7.86847\n",
            "[14800]\tvalid_0's rmse: 7.86839\n",
            "[14900]\tvalid_0's rmse: 7.86855\n",
            "[15000]\tvalid_0's rmse: 7.86845\n",
            "[15100]\tvalid_0's rmse: 7.86856\n",
            "[15200]\tvalid_0's rmse: 7.86856\n",
            "[15300]\tvalid_0's rmse: 7.86857\n",
            "[15400]\tvalid_0's rmse: 7.86855\n",
            "[15500]\tvalid_0's rmse: 7.86849\n",
            "[15600]\tvalid_0's rmse: 7.86856\n",
            "[15700]\tvalid_0's rmse: 7.8685\n",
            "[15800]\tvalid_0's rmse: 7.86852\n",
            "Early stopping, best iteration is:\n",
            "[14801]\tvalid_0's rmse: 7.86839\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070737 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.809740\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.94299\n",
            "[200]\tvalid_0's rmse: 7.93398\n",
            "[300]\tvalid_0's rmse: 7.92733\n",
            "[400]\tvalid_0's rmse: 7.92194\n",
            "[500]\tvalid_0's rmse: 7.91668\n",
            "[600]\tvalid_0's rmse: 7.91239\n",
            "[700]\tvalid_0's rmse: 7.90846\n",
            "[800]\tvalid_0's rmse: 7.90466\n",
            "[900]\tvalid_0's rmse: 7.90139\n",
            "[1000]\tvalid_0's rmse: 7.89817\n",
            "[1100]\tvalid_0's rmse: 7.89525\n",
            "[1200]\tvalid_0's rmse: 7.89233\n",
            "[1300]\tvalid_0's rmse: 7.88972\n",
            "[1400]\tvalid_0's rmse: 7.88703\n",
            "[1500]\tvalid_0's rmse: 7.88487\n",
            "[1600]\tvalid_0's rmse: 7.88267\n",
            "[1700]\tvalid_0's rmse: 7.88078\n",
            "[1800]\tvalid_0's rmse: 7.87865\n",
            "[1900]\tvalid_0's rmse: 7.87689\n",
            "[2000]\tvalid_0's rmse: 7.87522\n",
            "[2100]\tvalid_0's rmse: 7.87338\n",
            "[2200]\tvalid_0's rmse: 7.87191\n",
            "[2300]\tvalid_0's rmse: 7.8706\n",
            "[2400]\tvalid_0's rmse: 7.8692\n",
            "[2500]\tvalid_0's rmse: 7.86802\n",
            "[2600]\tvalid_0's rmse: 7.86689\n",
            "[2700]\tvalid_0's rmse: 7.86592\n",
            "[2800]\tvalid_0's rmse: 7.86501\n",
            "[2900]\tvalid_0's rmse: 7.8642\n",
            "[3000]\tvalid_0's rmse: 7.86338\n",
            "[3100]\tvalid_0's rmse: 7.86261\n",
            "[3200]\tvalid_0's rmse: 7.86191\n",
            "[3300]\tvalid_0's rmse: 7.86121\n",
            "[3400]\tvalid_0's rmse: 7.86056\n",
            "[3500]\tvalid_0's rmse: 7.86003\n",
            "[3600]\tvalid_0's rmse: 7.8593\n",
            "[3700]\tvalid_0's rmse: 7.85886\n",
            "[3800]\tvalid_0's rmse: 7.85837\n",
            "[3900]\tvalid_0's rmse: 7.8579\n",
            "[4000]\tvalid_0's rmse: 7.85729\n",
            "[4100]\tvalid_0's rmse: 7.85673\n",
            "[4200]\tvalid_0's rmse: 7.85629\n",
            "[4300]\tvalid_0's rmse: 7.85594\n",
            "[4400]\tvalid_0's rmse: 7.8555\n",
            "[4500]\tvalid_0's rmse: 7.85513\n",
            "[4600]\tvalid_0's rmse: 7.85457\n",
            "[4700]\tvalid_0's rmse: 7.85416\n",
            "[4800]\tvalid_0's rmse: 7.85376\n",
            "[4900]\tvalid_0's rmse: 7.85368\n",
            "[5000]\tvalid_0's rmse: 7.85322\n",
            "[5100]\tvalid_0's rmse: 7.8529\n",
            "[5200]\tvalid_0's rmse: 7.85256\n",
            "[5300]\tvalid_0's rmse: 7.8523\n",
            "[5400]\tvalid_0's rmse: 7.85203\n",
            "[5500]\tvalid_0's rmse: 7.85175\n",
            "[5600]\tvalid_0's rmse: 7.85148\n",
            "[5700]\tvalid_0's rmse: 7.85108\n",
            "[5800]\tvalid_0's rmse: 7.8508\n",
            "[5900]\tvalid_0's rmse: 7.85044\n",
            "[6000]\tvalid_0's rmse: 7.85027\n",
            "[6100]\tvalid_0's rmse: 7.85004\n",
            "[6200]\tvalid_0's rmse: 7.8499\n",
            "[6300]\tvalid_0's rmse: 7.84964\n",
            "[6400]\tvalid_0's rmse: 7.84934\n",
            "[6500]\tvalid_0's rmse: 7.84917\n",
            "[6600]\tvalid_0's rmse: 7.84891\n",
            "[6700]\tvalid_0's rmse: 7.84882\n",
            "[6800]\tvalid_0's rmse: 7.84858\n",
            "[6900]\tvalid_0's rmse: 7.84842\n",
            "[7000]\tvalid_0's rmse: 7.84825\n",
            "[7100]\tvalid_0's rmse: 7.84805\n",
            "[7200]\tvalid_0's rmse: 7.84799\n",
            "[7300]\tvalid_0's rmse: 7.84808\n",
            "[7400]\tvalid_0's rmse: 7.84785\n",
            "[7500]\tvalid_0's rmse: 7.84764\n",
            "[7600]\tvalid_0's rmse: 7.84754\n",
            "[7700]\tvalid_0's rmse: 7.84731\n",
            "[7800]\tvalid_0's rmse: 7.84703\n",
            "[7900]\tvalid_0's rmse: 7.84677\n",
            "[8000]\tvalid_0's rmse: 7.84669\n",
            "[8100]\tvalid_0's rmse: 7.84664\n",
            "[8200]\tvalid_0's rmse: 7.84652\n",
            "[8300]\tvalid_0's rmse: 7.84632\n",
            "[8400]\tvalid_0's rmse: 7.84631\n",
            "[8500]\tvalid_0's rmse: 7.84613\n",
            "[8600]\tvalid_0's rmse: 7.84594\n",
            "[8700]\tvalid_0's rmse: 7.84601\n",
            "[8800]\tvalid_0's rmse: 7.84589\n",
            "[8900]\tvalid_0's rmse: 7.84592\n",
            "[9000]\tvalid_0's rmse: 7.84582\n",
            "[9100]\tvalid_0's rmse: 7.84587\n",
            "[9200]\tvalid_0's rmse: 7.84565\n",
            "[9300]\tvalid_0's rmse: 7.84556\n",
            "[9400]\tvalid_0's rmse: 7.84547\n",
            "[9500]\tvalid_0's rmse: 7.8453\n",
            "[9600]\tvalid_0's rmse: 7.84517\n",
            "[9700]\tvalid_0's rmse: 7.8451\n",
            "[9800]\tvalid_0's rmse: 7.84486\n",
            "[9900]\tvalid_0's rmse: 7.84492\n",
            "[10000]\tvalid_0's rmse: 7.84488\n",
            "[10100]\tvalid_0's rmse: 7.84472\n",
            "[10200]\tvalid_0's rmse: 7.8448\n",
            "[10300]\tvalid_0's rmse: 7.84481\n",
            "[10400]\tvalid_0's rmse: 7.84463\n",
            "[10500]\tvalid_0's rmse: 7.84452\n",
            "[10600]\tvalid_0's rmse: 7.84459\n",
            "[10700]\tvalid_0's rmse: 7.84455\n",
            "[10800]\tvalid_0's rmse: 7.8445\n",
            "[10900]\tvalid_0's rmse: 7.84453\n",
            "[11000]\tvalid_0's rmse: 7.84444\n",
            "[11100]\tvalid_0's rmse: 7.84438\n",
            "[11200]\tvalid_0's rmse: 7.84426\n",
            "[11300]\tvalid_0's rmse: 7.844\n",
            "[11400]\tvalid_0's rmse: 7.84403\n",
            "[11500]\tvalid_0's rmse: 7.84402\n",
            "[11600]\tvalid_0's rmse: 7.84401\n",
            "[11700]\tvalid_0's rmse: 7.84406\n",
            "[11800]\tvalid_0's rmse: 7.84392\n",
            "[11900]\tvalid_0's rmse: 7.84377\n",
            "[12000]\tvalid_0's rmse: 7.84365\n",
            "[12100]\tvalid_0's rmse: 7.84347\n",
            "[12200]\tvalid_0's rmse: 7.84342\n",
            "[12300]\tvalid_0's rmse: 7.84333\n",
            "[12400]\tvalid_0's rmse: 7.84325\n",
            "[12500]\tvalid_0's rmse: 7.84324\n",
            "[12600]\tvalid_0's rmse: 7.84317\n",
            "[12700]\tvalid_0's rmse: 7.84314\n",
            "[12800]\tvalid_0's rmse: 7.843\n",
            "[12900]\tvalid_0's rmse: 7.84292\n",
            "[13000]\tvalid_0's rmse: 7.84281\n",
            "[13100]\tvalid_0's rmse: 7.84273\n",
            "[13200]\tvalid_0's rmse: 7.8428\n",
            "[13300]\tvalid_0's rmse: 7.8428\n",
            "[13400]\tvalid_0's rmse: 7.84277\n",
            "[13500]\tvalid_0's rmse: 7.84277\n",
            "[13600]\tvalid_0's rmse: 7.8428\n",
            "[13700]\tvalid_0's rmse: 7.84269\n",
            "[13800]\tvalid_0's rmse: 7.84274\n",
            "[13900]\tvalid_0's rmse: 7.84272\n",
            "[14000]\tvalid_0's rmse: 7.84269\n",
            "[14100]\tvalid_0's rmse: 7.84276\n",
            "[14200]\tvalid_0's rmse: 7.84263\n",
            "[14300]\tvalid_0's rmse: 7.84258\n",
            "[14400]\tvalid_0's rmse: 7.84243\n",
            "[14500]\tvalid_0's rmse: 7.84238\n",
            "[14600]\tvalid_0's rmse: 7.84232\n",
            "[14700]\tvalid_0's rmse: 7.84227\n",
            "[14800]\tvalid_0's rmse: 7.84212\n",
            "[14900]\tvalid_0's rmse: 7.84211\n",
            "[15000]\tvalid_0's rmse: 7.84214\n",
            "[15100]\tvalid_0's rmse: 7.84209\n",
            "[15200]\tvalid_0's rmse: 7.84204\n",
            "[15300]\tvalid_0's rmse: 7.84191\n",
            "[15400]\tvalid_0's rmse: 7.84182\n",
            "[15500]\tvalid_0's rmse: 7.84185\n",
            "[15600]\tvalid_0's rmse: 7.84174\n",
            "[15700]\tvalid_0's rmse: 7.84167\n",
            "[15800]\tvalid_0's rmse: 7.84163\n",
            "[15900]\tvalid_0's rmse: 7.84165\n",
            "[16000]\tvalid_0's rmse: 7.84154\n",
            "[16100]\tvalid_0's rmse: 7.84146\n",
            "[16200]\tvalid_0's rmse: 7.84141\n",
            "[16300]\tvalid_0's rmse: 7.8415\n",
            "[16400]\tvalid_0's rmse: 7.84142\n",
            "[16500]\tvalid_0's rmse: 7.84143\n",
            "[16600]\tvalid_0's rmse: 7.84136\n",
            "[16700]\tvalid_0's rmse: 7.84143\n",
            "[16800]\tvalid_0's rmse: 7.84137\n",
            "[16900]\tvalid_0's rmse: 7.84126\n",
            "[17000]\tvalid_0's rmse: 7.84128\n",
            "[17100]\tvalid_0's rmse: 7.84132\n",
            "[17200]\tvalid_0's rmse: 7.84142\n",
            "[17300]\tvalid_0's rmse: 7.84121\n",
            "[17400]\tvalid_0's rmse: 7.8412\n",
            "[17500]\tvalid_0's rmse: 7.84125\n",
            "[17600]\tvalid_0's rmse: 7.84115\n",
            "[17700]\tvalid_0's rmse: 7.84109\n",
            "[17800]\tvalid_0's rmse: 7.84105\n",
            "[17900]\tvalid_0's rmse: 7.841\n",
            "[18000]\tvalid_0's rmse: 7.84089\n",
            "[18100]\tvalid_0's rmse: 7.84084\n",
            "[18200]\tvalid_0's rmse: 7.84094\n",
            "[18300]\tvalid_0's rmse: 7.84091\n",
            "[18400]\tvalid_0's rmse: 7.84089\n",
            "[18500]\tvalid_0's rmse: 7.84091\n",
            "[18600]\tvalid_0's rmse: 7.84086\n",
            "[18700]\tvalid_0's rmse: 7.84085\n",
            "[18800]\tvalid_0's rmse: 7.84097\n",
            "[18900]\tvalid_0's rmse: 7.84102\n",
            "[19000]\tvalid_0's rmse: 7.84117\n",
            "[19100]\tvalid_0's rmse: 7.84116\n",
            "[19200]\tvalid_0's rmse: 7.84118\n",
            "[19300]\tvalid_0's rmse: 7.8412\n",
            "[19400]\tvalid_0's rmse: 7.84114\n",
            "Early stopping, best iteration is:\n",
            "[18431]\tvalid_0's rmse: 7.84081\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071397 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.805665\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.918\n",
            "[200]\tvalid_0's rmse: 7.90924\n",
            "[300]\tvalid_0's rmse: 7.90255\n",
            "[400]\tvalid_0's rmse: 7.89732\n",
            "[500]\tvalid_0's rmse: 7.89266\n",
            "[600]\tvalid_0's rmse: 7.88896\n",
            "[700]\tvalid_0's rmse: 7.88518\n",
            "[800]\tvalid_0's rmse: 7.88165\n",
            "[900]\tvalid_0's rmse: 7.87852\n",
            "[1000]\tvalid_0's rmse: 7.87578\n",
            "[1100]\tvalid_0's rmse: 7.87321\n",
            "[1200]\tvalid_0's rmse: 7.87053\n",
            "[1300]\tvalid_0's rmse: 7.86834\n",
            "[1400]\tvalid_0's rmse: 7.8662\n",
            "[1500]\tvalid_0's rmse: 7.86429\n",
            "[1600]\tvalid_0's rmse: 7.86264\n",
            "[1700]\tvalid_0's rmse: 7.86096\n",
            "[1800]\tvalid_0's rmse: 7.85956\n",
            "[1900]\tvalid_0's rmse: 7.85826\n",
            "[2000]\tvalid_0's rmse: 7.85714\n",
            "[2100]\tvalid_0's rmse: 7.85584\n",
            "[2200]\tvalid_0's rmse: 7.85479\n",
            "[2300]\tvalid_0's rmse: 7.85386\n",
            "[2400]\tvalid_0's rmse: 7.85296\n",
            "[2500]\tvalid_0's rmse: 7.85201\n",
            "[2600]\tvalid_0's rmse: 7.85134\n",
            "[2700]\tvalid_0's rmse: 7.85046\n",
            "[2800]\tvalid_0's rmse: 7.84972\n",
            "[2900]\tvalid_0's rmse: 7.84911\n",
            "[3000]\tvalid_0's rmse: 7.84846\n",
            "[3100]\tvalid_0's rmse: 7.84773\n",
            "[3200]\tvalid_0's rmse: 7.84712\n",
            "[3300]\tvalid_0's rmse: 7.84667\n",
            "[3400]\tvalid_0's rmse: 7.84628\n",
            "[3500]\tvalid_0's rmse: 7.84584\n",
            "[3600]\tvalid_0's rmse: 7.84561\n",
            "[3700]\tvalid_0's rmse: 7.84529\n",
            "[3800]\tvalid_0's rmse: 7.84505\n",
            "[3900]\tvalid_0's rmse: 7.84462\n",
            "[4000]\tvalid_0's rmse: 7.84435\n",
            "[4100]\tvalid_0's rmse: 7.84421\n",
            "[4200]\tvalid_0's rmse: 7.84386\n",
            "[4300]\tvalid_0's rmse: 7.84365\n",
            "[4400]\tvalid_0's rmse: 7.84351\n",
            "[4500]\tvalid_0's rmse: 7.84331\n",
            "[4600]\tvalid_0's rmse: 7.84301\n",
            "[4700]\tvalid_0's rmse: 7.84298\n",
            "[4800]\tvalid_0's rmse: 7.84254\n",
            "[4900]\tvalid_0's rmse: 7.84247\n",
            "[5000]\tvalid_0's rmse: 7.84228\n",
            "[5100]\tvalid_0's rmse: 7.84216\n",
            "[5200]\tvalid_0's rmse: 7.84185\n",
            "[5300]\tvalid_0's rmse: 7.84147\n",
            "[5400]\tvalid_0's rmse: 7.84111\n",
            "[5500]\tvalid_0's rmse: 7.84086\n",
            "[5600]\tvalid_0's rmse: 7.84069\n",
            "[5700]\tvalid_0's rmse: 7.84058\n",
            "[5800]\tvalid_0's rmse: 7.84046\n",
            "[5900]\tvalid_0's rmse: 7.84018\n",
            "[6000]\tvalid_0's rmse: 7.84002\n",
            "[6100]\tvalid_0's rmse: 7.84008\n",
            "[6200]\tvalid_0's rmse: 7.84002\n",
            "[6300]\tvalid_0's rmse: 7.83977\n",
            "[6400]\tvalid_0's rmse: 7.83961\n",
            "[6500]\tvalid_0's rmse: 7.83943\n",
            "[6600]\tvalid_0's rmse: 7.83943\n",
            "[6700]\tvalid_0's rmse: 7.83933\n",
            "[6800]\tvalid_0's rmse: 7.83931\n",
            "[6900]\tvalid_0's rmse: 7.83923\n",
            "[7000]\tvalid_0's rmse: 7.83908\n",
            "[7100]\tvalid_0's rmse: 7.83895\n",
            "[7200]\tvalid_0's rmse: 7.83878\n",
            "[7300]\tvalid_0's rmse: 7.83865\n",
            "[7400]\tvalid_0's rmse: 7.83855\n",
            "[7500]\tvalid_0's rmse: 7.83835\n",
            "[7600]\tvalid_0's rmse: 7.83832\n",
            "[7700]\tvalid_0's rmse: 7.83827\n",
            "[7800]\tvalid_0's rmse: 7.83819\n",
            "[7900]\tvalid_0's rmse: 7.83816\n",
            "[8000]\tvalid_0's rmse: 7.83813\n",
            "[8100]\tvalid_0's rmse: 7.83811\n",
            "[8200]\tvalid_0's rmse: 7.83791\n",
            "[8300]\tvalid_0's rmse: 7.83785\n",
            "[8400]\tvalid_0's rmse: 7.83778\n",
            "[8500]\tvalid_0's rmse: 7.83771\n",
            "[8600]\tvalid_0's rmse: 7.83736\n",
            "[8700]\tvalid_0's rmse: 7.83716\n",
            "[8800]\tvalid_0's rmse: 7.83695\n",
            "[8900]\tvalid_0's rmse: 7.83684\n",
            "[9000]\tvalid_0's rmse: 7.83669\n",
            "[9100]\tvalid_0's rmse: 7.83666\n",
            "[9200]\tvalid_0's rmse: 7.8366\n",
            "[9300]\tvalid_0's rmse: 7.83653\n",
            "[9400]\tvalid_0's rmse: 7.8363\n",
            "[9500]\tvalid_0's rmse: 7.83618\n",
            "[9600]\tvalid_0's rmse: 7.83613\n",
            "[9700]\tvalid_0's rmse: 7.83615\n",
            "[9800]\tvalid_0's rmse: 7.83619\n",
            "[9900]\tvalid_0's rmse: 7.8361\n",
            "[10000]\tvalid_0's rmse: 7.83623\n",
            "[10100]\tvalid_0's rmse: 7.83591\n",
            "[10200]\tvalid_0's rmse: 7.83596\n",
            "[10300]\tvalid_0's rmse: 7.83584\n",
            "[10400]\tvalid_0's rmse: 7.83564\n",
            "[10500]\tvalid_0's rmse: 7.83555\n",
            "[10600]\tvalid_0's rmse: 7.83547\n",
            "[10700]\tvalid_0's rmse: 7.83552\n",
            "[10800]\tvalid_0's rmse: 7.83559\n",
            "[10900]\tvalid_0's rmse: 7.83558\n",
            "[11000]\tvalid_0's rmse: 7.8356\n",
            "[11100]\tvalid_0's rmse: 7.83556\n",
            "[11200]\tvalid_0's rmse: 7.83547\n",
            "[11300]\tvalid_0's rmse: 7.83528\n",
            "[11400]\tvalid_0's rmse: 7.83542\n",
            "[11500]\tvalid_0's rmse: 7.83546\n",
            "[11600]\tvalid_0's rmse: 7.8354\n",
            "[11700]\tvalid_0's rmse: 7.83532\n",
            "[11800]\tvalid_0's rmse: 7.83524\n",
            "[11900]\tvalid_0's rmse: 7.83521\n",
            "[12000]\tvalid_0's rmse: 7.83527\n",
            "[12100]\tvalid_0's rmse: 7.83526\n",
            "[12200]\tvalid_0's rmse: 7.8351\n",
            "[12300]\tvalid_0's rmse: 7.83507\n",
            "[12400]\tvalid_0's rmse: 7.8351\n",
            "[12500]\tvalid_0's rmse: 7.83515\n",
            "[12600]\tvalid_0's rmse: 7.83506\n",
            "[12700]\tvalid_0's rmse: 7.83489\n",
            "[12800]\tvalid_0's rmse: 7.8349\n",
            "[12900]\tvalid_0's rmse: 7.83477\n",
            "[13000]\tvalid_0's rmse: 7.8347\n",
            "[13100]\tvalid_0's rmse: 7.83461\n",
            "[13200]\tvalid_0's rmse: 7.83453\n",
            "[13300]\tvalid_0's rmse: 7.83444\n",
            "[13400]\tvalid_0's rmse: 7.83451\n",
            "[13500]\tvalid_0's rmse: 7.83459\n",
            "[13600]\tvalid_0's rmse: 7.83457\n",
            "[13700]\tvalid_0's rmse: 7.83455\n",
            "[13800]\tvalid_0's rmse: 7.83459\n",
            "[13900]\tvalid_0's rmse: 7.83437\n",
            "[14000]\tvalid_0's rmse: 7.8343\n",
            "[14100]\tvalid_0's rmse: 7.8343\n",
            "[14200]\tvalid_0's rmse: 7.83425\n",
            "[14300]\tvalid_0's rmse: 7.83442\n",
            "[14400]\tvalid_0's rmse: 7.8344\n",
            "[14500]\tvalid_0's rmse: 7.83442\n",
            "[14600]\tvalid_0's rmse: 7.83434\n",
            "[14700]\tvalid_0's rmse: 7.83417\n",
            "[14800]\tvalid_0's rmse: 7.83427\n",
            "[14900]\tvalid_0's rmse: 7.8343\n",
            "[15000]\tvalid_0's rmse: 7.83429\n",
            "[15100]\tvalid_0's rmse: 7.83431\n",
            "[15200]\tvalid_0's rmse: 7.83427\n",
            "[15300]\tvalid_0's rmse: 7.83412\n",
            "[15400]\tvalid_0's rmse: 7.83406\n",
            "[15500]\tvalid_0's rmse: 7.83422\n",
            "[15600]\tvalid_0's rmse: 7.83441\n",
            "[15700]\tvalid_0's rmse: 7.83446\n",
            "[15800]\tvalid_0's rmse: 7.83453\n",
            "[15900]\tvalid_0's rmse: 7.83453\n",
            "[16000]\tvalid_0's rmse: 7.83452\n",
            "[16100]\tvalid_0's rmse: 7.83456\n",
            "[16200]\tvalid_0's rmse: 7.83456\n",
            "[16300]\tvalid_0's rmse: 7.8345\n",
            "Early stopping, best iteration is:\n",
            "[15384]\tvalid_0's rmse: 7.83402\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055454 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score 6.814090\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.91655\n",
            "[200]\tvalid_0's rmse: 7.90744\n",
            "[300]\tvalid_0's rmse: 7.9007\n",
            "[400]\tvalid_0's rmse: 7.8954\n",
            "[500]\tvalid_0's rmse: 7.89088\n",
            "[600]\tvalid_0's rmse: 7.88669\n",
            "[700]\tvalid_0's rmse: 7.88319\n",
            "[800]\tvalid_0's rmse: 7.88016\n",
            "[900]\tvalid_0's rmse: 7.87731\n",
            "[1000]\tvalid_0's rmse: 7.87437\n",
            "[1100]\tvalid_0's rmse: 7.87192\n",
            "[1200]\tvalid_0's rmse: 7.8697\n",
            "[1300]\tvalid_0's rmse: 7.86763\n",
            "[1400]\tvalid_0's rmse: 7.86562\n",
            "[1500]\tvalid_0's rmse: 7.86369\n",
            "[1600]\tvalid_0's rmse: 7.86202\n",
            "[1700]\tvalid_0's rmse: 7.86038\n",
            "[1800]\tvalid_0's rmse: 7.8588\n",
            "[1900]\tvalid_0's rmse: 7.85714\n",
            "[2000]\tvalid_0's rmse: 7.85603\n",
            "[2100]\tvalid_0's rmse: 7.85464\n",
            "[2200]\tvalid_0's rmse: 7.85357\n",
            "[2300]\tvalid_0's rmse: 7.85254\n",
            "[2400]\tvalid_0's rmse: 7.85168\n",
            "[2500]\tvalid_0's rmse: 7.85058\n",
            "[2600]\tvalid_0's rmse: 7.84969\n",
            "[2700]\tvalid_0's rmse: 7.84892\n",
            "[2800]\tvalid_0's rmse: 7.84826\n",
            "[2900]\tvalid_0's rmse: 7.84753\n",
            "[3000]\tvalid_0's rmse: 7.84676\n",
            "[3100]\tvalid_0's rmse: 7.84608\n",
            "[3200]\tvalid_0's rmse: 7.84558\n",
            "[3300]\tvalid_0's rmse: 7.845\n",
            "[3400]\tvalid_0's rmse: 7.84474\n",
            "[3500]\tvalid_0's rmse: 7.84451\n",
            "[3600]\tvalid_0's rmse: 7.8442\n",
            "[3700]\tvalid_0's rmse: 7.8439\n",
            "[3800]\tvalid_0's rmse: 7.84363\n",
            "[3900]\tvalid_0's rmse: 7.84326\n",
            "[4000]\tvalid_0's rmse: 7.84307\n",
            "[4100]\tvalid_0's rmse: 7.84256\n",
            "[4200]\tvalid_0's rmse: 7.84219\n",
            "[4300]\tvalid_0's rmse: 7.84177\n",
            "[4400]\tvalid_0's rmse: 7.84156\n",
            "[4500]\tvalid_0's rmse: 7.84133\n",
            "[4600]\tvalid_0's rmse: 7.84113\n",
            "[4700]\tvalid_0's rmse: 7.84084\n",
            "[4800]\tvalid_0's rmse: 7.84054\n",
            "[4900]\tvalid_0's rmse: 7.84029\n",
            "[5000]\tvalid_0's rmse: 7.84016\n",
            "[5100]\tvalid_0's rmse: 7.84002\n",
            "[5200]\tvalid_0's rmse: 7.8398\n",
            "[5300]\tvalid_0's rmse: 7.83974\n",
            "[5400]\tvalid_0's rmse: 7.83945\n",
            "[5500]\tvalid_0's rmse: 7.83927\n",
            "[5600]\tvalid_0's rmse: 7.83902\n",
            "[5700]\tvalid_0's rmse: 7.83886\n",
            "[5800]\tvalid_0's rmse: 7.83862\n",
            "[5900]\tvalid_0's rmse: 7.83847\n",
            "[6000]\tvalid_0's rmse: 7.8384\n",
            "[6100]\tvalid_0's rmse: 7.83822\n",
            "[6200]\tvalid_0's rmse: 7.83802\n",
            "[6300]\tvalid_0's rmse: 7.83788\n",
            "[6400]\tvalid_0's rmse: 7.8379\n",
            "[6500]\tvalid_0's rmse: 7.83778\n",
            "[6600]\tvalid_0's rmse: 7.83778\n",
            "[6700]\tvalid_0's rmse: 7.83758\n",
            "[6800]\tvalid_0's rmse: 7.83743\n",
            "[6900]\tvalid_0's rmse: 7.83722\n",
            "[7000]\tvalid_0's rmse: 7.83706\n",
            "[7100]\tvalid_0's rmse: 7.83696\n",
            "[7200]\tvalid_0's rmse: 7.83685\n",
            "[7300]\tvalid_0's rmse: 7.83678\n",
            "[7400]\tvalid_0's rmse: 7.83664\n",
            "[7500]\tvalid_0's rmse: 7.8365\n",
            "[7600]\tvalid_0's rmse: 7.83633\n",
            "[7700]\tvalid_0's rmse: 7.8362\n",
            "[7800]\tvalid_0's rmse: 7.83623\n",
            "[7900]\tvalid_0's rmse: 7.83617\n",
            "[8000]\tvalid_0's rmse: 7.83609\n",
            "[8100]\tvalid_0's rmse: 7.83591\n",
            "[8200]\tvalid_0's rmse: 7.83581\n",
            "[8300]\tvalid_0's rmse: 7.8357\n",
            "[8400]\tvalid_0's rmse: 7.83557\n",
            "[8500]\tvalid_0's rmse: 7.83551\n",
            "[8600]\tvalid_0's rmse: 7.83527\n",
            "[8700]\tvalid_0's rmse: 7.83515\n",
            "[8800]\tvalid_0's rmse: 7.83506\n",
            "[8900]\tvalid_0's rmse: 7.8349\n",
            "[9000]\tvalid_0's rmse: 7.83487\n",
            "[9100]\tvalid_0's rmse: 7.83488\n",
            "[9200]\tvalid_0's rmse: 7.83501\n",
            "[9300]\tvalid_0's rmse: 7.83499\n",
            "[9400]\tvalid_0's rmse: 7.83483\n",
            "[9500]\tvalid_0's rmse: 7.83477\n",
            "[9600]\tvalid_0's rmse: 7.83485\n",
            "[9700]\tvalid_0's rmse: 7.83478\n",
            "[9800]\tvalid_0's rmse: 7.8346\n",
            "[9900]\tvalid_0's rmse: 7.83447\n",
            "[10000]\tvalid_0's rmse: 7.83435\n",
            "[10100]\tvalid_0's rmse: 7.83433\n",
            "[10200]\tvalid_0's rmse: 7.83416\n",
            "[10300]\tvalid_0's rmse: 7.83414\n",
            "[10400]\tvalid_0's rmse: 7.83408\n",
            "[10500]\tvalid_0's rmse: 7.83399\n",
            "[10600]\tvalid_0's rmse: 7.83398\n",
            "[10700]\tvalid_0's rmse: 7.83395\n",
            "[10800]\tvalid_0's rmse: 7.83399\n",
            "[10900]\tvalid_0's rmse: 7.83392\n",
            "[11000]\tvalid_0's rmse: 7.83388\n",
            "[11100]\tvalid_0's rmse: 7.83374\n",
            "[11200]\tvalid_0's rmse: 7.83377\n",
            "[11300]\tvalid_0's rmse: 7.83371\n",
            "[11400]\tvalid_0's rmse: 7.83354\n",
            "[11500]\tvalid_0's rmse: 7.83363\n",
            "[11600]\tvalid_0's rmse: 7.8337\n",
            "[11700]\tvalid_0's rmse: 7.83362\n",
            "[11800]\tvalid_0's rmse: 7.83346\n",
            "[11900]\tvalid_0's rmse: 7.8335\n",
            "[12000]\tvalid_0's rmse: 7.8335\n",
            "[12100]\tvalid_0's rmse: 7.83333\n",
            "[12200]\tvalid_0's rmse: 7.83327\n",
            "[12300]\tvalid_0's rmse: 7.83329\n",
            "[12400]\tvalid_0's rmse: 7.83323\n",
            "[12500]\tvalid_0's rmse: 7.83306\n",
            "[12600]\tvalid_0's rmse: 7.833\n",
            "[12700]\tvalid_0's rmse: 7.83298\n",
            "[12800]\tvalid_0's rmse: 7.83292\n",
            "[12900]\tvalid_0's rmse: 7.83291\n",
            "[13000]\tvalid_0's rmse: 7.83282\n",
            "[13100]\tvalid_0's rmse: 7.83275\n",
            "[13200]\tvalid_0's rmse: 7.8327\n",
            "[13300]\tvalid_0's rmse: 7.83271\n",
            "[13400]\tvalid_0's rmse: 7.83282\n",
            "[13500]\tvalid_0's rmse: 7.83279\n",
            "[13600]\tvalid_0's rmse: 7.83276\n",
            "[13700]\tvalid_0's rmse: 7.83268\n",
            "[13800]\tvalid_0's rmse: 7.8328\n",
            "[13900]\tvalid_0's rmse: 7.83275\n",
            "[14000]\tvalid_0's rmse: 7.83282\n",
            "[14100]\tvalid_0's rmse: 7.83288\n",
            "[14200]\tvalid_0's rmse: 7.83284\n",
            "Early stopping, best iteration is:\n",
            "[13236]\tvalid_0's rmse: 7.83264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GR3URbtFEt_",
        "outputId": "8b321510-d42c-4d8c-9d8a-ba57acb16f04"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "for model1 in models:\n",
        "  print(mean_squared_error(y_test,model1.predict(X_test))**0.5)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.424619527429068\n",
            "7.321665905917379\n",
            "7.219004646126214\n",
            "7.30231316335688\n",
            "7.372796883256406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znuxvA3bFNd4"
      },
      "source": [
        "test_final=pd.read_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\test.csv')\n",
        "id_index=test_final['id']\n",
        "test_final.drop('id',inplace=True,axis=1)\n",
        "test_final[dependent_columns]=scaler.transform(test_final[dependent_columns])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "HIJ-gTplInFK",
        "outputId": "68f35591-af92-4d35-8246-d8f698628fc8"
      },
      "source": [
        "test_final"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              f0        f1        f2        f3        f4        f5        f6  \\\n",
              "0       0.772720  0.110345  0.376834  0.184523  0.173535  0.499419  0.064781   \n",
              "1       0.227467  0.510345  0.418600  0.721692  0.168458  0.166490  0.390414   \n",
              "2       0.866474  0.124138  0.443734  0.753222  0.430279  0.513110  0.317988   \n",
              "3       0.815056  0.124138  0.415874  0.617083  0.332129  0.158931  0.113397   \n",
              "4       0.335133  0.365517  0.480299  0.520721  0.231542  0.199143  0.043051   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "149995  0.718738  0.324138  0.484727  0.117018  0.638746  0.517924  0.165717   \n",
              "149996  0.704383  0.200000  0.481585  0.702127  0.173504  0.365400  0.066795   \n",
              "149997  0.426323  0.548276  0.426724  0.202462  0.193775  0.362679  0.265731   \n",
              "149998  0.957795  0.117241  0.467765  0.170763  0.207358  0.307747  0.307419   \n",
              "149999  0.279176  0.227586  0.466495  0.288108  0.217450  0.627955  0.396323   \n",
              "\n",
              "              f7        f8        f9  ...       f90       f91       f92  \\\n",
              "0       0.623703  0.549884  0.231115  ...  0.217152  0.183102  0.031116   \n",
              "1       0.607138  0.558683  0.209289  ...  0.198947  0.343843  0.299171   \n",
              "2       0.507733  0.558186  0.280292  ...  0.360174  0.237500  0.021511   \n",
              "3       0.605291  0.519431  0.343690  ...  0.467159  0.333960  0.224251   \n",
              "4       0.645294  0.527027  0.300654  ...  0.326602  0.229583  0.574476   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "149995  0.468975  0.434149  0.263514  ...  0.504944  0.191246  0.214853   \n",
              "149996  0.587191  0.504573  0.230486  ...  0.220127  0.294597  0.153844   \n",
              "149997  0.615467  0.476152  0.604092  ...  0.202751  0.351704  0.206957   \n",
              "149998  0.450003  0.526416  0.375948  ...  0.206270  0.226907  0.509885   \n",
              "149999  0.633061  0.528837  0.378993  ...  0.328352  0.369813  0.249765   \n",
              "\n",
              "             f93       f94       f95       f96       f97       f98       f99  \n",
              "0       0.305497  0.775579  0.320757  0.458946  0.476499  0.395735  0.276237  \n",
              "1       0.334076  0.570809  0.431019  0.452454  0.221138  0.293963  0.362470  \n",
              "2       0.279352  0.365115  0.629283  0.442132  0.842200  0.580560  0.283877  \n",
              "3       0.253703  0.590433  0.377182  0.501562  0.807252  0.390389  0.163238  \n",
              "4       0.284815  0.458479  0.753800  0.568627  0.324205  0.424113  0.130487  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "149995  0.428160  0.395748  0.327021  0.074967  0.320961  0.405083  0.269552  \n",
              "149996  0.267596  0.882559  0.369859  0.506035  0.507415  0.490268  0.384148  \n",
              "149997  0.385754  0.639840  0.694837  0.527072  0.257226  0.577989  0.378089  \n",
              "149998  0.269716  0.458858  0.478258  0.547498  0.407763  0.447528  0.918093  \n",
              "149999  0.298401  0.313268  0.540254  0.500405  0.347515  0.429655  0.187573  \n",
              "\n",
              "[150000 rows x 100 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.772720</td>\n",
              "      <td>0.110345</td>\n",
              "      <td>0.376834</td>\n",
              "      <td>0.184523</td>\n",
              "      <td>0.173535</td>\n",
              "      <td>0.499419</td>\n",
              "      <td>0.064781</td>\n",
              "      <td>0.623703</td>\n",
              "      <td>0.549884</td>\n",
              "      <td>0.231115</td>\n",
              "      <td>...</td>\n",
              "      <td>0.217152</td>\n",
              "      <td>0.183102</td>\n",
              "      <td>0.031116</td>\n",
              "      <td>0.305497</td>\n",
              "      <td>0.775579</td>\n",
              "      <td>0.320757</td>\n",
              "      <td>0.458946</td>\n",
              "      <td>0.476499</td>\n",
              "      <td>0.395735</td>\n",
              "      <td>0.276237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.227467</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.418600</td>\n",
              "      <td>0.721692</td>\n",
              "      <td>0.168458</td>\n",
              "      <td>0.166490</td>\n",
              "      <td>0.390414</td>\n",
              "      <td>0.607138</td>\n",
              "      <td>0.558683</td>\n",
              "      <td>0.209289</td>\n",
              "      <td>...</td>\n",
              "      <td>0.198947</td>\n",
              "      <td>0.343843</td>\n",
              "      <td>0.299171</td>\n",
              "      <td>0.334076</td>\n",
              "      <td>0.570809</td>\n",
              "      <td>0.431019</td>\n",
              "      <td>0.452454</td>\n",
              "      <td>0.221138</td>\n",
              "      <td>0.293963</td>\n",
              "      <td>0.362470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.866474</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.443734</td>\n",
              "      <td>0.753222</td>\n",
              "      <td>0.430279</td>\n",
              "      <td>0.513110</td>\n",
              "      <td>0.317988</td>\n",
              "      <td>0.507733</td>\n",
              "      <td>0.558186</td>\n",
              "      <td>0.280292</td>\n",
              "      <td>...</td>\n",
              "      <td>0.360174</td>\n",
              "      <td>0.237500</td>\n",
              "      <td>0.021511</td>\n",
              "      <td>0.279352</td>\n",
              "      <td>0.365115</td>\n",
              "      <td>0.629283</td>\n",
              "      <td>0.442132</td>\n",
              "      <td>0.842200</td>\n",
              "      <td>0.580560</td>\n",
              "      <td>0.283877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.815056</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.415874</td>\n",
              "      <td>0.617083</td>\n",
              "      <td>0.332129</td>\n",
              "      <td>0.158931</td>\n",
              "      <td>0.113397</td>\n",
              "      <td>0.605291</td>\n",
              "      <td>0.519431</td>\n",
              "      <td>0.343690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.467159</td>\n",
              "      <td>0.333960</td>\n",
              "      <td>0.224251</td>\n",
              "      <td>0.253703</td>\n",
              "      <td>0.590433</td>\n",
              "      <td>0.377182</td>\n",
              "      <td>0.501562</td>\n",
              "      <td>0.807252</td>\n",
              "      <td>0.390389</td>\n",
              "      <td>0.163238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.335133</td>\n",
              "      <td>0.365517</td>\n",
              "      <td>0.480299</td>\n",
              "      <td>0.520721</td>\n",
              "      <td>0.231542</td>\n",
              "      <td>0.199143</td>\n",
              "      <td>0.043051</td>\n",
              "      <td>0.645294</td>\n",
              "      <td>0.527027</td>\n",
              "      <td>0.300654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326602</td>\n",
              "      <td>0.229583</td>\n",
              "      <td>0.574476</td>\n",
              "      <td>0.284815</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.753800</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.324205</td>\n",
              "      <td>0.424113</td>\n",
              "      <td>0.130487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>0.718738</td>\n",
              "      <td>0.324138</td>\n",
              "      <td>0.484727</td>\n",
              "      <td>0.117018</td>\n",
              "      <td>0.638746</td>\n",
              "      <td>0.517924</td>\n",
              "      <td>0.165717</td>\n",
              "      <td>0.468975</td>\n",
              "      <td>0.434149</td>\n",
              "      <td>0.263514</td>\n",
              "      <td>...</td>\n",
              "      <td>0.504944</td>\n",
              "      <td>0.191246</td>\n",
              "      <td>0.214853</td>\n",
              "      <td>0.428160</td>\n",
              "      <td>0.395748</td>\n",
              "      <td>0.327021</td>\n",
              "      <td>0.074967</td>\n",
              "      <td>0.320961</td>\n",
              "      <td>0.405083</td>\n",
              "      <td>0.269552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>0.704383</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.481585</td>\n",
              "      <td>0.702127</td>\n",
              "      <td>0.173504</td>\n",
              "      <td>0.365400</td>\n",
              "      <td>0.066795</td>\n",
              "      <td>0.587191</td>\n",
              "      <td>0.504573</td>\n",
              "      <td>0.230486</td>\n",
              "      <td>...</td>\n",
              "      <td>0.220127</td>\n",
              "      <td>0.294597</td>\n",
              "      <td>0.153844</td>\n",
              "      <td>0.267596</td>\n",
              "      <td>0.882559</td>\n",
              "      <td>0.369859</td>\n",
              "      <td>0.506035</td>\n",
              "      <td>0.507415</td>\n",
              "      <td>0.490268</td>\n",
              "      <td>0.384148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>0.426323</td>\n",
              "      <td>0.548276</td>\n",
              "      <td>0.426724</td>\n",
              "      <td>0.202462</td>\n",
              "      <td>0.193775</td>\n",
              "      <td>0.362679</td>\n",
              "      <td>0.265731</td>\n",
              "      <td>0.615467</td>\n",
              "      <td>0.476152</td>\n",
              "      <td>0.604092</td>\n",
              "      <td>...</td>\n",
              "      <td>0.202751</td>\n",
              "      <td>0.351704</td>\n",
              "      <td>0.206957</td>\n",
              "      <td>0.385754</td>\n",
              "      <td>0.639840</td>\n",
              "      <td>0.694837</td>\n",
              "      <td>0.527072</td>\n",
              "      <td>0.257226</td>\n",
              "      <td>0.577989</td>\n",
              "      <td>0.378089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>0.957795</td>\n",
              "      <td>0.117241</td>\n",
              "      <td>0.467765</td>\n",
              "      <td>0.170763</td>\n",
              "      <td>0.207358</td>\n",
              "      <td>0.307747</td>\n",
              "      <td>0.307419</td>\n",
              "      <td>0.450003</td>\n",
              "      <td>0.526416</td>\n",
              "      <td>0.375948</td>\n",
              "      <td>...</td>\n",
              "      <td>0.206270</td>\n",
              "      <td>0.226907</td>\n",
              "      <td>0.509885</td>\n",
              "      <td>0.269716</td>\n",
              "      <td>0.458858</td>\n",
              "      <td>0.478258</td>\n",
              "      <td>0.547498</td>\n",
              "      <td>0.407763</td>\n",
              "      <td>0.447528</td>\n",
              "      <td>0.918093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>0.279176</td>\n",
              "      <td>0.227586</td>\n",
              "      <td>0.466495</td>\n",
              "      <td>0.288108</td>\n",
              "      <td>0.217450</td>\n",
              "      <td>0.627955</td>\n",
              "      <td>0.396323</td>\n",
              "      <td>0.633061</td>\n",
              "      <td>0.528837</td>\n",
              "      <td>0.378993</td>\n",
              "      <td>...</td>\n",
              "      <td>0.328352</td>\n",
              "      <td>0.369813</td>\n",
              "      <td>0.249765</td>\n",
              "      <td>0.298401</td>\n",
              "      <td>0.313268</td>\n",
              "      <td>0.540254</td>\n",
              "      <td>0.500405</td>\n",
              "      <td>0.347515</td>\n",
              "      <td>0.429655</td>\n",
              "      <td>0.187573</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows × 100 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7HaqMbkDvMR"
      },
      "source": [
        "lpred=[]\n",
        "spl=5\n",
        "for model in models:\n",
        "   lpred.append(model.predict(test_final[dependent_columns]))\n",
        "pred=lpred[0]\n",
        "for i in range(1,spl):\n",
        "   pred += lpred[i]\n",
        "test_pred = pred /spl"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c9WkjbkD29e"
      },
      "source": [
        "test_pred=pd.DataFrame(test_pred)\n",
        "test_pred.columns=['loss']"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMvy4MEHEB0e"
      },
      "source": [
        "test_pred['id']=id_index"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npK_4IBnEE9-"
      },
      "source": [
        "test_pred=test_pred[['id','loss']]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC96uS-5EJA0",
        "outputId": "32410257-c01e-4b8b-bdf9-c63caef9c6e6"
      },
      "source": [
        "%%time\n",
        "test_pred.to_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\submit.csv',index=False)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 314 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAO9HkRIExSE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}