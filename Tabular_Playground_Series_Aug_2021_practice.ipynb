{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular_Playground_Series_Aug_2021_practice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuuOXcEQ58Rs2hnyQexS0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkat2ram/AI_Playground/blob/main/Tabular_Playground_Series_Aug_2021_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rl7qXxUfKnE"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_HzVy6DfZN6"
      },
      "source": [
        "dataset = pd.read_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\train.csv')\n",
        "#dataset = pd.read_csv(r'/content/train.csv')\n",
        "pred_dataset=pd.read_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\test.csv')\n",
        "#pred_dataset=pd.read_csv(r'/content/test.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQrIpGD3fnSY"
      },
      "source": [
        "dataset.drop('id',inplace=True,axis=1)\n",
        "pred_dataset.drop('id',inplace=True,axis=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "ELmm9x_C5Hqu",
        "outputId": "edd415ed-1925-4943-99fe-88f52fb5c82a"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f0   f1        f2        f3          f4        f5        f6  \\\n",
              "0 -0.002350   59  0.766739 -1.350460     42.2727  16.68570   30.3599   \n",
              "1  0.784462  145 -0.463845 -0.530421  27324.9000   3.47545  160.4980   \n",
              "2  0.317816   19 -0.432571 -0.382644   1383.2600  19.71290   31.1026   \n",
              "3  0.210753   17 -0.616454  0.946362   -119.2530   4.08235  185.2570   \n",
              "4  0.439671   20  0.968126 -0.092546     74.3020  12.30650   72.1860   \n",
              "\n",
              "         f7         f8       f9  ...        f91        f92      f93       f94  \\\n",
              "0  1.267300   0.392007  1.09101  ...  -42.43990  26.854000  1.45751  0.696161   \n",
              "1  0.828007   3.735860  1.28138  ... -184.13200   7.901370  1.70644 -0.494699   \n",
              "2 -0.515354  34.430800  1.24210  ...    7.43721  37.218100  3.25339  0.337934   \n",
              "3  1.383310 -47.521400  1.09130  ...    9.66778   0.626942  1.49425  0.517513   \n",
              "4 -0.233964  24.399100  1.10151  ...  290.65700  15.604300  1.73557 -0.476668   \n",
              "\n",
              "         f95       f96       f97      f98       f99  loss  \n",
              "0   0.941764  1.828470  0.924090  2.29658  10.48980    15  \n",
              "1  -2.058300  0.819184  0.439152  2.36470   1.14383     3  \n",
              "2   0.615037  2.216760  0.745268  1.69679  12.30550     6  \n",
              "3 -10.222100  2.627310  0.617270  1.45645  10.02880     2  \n",
              "4   1.390190  2.195740  0.826987  1.78485   7.07197     1  \n",
              "\n",
              "[5 rows x 101 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.002350</td>\n",
              "      <td>59</td>\n",
              "      <td>0.766739</td>\n",
              "      <td>-1.350460</td>\n",
              "      <td>42.2727</td>\n",
              "      <td>16.68570</td>\n",
              "      <td>30.3599</td>\n",
              "      <td>1.267300</td>\n",
              "      <td>0.392007</td>\n",
              "      <td>1.09101</td>\n",
              "      <td>...</td>\n",
              "      <td>-42.43990</td>\n",
              "      <td>26.854000</td>\n",
              "      <td>1.45751</td>\n",
              "      <td>0.696161</td>\n",
              "      <td>0.941764</td>\n",
              "      <td>1.828470</td>\n",
              "      <td>0.924090</td>\n",
              "      <td>2.29658</td>\n",
              "      <td>10.48980</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.784462</td>\n",
              "      <td>145</td>\n",
              "      <td>-0.463845</td>\n",
              "      <td>-0.530421</td>\n",
              "      <td>27324.9000</td>\n",
              "      <td>3.47545</td>\n",
              "      <td>160.4980</td>\n",
              "      <td>0.828007</td>\n",
              "      <td>3.735860</td>\n",
              "      <td>1.28138</td>\n",
              "      <td>...</td>\n",
              "      <td>-184.13200</td>\n",
              "      <td>7.901370</td>\n",
              "      <td>1.70644</td>\n",
              "      <td>-0.494699</td>\n",
              "      <td>-2.058300</td>\n",
              "      <td>0.819184</td>\n",
              "      <td>0.439152</td>\n",
              "      <td>2.36470</td>\n",
              "      <td>1.14383</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.317816</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.432571</td>\n",
              "      <td>-0.382644</td>\n",
              "      <td>1383.2600</td>\n",
              "      <td>19.71290</td>\n",
              "      <td>31.1026</td>\n",
              "      <td>-0.515354</td>\n",
              "      <td>34.430800</td>\n",
              "      <td>1.24210</td>\n",
              "      <td>...</td>\n",
              "      <td>7.43721</td>\n",
              "      <td>37.218100</td>\n",
              "      <td>3.25339</td>\n",
              "      <td>0.337934</td>\n",
              "      <td>0.615037</td>\n",
              "      <td>2.216760</td>\n",
              "      <td>0.745268</td>\n",
              "      <td>1.69679</td>\n",
              "      <td>12.30550</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.210753</td>\n",
              "      <td>17</td>\n",
              "      <td>-0.616454</td>\n",
              "      <td>0.946362</td>\n",
              "      <td>-119.2530</td>\n",
              "      <td>4.08235</td>\n",
              "      <td>185.2570</td>\n",
              "      <td>1.383310</td>\n",
              "      <td>-47.521400</td>\n",
              "      <td>1.09130</td>\n",
              "      <td>...</td>\n",
              "      <td>9.66778</td>\n",
              "      <td>0.626942</td>\n",
              "      <td>1.49425</td>\n",
              "      <td>0.517513</td>\n",
              "      <td>-10.222100</td>\n",
              "      <td>2.627310</td>\n",
              "      <td>0.617270</td>\n",
              "      <td>1.45645</td>\n",
              "      <td>10.02880</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.439671</td>\n",
              "      <td>20</td>\n",
              "      <td>0.968126</td>\n",
              "      <td>-0.092546</td>\n",
              "      <td>74.3020</td>\n",
              "      <td>12.30650</td>\n",
              "      <td>72.1860</td>\n",
              "      <td>-0.233964</td>\n",
              "      <td>24.399100</td>\n",
              "      <td>1.10151</td>\n",
              "      <td>...</td>\n",
              "      <td>290.65700</td>\n",
              "      <td>15.604300</td>\n",
              "      <td>1.73557</td>\n",
              "      <td>-0.476668</td>\n",
              "      <td>1.390190</td>\n",
              "      <td>2.195740</td>\n",
              "      <td>0.826987</td>\n",
              "      <td>1.78485</td>\n",
              "      <td>7.07197</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "3m4HsYgY5wHu",
        "outputId": "15677e6b-f440-4667-e260-873027bc1fec"
      },
      "source": [
        "pred_dataset.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f0   f1        f2        f3          f4        f5         f6  \\\n",
              "0  0.812665   15 -1.239120 -0.893251    295.5770  15.87120   23.04360   \n",
              "1  0.190344  131 -0.501361  0.801921     64.8866   3.09703  344.80500   \n",
              "2  0.919671   19 -0.057382  0.901419  11961.2000  16.39650  273.24000   \n",
              "3  0.860985   19 -0.549509  0.471799   7501.6000   2.80698   71.08170   \n",
              "4  0.313229   89  0.588509  0.167705   2931.2600   4.34986    1.57187   \n",
              "\n",
              "         f7         f8       f9  ...       f90        f91       f92      f93  \\\n",
              "0  0.942256  29.898000  1.11394  ...  0.446389   -422.332  -1.44630  1.69075   \n",
              "1  0.807194  38.421900  1.09695  ...  0.377179  10352.200  21.06270  1.84351   \n",
              "2 -0.003300  37.940000  1.15222  ...  0.990140   3224.020  -2.25287  1.55100   \n",
              "3  0.792136   0.395235  1.20157  ...  1.396880   9689.760  14.77150  1.41390   \n",
              "4  1.118300   7.754630  1.16807  ...  0.862502   2693.350  44.18050  1.58020   \n",
              "\n",
              "        f94        f95      f96       f97       f98       f99  \n",
              "0  1.059300  -3.010570  1.94664  0.529470  1.386950   8.78767  \n",
              "1  0.251895   4.440570  1.90309  0.248534  0.863881  11.79390  \n",
              "2 -0.559157  17.838600  1.83385  0.931796  2.336870   9.05400  \n",
              "3  0.329272   0.802437  2.23251  0.893348  1.359470   4.84833  \n",
              "4 -0.191021  26.253000  2.68238  0.361923  1.532800   3.70660  \n",
              "\n",
              "[5 rows x 100 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.812665</td>\n",
              "      <td>15</td>\n",
              "      <td>-1.239120</td>\n",
              "      <td>-0.893251</td>\n",
              "      <td>295.5770</td>\n",
              "      <td>15.87120</td>\n",
              "      <td>23.04360</td>\n",
              "      <td>0.942256</td>\n",
              "      <td>29.898000</td>\n",
              "      <td>1.11394</td>\n",
              "      <td>...</td>\n",
              "      <td>0.446389</td>\n",
              "      <td>-422.332</td>\n",
              "      <td>-1.44630</td>\n",
              "      <td>1.69075</td>\n",
              "      <td>1.059300</td>\n",
              "      <td>-3.010570</td>\n",
              "      <td>1.94664</td>\n",
              "      <td>0.529470</td>\n",
              "      <td>1.386950</td>\n",
              "      <td>8.78767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.190344</td>\n",
              "      <td>131</td>\n",
              "      <td>-0.501361</td>\n",
              "      <td>0.801921</td>\n",
              "      <td>64.8866</td>\n",
              "      <td>3.09703</td>\n",
              "      <td>344.80500</td>\n",
              "      <td>0.807194</td>\n",
              "      <td>38.421900</td>\n",
              "      <td>1.09695</td>\n",
              "      <td>...</td>\n",
              "      <td>0.377179</td>\n",
              "      <td>10352.200</td>\n",
              "      <td>21.06270</td>\n",
              "      <td>1.84351</td>\n",
              "      <td>0.251895</td>\n",
              "      <td>4.440570</td>\n",
              "      <td>1.90309</td>\n",
              "      <td>0.248534</td>\n",
              "      <td>0.863881</td>\n",
              "      <td>11.79390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.919671</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.057382</td>\n",
              "      <td>0.901419</td>\n",
              "      <td>11961.2000</td>\n",
              "      <td>16.39650</td>\n",
              "      <td>273.24000</td>\n",
              "      <td>-0.003300</td>\n",
              "      <td>37.940000</td>\n",
              "      <td>1.15222</td>\n",
              "      <td>...</td>\n",
              "      <td>0.990140</td>\n",
              "      <td>3224.020</td>\n",
              "      <td>-2.25287</td>\n",
              "      <td>1.55100</td>\n",
              "      <td>-0.559157</td>\n",
              "      <td>17.838600</td>\n",
              "      <td>1.83385</td>\n",
              "      <td>0.931796</td>\n",
              "      <td>2.336870</td>\n",
              "      <td>9.05400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.860985</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.549509</td>\n",
              "      <td>0.471799</td>\n",
              "      <td>7501.6000</td>\n",
              "      <td>2.80698</td>\n",
              "      <td>71.08170</td>\n",
              "      <td>0.792136</td>\n",
              "      <td>0.395235</td>\n",
              "      <td>1.20157</td>\n",
              "      <td>...</td>\n",
              "      <td>1.396880</td>\n",
              "      <td>9689.760</td>\n",
              "      <td>14.77150</td>\n",
              "      <td>1.41390</td>\n",
              "      <td>0.329272</td>\n",
              "      <td>0.802437</td>\n",
              "      <td>2.23251</td>\n",
              "      <td>0.893348</td>\n",
              "      <td>1.359470</td>\n",
              "      <td>4.84833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.313229</td>\n",
              "      <td>89</td>\n",
              "      <td>0.588509</td>\n",
              "      <td>0.167705</td>\n",
              "      <td>2931.2600</td>\n",
              "      <td>4.34986</td>\n",
              "      <td>1.57187</td>\n",
              "      <td>1.118300</td>\n",
              "      <td>7.754630</td>\n",
              "      <td>1.16807</td>\n",
              "      <td>...</td>\n",
              "      <td>0.862502</td>\n",
              "      <td>2693.350</td>\n",
              "      <td>44.18050</td>\n",
              "      <td>1.58020</td>\n",
              "      <td>-0.191021</td>\n",
              "      <td>26.253000</td>\n",
              "      <td>2.68238</td>\n",
              "      <td>0.361923</td>\n",
              "      <td>1.532800</td>\n",
              "      <td>3.70660</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbsWNwhW5Ijy"
      },
      "source": [
        "dependent_columns=pred_dataset.columns"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5lhOMEA6ncx"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler((0,1))\n",
        "scaler.fit(dataset[dependent_columns])\n",
        "dataset[dependent_columns]=scaler.transform(dataset[dependent_columns])\n",
        "pred_dataset[dependent_columns]=scaler.transform(pred_dataset[dependent_columns])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "GVS0ZJGm72_7",
        "outputId": "ec21f759-3444-433e-c81d-0e6497b75c4d"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              f0        f1        f2        f3        f4        f5        f6  \\\n",
              "0       0.058636  0.262069  0.490389  0.039642  0.167960  0.520648  0.072185   \n",
              "1       0.748009  0.558621  0.420724  0.299497  0.768412  0.176353  0.203889   \n",
              "2       0.339152  0.124138  0.422494  0.346325  0.197473  0.599544  0.072937   \n",
              "3       0.245348  0.117241  0.412084  0.767463  0.164405  0.192171  0.228946   \n",
              "4       0.445917  0.127586  0.501790  0.438252  0.168665  0.406514  0.114515   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "249995  0.870249  0.079310  0.484528  0.450112  0.167175  0.118774  0.082395   \n",
              "249996  0.274089  0.082759  0.415447  0.890131  0.165821  0.544036  0.256987   \n",
              "249997  0.101018  0.241379  0.473186  0.690674  0.212432  0.465809  0.053025   \n",
              "249998  0.916993  0.100000  0.390208  0.650222  0.271376  0.375093  0.045322   \n",
              "249999  0.274682  0.234483  0.488270  0.244940  0.171375  0.581590  0.084303   \n",
              "\n",
              "              f7        f8        f9  ...       f91       f92       f93  \\\n",
              "0       0.663569  0.519427  0.201658  ...  0.188769  0.368139  0.261862   \n",
              "1       0.609690  0.522879  0.446219  ...  0.186655  0.142436  0.308433   \n",
              "2       0.444931  0.554563  0.395758  ...  0.189513  0.491562  0.597841   \n",
              "3       0.677797  0.469969  0.202030  ...  0.189547  0.055806  0.268735   \n",
              "4       0.479443  0.544208  0.215147  ...  0.193739  0.234168  0.313882   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "249995  0.612679  0.545368  0.242677  ...  0.241886  0.269060  0.254725   \n",
              "249996  0.360378  0.386499  0.333118  ...  0.218431  0.280349  0.288654   \n",
              "249997  0.445611  0.602446  0.346594  ...  0.188978  0.179434  0.290540   \n",
              "249998  0.583712  0.641758  0.732520  ...  0.185292  0.310163  0.580464   \n",
              "249999  0.565191  0.399904  0.260418  ...  0.269773  0.507717  0.666021   \n",
              "\n",
              "             f94       f95       f96       f97       f98       f99  loss  \n",
              "0       0.683482  0.379244  0.441330  0.835195  0.572721  0.325062    15  \n",
              "1       0.381462  0.334849  0.290870  0.394403  0.585975  0.056975     3  \n",
              "2       0.592630  0.374409  0.499214  0.672652  0.456020  0.377146     6  \n",
              "3       0.638174  0.214041  0.560417  0.556307  0.409258  0.311839     2  \n",
              "4       0.386035  0.385880  0.496081  0.746932  0.473154  0.227023     1  \n",
              "...          ...       ...       ...       ...       ...       ...   ...  \n",
              "249995  0.660997  0.438570  0.594125  0.869547  0.333680  0.065403    11  \n",
              "249996  0.467646  0.449557  0.466087  0.686884  0.403699  0.059352     5  \n",
              "249997  0.417925  0.779458  0.319717  0.750155  0.321512  0.037225     1  \n",
              "249998  0.702730  0.403668  0.755847  0.629169  0.313121  0.236888    10  \n",
              "249999  0.368713  0.350692  0.447806  0.406551  0.307880  0.853506     7  \n",
              "\n",
              "[250000 rows x 101 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.058636</td>\n",
              "      <td>0.262069</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.039642</td>\n",
              "      <td>0.167960</td>\n",
              "      <td>0.520648</td>\n",
              "      <td>0.072185</td>\n",
              "      <td>0.663569</td>\n",
              "      <td>0.519427</td>\n",
              "      <td>0.201658</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188769</td>\n",
              "      <td>0.368139</td>\n",
              "      <td>0.261862</td>\n",
              "      <td>0.683482</td>\n",
              "      <td>0.379244</td>\n",
              "      <td>0.441330</td>\n",
              "      <td>0.835195</td>\n",
              "      <td>0.572721</td>\n",
              "      <td>0.325062</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.748009</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.420724</td>\n",
              "      <td>0.299497</td>\n",
              "      <td>0.768412</td>\n",
              "      <td>0.176353</td>\n",
              "      <td>0.203889</td>\n",
              "      <td>0.609690</td>\n",
              "      <td>0.522879</td>\n",
              "      <td>0.446219</td>\n",
              "      <td>...</td>\n",
              "      <td>0.186655</td>\n",
              "      <td>0.142436</td>\n",
              "      <td>0.308433</td>\n",
              "      <td>0.381462</td>\n",
              "      <td>0.334849</td>\n",
              "      <td>0.290870</td>\n",
              "      <td>0.394403</td>\n",
              "      <td>0.585975</td>\n",
              "      <td>0.056975</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.339152</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.422494</td>\n",
              "      <td>0.346325</td>\n",
              "      <td>0.197473</td>\n",
              "      <td>0.599544</td>\n",
              "      <td>0.072937</td>\n",
              "      <td>0.444931</td>\n",
              "      <td>0.554563</td>\n",
              "      <td>0.395758</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189513</td>\n",
              "      <td>0.491562</td>\n",
              "      <td>0.597841</td>\n",
              "      <td>0.592630</td>\n",
              "      <td>0.374409</td>\n",
              "      <td>0.499214</td>\n",
              "      <td>0.672652</td>\n",
              "      <td>0.456020</td>\n",
              "      <td>0.377146</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.245348</td>\n",
              "      <td>0.117241</td>\n",
              "      <td>0.412084</td>\n",
              "      <td>0.767463</td>\n",
              "      <td>0.164405</td>\n",
              "      <td>0.192171</td>\n",
              "      <td>0.228946</td>\n",
              "      <td>0.677797</td>\n",
              "      <td>0.469969</td>\n",
              "      <td>0.202030</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189547</td>\n",
              "      <td>0.055806</td>\n",
              "      <td>0.268735</td>\n",
              "      <td>0.638174</td>\n",
              "      <td>0.214041</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.556307</td>\n",
              "      <td>0.409258</td>\n",
              "      <td>0.311839</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.445917</td>\n",
              "      <td>0.127586</td>\n",
              "      <td>0.501790</td>\n",
              "      <td>0.438252</td>\n",
              "      <td>0.168665</td>\n",
              "      <td>0.406514</td>\n",
              "      <td>0.114515</td>\n",
              "      <td>0.479443</td>\n",
              "      <td>0.544208</td>\n",
              "      <td>0.215147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.193739</td>\n",
              "      <td>0.234168</td>\n",
              "      <td>0.313882</td>\n",
              "      <td>0.386035</td>\n",
              "      <td>0.385880</td>\n",
              "      <td>0.496081</td>\n",
              "      <td>0.746932</td>\n",
              "      <td>0.473154</td>\n",
              "      <td>0.227023</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249995</th>\n",
              "      <td>0.870249</td>\n",
              "      <td>0.079310</td>\n",
              "      <td>0.484528</td>\n",
              "      <td>0.450112</td>\n",
              "      <td>0.167175</td>\n",
              "      <td>0.118774</td>\n",
              "      <td>0.082395</td>\n",
              "      <td>0.612679</td>\n",
              "      <td>0.545368</td>\n",
              "      <td>0.242677</td>\n",
              "      <td>...</td>\n",
              "      <td>0.241886</td>\n",
              "      <td>0.269060</td>\n",
              "      <td>0.254725</td>\n",
              "      <td>0.660997</td>\n",
              "      <td>0.438570</td>\n",
              "      <td>0.594125</td>\n",
              "      <td>0.869547</td>\n",
              "      <td>0.333680</td>\n",
              "      <td>0.065403</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249996</th>\n",
              "      <td>0.274089</td>\n",
              "      <td>0.082759</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.890131</td>\n",
              "      <td>0.165821</td>\n",
              "      <td>0.544036</td>\n",
              "      <td>0.256987</td>\n",
              "      <td>0.360378</td>\n",
              "      <td>0.386499</td>\n",
              "      <td>0.333118</td>\n",
              "      <td>...</td>\n",
              "      <td>0.218431</td>\n",
              "      <td>0.280349</td>\n",
              "      <td>0.288654</td>\n",
              "      <td>0.467646</td>\n",
              "      <td>0.449557</td>\n",
              "      <td>0.466087</td>\n",
              "      <td>0.686884</td>\n",
              "      <td>0.403699</td>\n",
              "      <td>0.059352</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249997</th>\n",
              "      <td>0.101018</td>\n",
              "      <td>0.241379</td>\n",
              "      <td>0.473186</td>\n",
              "      <td>0.690674</td>\n",
              "      <td>0.212432</td>\n",
              "      <td>0.465809</td>\n",
              "      <td>0.053025</td>\n",
              "      <td>0.445611</td>\n",
              "      <td>0.602446</td>\n",
              "      <td>0.346594</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188978</td>\n",
              "      <td>0.179434</td>\n",
              "      <td>0.290540</td>\n",
              "      <td>0.417925</td>\n",
              "      <td>0.779458</td>\n",
              "      <td>0.319717</td>\n",
              "      <td>0.750155</td>\n",
              "      <td>0.321512</td>\n",
              "      <td>0.037225</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249998</th>\n",
              "      <td>0.916993</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.390208</td>\n",
              "      <td>0.650222</td>\n",
              "      <td>0.271376</td>\n",
              "      <td>0.375093</td>\n",
              "      <td>0.045322</td>\n",
              "      <td>0.583712</td>\n",
              "      <td>0.641758</td>\n",
              "      <td>0.732520</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185292</td>\n",
              "      <td>0.310163</td>\n",
              "      <td>0.580464</td>\n",
              "      <td>0.702730</td>\n",
              "      <td>0.403668</td>\n",
              "      <td>0.755847</td>\n",
              "      <td>0.629169</td>\n",
              "      <td>0.313121</td>\n",
              "      <td>0.236888</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249999</th>\n",
              "      <td>0.274682</td>\n",
              "      <td>0.234483</td>\n",
              "      <td>0.488270</td>\n",
              "      <td>0.244940</td>\n",
              "      <td>0.171375</td>\n",
              "      <td>0.581590</td>\n",
              "      <td>0.084303</td>\n",
              "      <td>0.565191</td>\n",
              "      <td>0.399904</td>\n",
              "      <td>0.260418</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269773</td>\n",
              "      <td>0.507717</td>\n",
              "      <td>0.666021</td>\n",
              "      <td>0.368713</td>\n",
              "      <td>0.350692</td>\n",
              "      <td>0.447806</td>\n",
              "      <td>0.406551</td>\n",
              "      <td>0.307880</td>\n",
              "      <td>0.853506</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250000 rows × 101 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z8R4hGm51Tb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(dataset[dependent_columns],dataset['loss'],test_size=0.3,random_state=True,shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYxTrjW9K2uc"
      },
      "source": [
        "#**Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVyTWKstP2HH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1c548a-c4a1-4177-a010-c0ca9860fd4a"
      },
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin=LinearRegression()\n",
        "lin.fit(X_train,y_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mXQWz3wOkCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0f1ffa-3ac4-4066-e8ff-eff66a325253"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dct=DecisionTreeClassifier()\n",
        "dct.fit(X_train,y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR2ANNTSK1qF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ffc910-e5df-43df-e779-68e3cd28cc5f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc=RandomForestClassifier(n_estimators=40)\n",
        "rfc.fit(X_train,y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Oig3FYLUis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4a4126-7f69-40a6-e09c-70b81163ae86"
      },
      "source": [
        "mean_squared_error(y_test,lin.predict(X_test))**0.5"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.902357673033055"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_ACN1KrN6io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64013f4d-69c9-4088-dc3e-517956ff0ed3"
      },
      "source": [
        "lin.predict(X_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.20759196, 7.14726112, 7.19272219, ..., 7.37001613, 6.44305194,\n",
              "       6.35796864])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9ODCGILOB47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08c2c70-155d-4761-9acf-e9c14b40796b"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "240208    18\n",
              "18744      1\n",
              "207175     9\n",
              "18669      2\n",
              "189086     0\n",
              "          ..\n",
              "64115     20\n",
              "143448     0\n",
              "225604     5\n",
              "5328      13\n",
              "180420     9\n",
              "Name: loss, Length: 75000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3AumTS3Nvsg"
      },
      "source": [
        "appending the prediction from above model to datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr3jAwzZLUfG"
      },
      "source": [
        "dataset['extra']=lin.predict(dataset[dependent_columns])\n",
        "pred_dataset['extra']=lin.predict(pred_dataset[dependent_columns])\n",
        "scaler.fit(np.array(dataset['extra']).reshape(-1,1))\n",
        "dataset['extra']=scaler.transform(np.array(dataset['extra']).reshape(-1,1))\n",
        "pred_dataset['extra']=scaler.transform(np.array(pred_dataset['extra']).reshape(-1,1))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyRp4sQBLUch",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "292f4af9-4993-4543-f25e-cf32570428db"
      },
      "source": [
        "pred_dataset"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              f0        f1        f2        f3        f4        f5        f6  \\\n",
              "0       0.772720  0.110345  0.376834  0.184523  0.173535  0.499419  0.064781   \n",
              "1       0.227467  0.510345  0.418600  0.721692  0.168458  0.166490  0.390414   \n",
              "2       0.866474  0.124138  0.443734  0.753222  0.430279  0.513110  0.317988   \n",
              "3       0.815056  0.124138  0.415874  0.617083  0.332129  0.158931  0.113397   \n",
              "4       0.335133  0.365517  0.480299  0.520721  0.231542  0.199143  0.043051   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "149995  0.718738  0.324138  0.484727  0.117018  0.638746  0.517924  0.165717   \n",
              "149996  0.704383  0.200000  0.481585  0.702127  0.173504  0.365400  0.066795   \n",
              "149997  0.426323  0.548276  0.426724  0.202462  0.193775  0.362679  0.265731   \n",
              "149998  0.957795  0.117241  0.467765  0.170763  0.207358  0.307747  0.307419   \n",
              "149999  0.279176  0.227586  0.466495  0.288108  0.217450  0.627955  0.396323   \n",
              "\n",
              "              f7        f8        f9  ...       f91       f92       f93  \\\n",
              "0       0.623703  0.549884  0.231115  ...  0.183102  0.031116  0.305497   \n",
              "1       0.607138  0.558683  0.209289  ...  0.343843  0.299171  0.334076   \n",
              "2       0.507733  0.558186  0.280292  ...  0.237500  0.021511  0.279352   \n",
              "3       0.605291  0.519431  0.343690  ...  0.333960  0.224251  0.253703   \n",
              "4       0.645294  0.527027  0.300654  ...  0.229583  0.574476  0.284815   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "149995  0.468975  0.434149  0.263514  ...  0.191246  0.214853  0.428160   \n",
              "149996  0.587191  0.504573  0.230486  ...  0.294597  0.153844  0.267596   \n",
              "149997  0.615467  0.476152  0.604092  ...  0.351704  0.206957  0.385754   \n",
              "149998  0.450003  0.526416  0.375948  ...  0.226907  0.509885  0.269716   \n",
              "149999  0.633061  0.528837  0.378993  ...  0.369813  0.249765  0.298401   \n",
              "\n",
              "             f94       f95       f96       f97       f98       f99     extra  \n",
              "0       0.775579  0.320757  0.458946  0.476499  0.395735  0.276237  0.484618  \n",
              "1       0.570809  0.431019  0.452454  0.221138  0.293963  0.362470  0.286629  \n",
              "2       0.365115  0.629283  0.442132  0.842200  0.580560  0.283877  0.532077  \n",
              "3       0.590433  0.377182  0.501562  0.807252  0.390389  0.163238  0.413293  \n",
              "4       0.458479  0.753800  0.568627  0.324205  0.424113  0.130487  0.501549  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "149995  0.395748  0.327021  0.074967  0.320961  0.405083  0.269552  0.604612  \n",
              "149996  0.882559  0.369859  0.506035  0.507415  0.490268  0.384148  0.465247  \n",
              "149997  0.639840  0.694837  0.527072  0.257226  0.577989  0.378089  0.407385  \n",
              "149998  0.458858  0.478258  0.547498  0.407763  0.447528  0.918093  0.380684  \n",
              "149999  0.313268  0.540254  0.500405  0.347515  0.429655  0.187573  0.405743  \n",
              "\n",
              "[150000 rows x 101 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>extra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.772720</td>\n",
              "      <td>0.110345</td>\n",
              "      <td>0.376834</td>\n",
              "      <td>0.184523</td>\n",
              "      <td>0.173535</td>\n",
              "      <td>0.499419</td>\n",
              "      <td>0.064781</td>\n",
              "      <td>0.623703</td>\n",
              "      <td>0.549884</td>\n",
              "      <td>0.231115</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183102</td>\n",
              "      <td>0.031116</td>\n",
              "      <td>0.305497</td>\n",
              "      <td>0.775579</td>\n",
              "      <td>0.320757</td>\n",
              "      <td>0.458946</td>\n",
              "      <td>0.476499</td>\n",
              "      <td>0.395735</td>\n",
              "      <td>0.276237</td>\n",
              "      <td>0.484618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.227467</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.418600</td>\n",
              "      <td>0.721692</td>\n",
              "      <td>0.168458</td>\n",
              "      <td>0.166490</td>\n",
              "      <td>0.390414</td>\n",
              "      <td>0.607138</td>\n",
              "      <td>0.558683</td>\n",
              "      <td>0.209289</td>\n",
              "      <td>...</td>\n",
              "      <td>0.343843</td>\n",
              "      <td>0.299171</td>\n",
              "      <td>0.334076</td>\n",
              "      <td>0.570809</td>\n",
              "      <td>0.431019</td>\n",
              "      <td>0.452454</td>\n",
              "      <td>0.221138</td>\n",
              "      <td>0.293963</td>\n",
              "      <td>0.362470</td>\n",
              "      <td>0.286629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.866474</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.443734</td>\n",
              "      <td>0.753222</td>\n",
              "      <td>0.430279</td>\n",
              "      <td>0.513110</td>\n",
              "      <td>0.317988</td>\n",
              "      <td>0.507733</td>\n",
              "      <td>0.558186</td>\n",
              "      <td>0.280292</td>\n",
              "      <td>...</td>\n",
              "      <td>0.237500</td>\n",
              "      <td>0.021511</td>\n",
              "      <td>0.279352</td>\n",
              "      <td>0.365115</td>\n",
              "      <td>0.629283</td>\n",
              "      <td>0.442132</td>\n",
              "      <td>0.842200</td>\n",
              "      <td>0.580560</td>\n",
              "      <td>0.283877</td>\n",
              "      <td>0.532077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.815056</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.415874</td>\n",
              "      <td>0.617083</td>\n",
              "      <td>0.332129</td>\n",
              "      <td>0.158931</td>\n",
              "      <td>0.113397</td>\n",
              "      <td>0.605291</td>\n",
              "      <td>0.519431</td>\n",
              "      <td>0.343690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.333960</td>\n",
              "      <td>0.224251</td>\n",
              "      <td>0.253703</td>\n",
              "      <td>0.590433</td>\n",
              "      <td>0.377182</td>\n",
              "      <td>0.501562</td>\n",
              "      <td>0.807252</td>\n",
              "      <td>0.390389</td>\n",
              "      <td>0.163238</td>\n",
              "      <td>0.413293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.335133</td>\n",
              "      <td>0.365517</td>\n",
              "      <td>0.480299</td>\n",
              "      <td>0.520721</td>\n",
              "      <td>0.231542</td>\n",
              "      <td>0.199143</td>\n",
              "      <td>0.043051</td>\n",
              "      <td>0.645294</td>\n",
              "      <td>0.527027</td>\n",
              "      <td>0.300654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229583</td>\n",
              "      <td>0.574476</td>\n",
              "      <td>0.284815</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.753800</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.324205</td>\n",
              "      <td>0.424113</td>\n",
              "      <td>0.130487</td>\n",
              "      <td>0.501549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>0.718738</td>\n",
              "      <td>0.324138</td>\n",
              "      <td>0.484727</td>\n",
              "      <td>0.117018</td>\n",
              "      <td>0.638746</td>\n",
              "      <td>0.517924</td>\n",
              "      <td>0.165717</td>\n",
              "      <td>0.468975</td>\n",
              "      <td>0.434149</td>\n",
              "      <td>0.263514</td>\n",
              "      <td>...</td>\n",
              "      <td>0.191246</td>\n",
              "      <td>0.214853</td>\n",
              "      <td>0.428160</td>\n",
              "      <td>0.395748</td>\n",
              "      <td>0.327021</td>\n",
              "      <td>0.074967</td>\n",
              "      <td>0.320961</td>\n",
              "      <td>0.405083</td>\n",
              "      <td>0.269552</td>\n",
              "      <td>0.604612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>0.704383</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.481585</td>\n",
              "      <td>0.702127</td>\n",
              "      <td>0.173504</td>\n",
              "      <td>0.365400</td>\n",
              "      <td>0.066795</td>\n",
              "      <td>0.587191</td>\n",
              "      <td>0.504573</td>\n",
              "      <td>0.230486</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294597</td>\n",
              "      <td>0.153844</td>\n",
              "      <td>0.267596</td>\n",
              "      <td>0.882559</td>\n",
              "      <td>0.369859</td>\n",
              "      <td>0.506035</td>\n",
              "      <td>0.507415</td>\n",
              "      <td>0.490268</td>\n",
              "      <td>0.384148</td>\n",
              "      <td>0.465247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>0.426323</td>\n",
              "      <td>0.548276</td>\n",
              "      <td>0.426724</td>\n",
              "      <td>0.202462</td>\n",
              "      <td>0.193775</td>\n",
              "      <td>0.362679</td>\n",
              "      <td>0.265731</td>\n",
              "      <td>0.615467</td>\n",
              "      <td>0.476152</td>\n",
              "      <td>0.604092</td>\n",
              "      <td>...</td>\n",
              "      <td>0.351704</td>\n",
              "      <td>0.206957</td>\n",
              "      <td>0.385754</td>\n",
              "      <td>0.639840</td>\n",
              "      <td>0.694837</td>\n",
              "      <td>0.527072</td>\n",
              "      <td>0.257226</td>\n",
              "      <td>0.577989</td>\n",
              "      <td>0.378089</td>\n",
              "      <td>0.407385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>0.957795</td>\n",
              "      <td>0.117241</td>\n",
              "      <td>0.467765</td>\n",
              "      <td>0.170763</td>\n",
              "      <td>0.207358</td>\n",
              "      <td>0.307747</td>\n",
              "      <td>0.307419</td>\n",
              "      <td>0.450003</td>\n",
              "      <td>0.526416</td>\n",
              "      <td>0.375948</td>\n",
              "      <td>...</td>\n",
              "      <td>0.226907</td>\n",
              "      <td>0.509885</td>\n",
              "      <td>0.269716</td>\n",
              "      <td>0.458858</td>\n",
              "      <td>0.478258</td>\n",
              "      <td>0.547498</td>\n",
              "      <td>0.407763</td>\n",
              "      <td>0.447528</td>\n",
              "      <td>0.918093</td>\n",
              "      <td>0.380684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>0.279176</td>\n",
              "      <td>0.227586</td>\n",
              "      <td>0.466495</td>\n",
              "      <td>0.288108</td>\n",
              "      <td>0.217450</td>\n",
              "      <td>0.627955</td>\n",
              "      <td>0.396323</td>\n",
              "      <td>0.633061</td>\n",
              "      <td>0.528837</td>\n",
              "      <td>0.378993</td>\n",
              "      <td>...</td>\n",
              "      <td>0.369813</td>\n",
              "      <td>0.249765</td>\n",
              "      <td>0.298401</td>\n",
              "      <td>0.313268</td>\n",
              "      <td>0.540254</td>\n",
              "      <td>0.500405</td>\n",
              "      <td>0.347515</td>\n",
              "      <td>0.429655</td>\n",
              "      <td>0.187573</td>\n",
              "      <td>0.405743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows × 101 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lpV734OLUaJ"
      },
      "source": [
        "dependent_columns_lgb=pred_dataset.columns"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPKeCkLiLUXu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHCE129BLUVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_9ErfjKLUSh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAr7s-7oLUPr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgAAeWdbLUMU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVrqgYhcLUCC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UbcTDYFK5AC"
      },
      "source": [
        "#**LightGBM using Optuna Hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIEQoVUwALn3"
      },
      "source": [
        "import optuna.integration.lightgbm as lgbo"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCyyLcLTuXF"
      },
      "source": [
        "X_train_lgb,X_test_lgb,y_train_lgb,y_test_lgb=train_test_split(dataset[dependent_columns_lgb],dataset['loss'],test_size=0.3,random_state=True,shuffle=True)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW91z3kOBRd5"
      },
      "source": [
        "lgb_train=lgbo.Dataset(X_train_lgb,y_train_lgb)\n",
        "lgb_valid=lgbo.Dataset(X_test_lgb,y_test_lgb)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SogcyeOLBswh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5e4839-373e-43c8-fc89-3284c4630634"
      },
      "source": [
        "params={'objective':'mean_squared_error','metric': 'rmse'}\n",
        "model=lgbo.train(params,lgb_train,valid_sets=lgb_valid,verbose_eval=False, num_boost_round=100, early_stopping_rounds=5)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-13 17:14:16,394]\u001b[0m A new study created in memory with name: no-name-8d411e5b-8d3b-4295-834e-1388549efc2a\u001b[0m\n",
            "feature_fraction, val_score: inf:   0%|                                                          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037840 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction, val_score: 7.868303:  14%|######4                                      | 1/7 [00:01<00:10,  1.72s/it]\u001b[32m[I 2021-08-13 17:14:18,117]\u001b[0m Trial 0 finished with value: 7.868302787378062 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 7.868302787378062.\u001b[0m\n",
            "feature_fraction, val_score: 7.868303:  14%|######4                                      | 1/7 [00:01<00:10,  1.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046096 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction, val_score: 7.868303:  29%|############8                                | 2/7 [00:03<00:09,  1.90s/it]\u001b[32m[I 2021-08-13 17:14:20,139]\u001b[0m Trial 1 finished with value: 7.870506166421898 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 7.868302787378062.\u001b[0m\n",
            "feature_fraction, val_score: 7.868303:  29%|############8                                | 2/7 [00:03<00:09,  1.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036062 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction, val_score: 7.868303:  43%|###################2                         | 3/7 [00:05<00:07,  1.85s/it]\u001b[32m[I 2021-08-13 17:14:21,938]\u001b[0m Trial 2 finished with value: 7.87124970090329 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 7.868302787378062.\u001b[0m\n",
            "feature_fraction, val_score: 7.868303:  43%|###################2                         | 3/7 [00:05<00:07,  1.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044743 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction, val_score: 7.868303:  57%|#########################7                   | 4/7 [00:07<00:05,  1.87s/it]\u001b[32m[I 2021-08-13 17:14:23,844]\u001b[0m Trial 3 finished with value: 7.872317900542722 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 7.868302787378062.\u001b[0m\n",
            "feature_fraction, val_score: 7.868303:  57%|#########################7                   | 4/7 [00:07<00:05,  1.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038989 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction, val_score: 7.868303:  71%|################################1            | 5/7 [00:09<00:03,  1.85s/it]\u001b[32m[I 2021-08-13 17:14:25,659]\u001b[0m Trial 4 finished with value: 7.870310192082292 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 7.868302787378062.\u001b[0m\n",
            "feature_fraction, val_score: 7.868303:  71%|################################1            | 5/7 [00:09<00:03,  1.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038288 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction, val_score: 7.868303:  86%|######################################5      | 6/7 [00:11<00:02,  2.08s/it]\u001b[32m[I 2021-08-13 17:14:28,183]\u001b[0m Trial 5 finished with value: 7.871473457657024 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 7.868302787378062.\u001b[0m\n",
            "feature_fraction, val_score: 7.868303:  86%|######################################5      | 6/7 [00:11<00:02,  2.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040076 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction, val_score: 7.868303: 100%|#############################################| 7/7 [00:13<00:00,  1.98s/it]\u001b[32m[I 2021-08-13 17:14:29,966]\u001b[0m Trial 6 finished with value: 7.869419610921902 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 7.868302787378062.\u001b[0m\n",
            "feature_fraction, val_score: 7.868303: 100%|#############################################| 7/7 [00:13<00:00,  1.94s/it]\n",
            "num_leaves, val_score: 7.868303:   0%|                                                          | 0/20 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031545 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:   5%|##5                                               | 1/20 [00:01<00:29,  1.57s/it]\u001b[32m[I 2021-08-13 17:14:31,545]\u001b[0m Trial 7 finished with value: 7.88491376413582 and parameters: {'num_leaves': 164}. Best is trial 7 with value: 7.88491376413582.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:   5%|##5                                               | 1/20 [00:01<00:29,  1.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032822 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  10%|#####                                             | 2/20 [00:03<00:29,  1.63s/it]\u001b[32m[I 2021-08-13 17:14:33,213]\u001b[0m Trial 8 finished with value: 7.876885115766257 and parameters: {'num_leaves': 127}. Best is trial 8 with value: 7.876885115766257.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  10%|#####                                             | 2/20 [00:03<00:29,  1.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032816 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  15%|#######5                                          | 3/20 [00:05<00:29,  1.73s/it]\u001b[32m[I 2021-08-13 17:14:35,059]\u001b[0m Trial 9 finished with value: 7.887462915616702 and parameters: {'num_leaves': 226}. Best is trial 8 with value: 7.876885115766257.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  15%|#######5                                          | 3/20 [00:05<00:29,  1.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033517 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  20%|##########                                        | 4/20 [00:06<00:26,  1.65s/it]\u001b[32m[I 2021-08-13 17:14:36,582]\u001b[0m Trial 10 finished with value: 7.879911759148975 and parameters: {'num_leaves': 102}. Best is trial 8 with value: 7.876885115766257.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  20%|##########                                        | 4/20 [00:06<00:26,  1.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032529 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  25%|############5                                     | 5/20 [00:08<00:24,  1.62s/it]\u001b[32m[I 2021-08-13 17:14:38,149]\u001b[0m Trial 11 finished with value: 7.891601615899324 and parameters: {'num_leaves': 206}. Best is trial 8 with value: 7.876885115766257.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  25%|############5                                     | 5/20 [00:08<00:24,  1.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029825 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  30%|###############                                   | 6/20 [00:09<00:21,  1.53s/it]\u001b[32m[I 2021-08-13 17:14:39,518]\u001b[0m Trial 12 finished with value: 7.870485712330514 and parameters: {'num_leaves': 14}. Best is trial 12 with value: 7.870485712330514.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  30%|###############                                   | 6/20 [00:09<00:21,  1.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036883 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  35%|#################5                                | 7/20 [00:11<00:20,  1.61s/it]\u001b[32m[I 2021-08-13 17:14:41,288]\u001b[0m Trial 13 finished with value: 7.869981420460791 and parameters: {'num_leaves': 55}. Best is trial 13 with value: 7.869981420460791.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  35%|#################5                                | 7/20 [00:11<00:20,  1.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044449 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  40%|####################                              | 8/20 [00:13<00:22,  1.85s/it]\u001b[32m[I 2021-08-13 17:14:43,638]\u001b[0m Trial 14 finished with value: 7.874916814879261 and parameters: {'num_leaves': 96}. Best is trial 13 with value: 7.869981420460791.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  40%|####################                              | 8/20 [00:13<00:22,  1.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035923 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  45%|######################5                           | 9/20 [00:15<00:20,  1.86s/it]\u001b[32m[I 2021-08-13 17:14:45,540]\u001b[0m Trial 15 finished with value: 7.868302787378062 and parameters: {'num_leaves': 31}. Best is trial 15 with value: 7.868302787378062.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  45%|######################5                           | 9/20 [00:15<00:20,  1.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033489 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  50%|########################5                        | 10/20 [00:17<00:17,  1.74s/it]\u001b[32m[I 2021-08-13 17:14:47,005]\u001b[0m Trial 16 finished with value: 7.870485712330514 and parameters: {'num_leaves': 14}. Best is trial 15 with value: 7.868302787378062.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  50%|########################5                        | 10/20 [00:17<00:17,  1.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042482 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  55%|##########################9                      | 11/20 [00:18<00:15,  1.68s/it]\u001b[32m[I 2021-08-13 17:14:48,534]\u001b[0m Trial 17 finished with value: 7.872077981629896 and parameters: {'num_leaves': 58}. Best is trial 15 with value: 7.868302787378062.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  55%|##########################9                      | 11/20 [00:18<00:15,  1.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029937 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  60%|#############################4                   | 12/20 [00:20<00:13,  1.64s/it]\u001b[32m[I 2021-08-13 17:14:50,099]\u001b[0m Trial 18 finished with value: 7.869519457983328 and parameters: {'num_leaves': 59}. Best is trial 15 with value: 7.868302787378062.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  60%|#############################4                   | 12/20 [00:20<00:13,  1.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029984 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  65%|###############################8                 | 13/20 [00:21<00:11,  1.61s/it]\u001b[32m[I 2021-08-13 17:14:51,618]\u001b[0m Trial 19 finished with value: 7.869519457983328 and parameters: {'num_leaves': 59}. Best is trial 15 with value: 7.868302787378062.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  65%|###############################8                 | 13/20 [00:21<00:11,  1.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034505 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  70%|##################################3              | 14/20 [00:22<00:09,  1.52s/it]\u001b[32m[I 2021-08-13 17:14:52,952]\u001b[0m Trial 20 finished with value: 7.892420970509794 and parameters: {'num_leaves': 2}. Best is trial 15 with value: 7.868302787378062.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  70%|##################################3              | 14/20 [00:22<00:09,  1.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038939 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868303:  75%|####################################7            | 15/20 [00:24<00:07,  1.60s/it]\u001b[32m[I 2021-08-13 17:14:54,729]\u001b[0m Trial 21 finished with value: 7.870982428447451 and parameters: {'num_leaves': 81}. Best is trial 15 with value: 7.868302787378062.\u001b[0m\n",
            "num_leaves, val_score: 7.868303:  75%|####################################7            | 15/20 [00:24<00:07,  1.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030369 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868187:  80%|#######################################2         | 16/20 [00:26<00:06,  1.59s/it]\u001b[32m[I 2021-08-13 17:14:56,306]\u001b[0m Trial 22 finished with value: 7.868187007296521 and parameters: {'num_leaves': 32}. Best is trial 22 with value: 7.868187007296521.\u001b[0m\n",
            "num_leaves, val_score: 7.868187:  80%|#######################################2         | 16/20 [00:26<00:06,  1.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030207 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868187:  85%|#########################################6       | 17/20 [00:27<00:04,  1.61s/it]\u001b[32m[I 2021-08-13 17:14:57,938]\u001b[0m Trial 23 finished with value: 7.880407395082118 and parameters: {'num_leaves': 155}. Best is trial 22 with value: 7.868187007296521.\u001b[0m\n",
            "num_leaves, val_score: 7.868187:  85%|#########################################6       | 17/20 [00:27<00:04,  1.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033911 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868187:  90%|############################################1    | 18/20 [00:29<00:03,  1.57s/it]\u001b[32m[I 2021-08-13 17:14:59,427]\u001b[0m Trial 24 finished with value: 7.868721395781403 and parameters: {'num_leaves': 35}. Best is trial 22 with value: 7.868187007296521.\u001b[0m\n",
            "num_leaves, val_score: 7.868187:  90%|############################################1    | 18/20 [00:29<00:03,  1.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035828 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868168:  95%|##############################################5  | 19/20 [00:31<00:01,  1.64s/it]\u001b[32m[I 2021-08-13 17:15:01,225]\u001b[0m Trial 25 finished with value: 7.868168267855619 and parameters: {'num_leaves': 36}. Best is trial 25 with value: 7.868168267855619.\u001b[0m\n",
            "num_leaves, val_score: 7.868168:  95%|##############################################5  | 19/20 [00:31<00:01,  1.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045857 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "num_leaves, val_score: 7.868168: 100%|#################################################| 20/20 [00:33<00:00,  1.68s/it]\u001b[32m[I 2021-08-13 17:15:03,016]\u001b[0m Trial 26 finished with value: 7.878810602006954 and parameters: {'num_leaves': 104}. Best is trial 25 with value: 7.868168267855619.\u001b[0m\n",
            "num_leaves, val_score: 7.868168: 100%|#################################################| 20/20 [00:33<00:00,  1.65s/it]\n",
            "bagging, val_score: 7.868168:   0%|                                                             | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040851 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  10%|#####3                                               | 1/10 [00:01<00:15,  1.72s/it]\u001b[32m[I 2021-08-13 17:15:04,742]\u001b[0m Trial 27 finished with value: 7.877061951997416 and parameters: {'bagging_fraction': 0.6176166124594521, 'bagging_freq': 1}. Best is trial 27 with value: 7.877061951997416.\u001b[0m\n",
            "bagging, val_score: 7.868168:  10%|#####3                                               | 1/10 [00:01<00:15,  1.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045419 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  20%|##########6                                          | 2/10 [00:03<00:15,  1.89s/it]\u001b[32m[I 2021-08-13 17:15:06,746]\u001b[0m Trial 28 finished with value: 7.87033585147097 and parameters: {'bagging_fraction': 0.8264987032511715, 'bagging_freq': 6}. Best is trial 28 with value: 7.87033585147097.\u001b[0m\n",
            "bagging, val_score: 7.868168:  20%|##########6                                          | 2/10 [00:03<00:15,  1.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034562 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  30%|###############9                                     | 3/10 [00:05<00:13,  1.97s/it]\u001b[32m[I 2021-08-13 17:15:08,807]\u001b[0m Trial 29 finished with value: 7.8684721179861015 and parameters: {'bagging_fraction': 0.7996070175256147, 'bagging_freq': 2}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168:  30%|###############9                                     | 3/10 [00:05<00:13,  1.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034674 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  40%|#####################2                               | 4/10 [00:07<00:11,  1.90s/it]\u001b[32m[I 2021-08-13 17:15:10,596]\u001b[0m Trial 30 finished with value: 7.871932390952163 and parameters: {'bagging_fraction': 0.8856941715940094, 'bagging_freq': 5}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168:  40%|#####################2                               | 4/10 [00:07<00:11,  1.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035797 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  50%|##########################5                          | 5/10 [00:09<00:08,  1.77s/it]\u001b[32m[I 2021-08-13 17:15:12,143]\u001b[0m Trial 31 finished with value: 7.877914821852417 and parameters: {'bagging_fraction': 0.5789608964011824, 'bagging_freq': 6}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168:  50%|##########################5                          | 5/10 [00:09<00:08,  1.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050396 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  60%|###############################8                     | 6/10 [00:10<00:07,  1.78s/it]\u001b[32m[I 2021-08-13 17:15:13,942]\u001b[0m Trial 32 finished with value: 7.8697551379436534 and parameters: {'bagging_fraction': 0.983434252214781, 'bagging_freq': 6}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168:  60%|###############################8                     | 6/10 [00:10<00:07,  1.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035624 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  70%|#####################################                | 7/10 [00:12<00:05,  1.71s/it]\u001b[32m[I 2021-08-13 17:15:15,520]\u001b[0m Trial 33 finished with value: 7.876340394451335 and parameters: {'bagging_fraction': 0.6919922472784266, 'bagging_freq': 3}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168:  70%|#####################################                | 7/10 [00:12<00:05,  1.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052259 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  80%|##########################################4          | 8/10 [00:14<00:03,  1.72s/it]\u001b[32m[I 2021-08-13 17:15:17,246]\u001b[0m Trial 34 finished with value: 7.871975693234551 and parameters: {'bagging_fraction': 0.7345236504578598, 'bagging_freq': 6}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168:  80%|##########################################4          | 8/10 [00:14<00:03,  1.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036095 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168:  90%|###############################################7     | 9/10 [00:15<00:01,  1.68s/it]\u001b[32m[I 2021-08-13 17:15:18,830]\u001b[0m Trial 35 finished with value: 7.876554462753427 and parameters: {'bagging_fraction': 0.7204014968589867, 'bagging_freq': 3}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168:  90%|###############################################7     | 9/10 [00:15<00:01,  1.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037108 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bagging, val_score: 7.868168: 100%|####################################################| 10/10 [00:17<00:00,  1.59s/it]\u001b[32m[I 2021-08-13 17:15:20,241]\u001b[0m Trial 36 finished with value: 7.881133395266275 and parameters: {'bagging_fraction': 0.4296258517483743, 'bagging_freq': 5}. Best is trial 29 with value: 7.8684721179861015.\u001b[0m\n",
            "bagging, val_score: 7.868168: 100%|####################################################| 10/10 [00:17<00:00,  1.72s/it]\n",
            "feature_fraction_stage2, val_score: 7.868168:   0%|                                              | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036858 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction_stage2, val_score: 7.868168:  33%|############6                         | 1/3 [00:01<00:03,  1.85s/it]\u001b[32m[I 2021-08-13 17:15:22,091]\u001b[0m Trial 37 finished with value: 7.869008758434116 and parameters: {'feature_fraction': 0.48000000000000004}. Best is trial 37 with value: 7.869008758434116.\u001b[0m\n",
            "feature_fraction_stage2, val_score: 7.868168:  33%|############6                         | 1/3 [00:01<00:03,  1.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036901 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction_stage2, val_score: 7.868168:  67%|#########################3            | 2/3 [00:03<00:01,  1.78s/it]\u001b[32m[I 2021-08-13 17:15:23,819]\u001b[0m Trial 38 finished with value: 7.868288690326486 and parameters: {'feature_fraction': 0.44800000000000006}. Best is trial 38 with value: 7.868288690326486.\u001b[0m\n",
            "feature_fraction_stage2, val_score: 7.868168:  67%|#########################3            | 2/3 [00:03<00:01,  1.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039938 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "feature_fraction_stage2, val_score: 7.868010: 100%|######################################| 3/3 [00:05<00:00,  1.74s/it]\u001b[32m[I 2021-08-13 17:15:25,510]\u001b[0m Trial 39 finished with value: 7.868010160522596 and parameters: {'feature_fraction': 0.41600000000000004}. Best is trial 39 with value: 7.868010160522596.\u001b[0m\n",
            "feature_fraction_stage2, val_score: 7.868010: 100%|######################################| 3/3 [00:05<00:00,  1.76s/it]\n",
            "regularization_factors, val_score: 7.868010:   0%|                                              | 0/20 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037329 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:   5%|#9                                    | 1/20 [00:01<00:37,  1.96s/it]\u001b[32m[I 2021-08-13 17:15:27,478]\u001b[0m Trial 40 finished with value: 7.86595711895588 and parameters: {'lambda_l1': 3.1616269151235284e-08, 'lambda_l2': 5.652867026946649}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:   5%|#9                                    | 1/20 [00:01<00:37,  1.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035456 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  10%|###8                                  | 2/20 [00:03<00:33,  1.87s/it]\u001b[32m[I 2021-08-13 17:15:29,279]\u001b[0m Trial 41 finished with value: 7.868010013047726 and parameters: {'lambda_l1': 0.005565580668664953, 'lambda_l2': 4.076785547105805e-05}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  10%|###8                                  | 2/20 [00:03<00:33,  1.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039647 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  15%|#####7                                | 3/20 [00:05<00:31,  1.83s/it]\u001b[32m[I 2021-08-13 17:15:31,063]\u001b[0m Trial 42 finished with value: 7.868294808026976 and parameters: {'lambda_l1': 0.03223283335372097, 'lambda_l2': 0.007238977577266555}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  15%|#####7                                | 3/20 [00:05<00:31,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037449 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  20%|#######6                              | 4/20 [00:07<00:29,  1.84s/it]\u001b[32m[I 2021-08-13 17:15:32,910]\u001b[0m Trial 43 finished with value: 7.868010128266735 and parameters: {'lambda_l1': 0.0012956028809216927, 'lambda_l2': 1.8009837299349925e-08}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  20%|#######6                              | 4/20 [00:07<00:29,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040169 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  25%|#########5                            | 5/20 [00:09<00:26,  1.80s/it]\u001b[32m[I 2021-08-13 17:15:34,641]\u001b[0m Trial 44 finished with value: 7.867545597484344 and parameters: {'lambda_l1': 3.8790479708306957, 'lambda_l2': 4.367212032746781e-08}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  25%|#########5                            | 5/20 [00:09<00:26,  1.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036131 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  30%|###########4                          | 6/20 [00:10<00:25,  1.80s/it]\u001b[32m[I 2021-08-13 17:15:36,449]\u001b[0m Trial 45 finished with value: 7.868009684137987 and parameters: {'lambda_l1': 0.01894524887257913, 'lambda_l2': 2.2529749409066894e-05}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  30%|###########4                          | 6/20 [00:10<00:25,  1.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036746 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  35%|#############3                        | 7/20 [00:12<00:23,  1.81s/it]\u001b[32m[I 2021-08-13 17:15:38,273]\u001b[0m Trial 46 finished with value: 7.868010123979855 and parameters: {'lambda_l1': 5.315281201385413e-07, 'lambda_l2': 0.00016614205878469682}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  35%|#############3                        | 7/20 [00:12<00:23,  1.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041013 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  40%|###############2                      | 8/20 [00:14<00:21,  1.80s/it]\u001b[32m[I 2021-08-13 17:15:40,065]\u001b[0m Trial 47 finished with value: 7.86801015729141 and parameters: {'lambda_l1': 0.00013009609978548818, 'lambda_l2': 5.2347637206031626e-08}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  40%|###############2                      | 8/20 [00:14<00:21,  1.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037332 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  45%|#################1                    | 9/20 [00:16<00:20,  1.85s/it]\u001b[32m[I 2021-08-13 17:15:42,004]\u001b[0m Trial 48 finished with value: 7.868903270974472 and parameters: {'lambda_l1': 0.014567730556442266, 'lambda_l2': 3.4428433515458723}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  45%|#################1                    | 9/20 [00:16<00:20,  1.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045071 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  50%|##################5                  | 10/20 [00:18<00:18,  1.86s/it]\u001b[32m[I 2021-08-13 17:15:43,907]\u001b[0m Trial 49 finished with value: 7.868009660650126 and parameters: {'lambda_l1': 0.00024180068306115134, 'lambda_l2': 0.0022454640467638788}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  50%|##################5                  | 10/20 [00:18<00:18,  1.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051151 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.865957:  55%|####################3                | 11/20 [00:20<00:17,  1.98s/it]\u001b[32m[I 2021-08-13 17:15:46,164]\u001b[0m Trial 50 finished with value: 7.8667896258903705 and parameters: {'lambda_l1': 2.0997092679698487e-08, 'lambda_l2': 5.26657135276247}. Best is trial 40 with value: 7.86595711895588.\u001b[0m\n",
            "regularization_factors, val_score: 7.865957:  55%|####################3                | 11/20 [00:20<00:17,  1.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034973 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  60%|######################2              | 12/20 [00:22<00:15,  1.94s/it]\u001b[32m[I 2021-08-13 17:15:47,997]\u001b[0m Trial 51 finished with value: 7.864950033761913 and parameters: {'lambda_l1': 1.0260039770671602e-08, 'lambda_l2': 9.733896707695624}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  60%|######################2              | 12/20 [00:22<00:15,  1.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045879 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  65%|########################             | 13/20 [00:24<00:13,  1.94s/it]\u001b[32m[I 2021-08-13 17:15:49,948]\u001b[0m Trial 52 finished with value: 7.872245350249529 and parameters: {'lambda_l1': 1.370134001819222e-08, 'lambda_l2': 0.14999080635496745}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  65%|########################             | 13/20 [00:24<00:13,  1.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034538 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  70%|#########################9           | 14/20 [00:26<00:11,  1.94s/it]\u001b[32m[I 2021-08-13 17:15:51,890]\u001b[0m Trial 53 finished with value: 7.869398946703883 and parameters: {'lambda_l1': 2.73738485717377e-06, 'lambda_l2': 0.18202891369710647}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  70%|#########################9           | 14/20 [00:26<00:11,  1.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040850 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  75%|###########################7         | 15/20 [00:28<00:09,  1.90s/it]\u001b[32m[I 2021-08-13 17:15:53,680]\u001b[0m Trial 54 finished with value: 7.868690092877893 and parameters: {'lambda_l1': 9.230452942819844e-07, 'lambda_l2': 0.31135017623848205}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  75%|###########################7         | 15/20 [00:28<00:09,  1.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035436 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  80%|#############################6       | 16/20 [00:30<00:07,  1.92s/it]\u001b[32m[I 2021-08-13 17:15:55,668]\u001b[0m Trial 55 finished with value: 7.866626738933726 and parameters: {'lambda_l1': 1.6663874411004396e-07, 'lambda_l2': 5.875685920271504}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  80%|#############################6       | 16/20 [00:30<00:07,  1.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037748 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  85%|###############################4     | 17/20 [00:31<00:05,  1.90s/it]\u001b[32m[I 2021-08-13 17:15:57,510]\u001b[0m Trial 56 finished with value: 7.868010159886645 and parameters: {'lambda_l1': 8.425508900566815e-06, 'lambda_l2': 1.965024135572889e-06}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  85%|###############################4     | 17/20 [00:31<00:05,  1.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035427 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  90%|#################################3   | 18/20 [00:33<00:03,  1.86s/it]\u001b[32m[I 2021-08-13 17:15:59,279]\u001b[0m Trial 57 finished with value: 7.868294485174126 and parameters: {'lambda_l1': 2.2350376242926874e-05, 'lambda_l2': 0.012196556188897367}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  90%|#################################3   | 18/20 [00:33<00:03,  1.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034669 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950:  95%|###################################1 | 19/20 [00:35<00:01,  1.84s/it]\u001b[32m[I 2021-08-13 17:16:01,076]\u001b[0m Trial 58 finished with value: 7.868690130537782 and parameters: {'lambda_l1': 1.613834256747053e-07, 'lambda_l2': 0.31114656729557594}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950:  95%|###################################1 | 19/20 [00:35<00:01,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035029 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "regularization_factors, val_score: 7.864950: 100%|#####################################| 20/20 [00:37<00:00,  1.83s/it]\u001b[32m[I 2021-08-13 17:16:02,889]\u001b[0m Trial 59 finished with value: 7.867967988117967 and parameters: {'lambda_l1': 1.1320489200200019e-08, 'lambda_l2': 0.03816475025734338}. Best is trial 51 with value: 7.864950033761913.\u001b[0m\n",
            "regularization_factors, val_score: 7.864950: 100%|#####################################| 20/20 [00:37<00:00,  1.87s/it]\n",
            "min_data_in_leaf, val_score: 7.864950:   0%|                                                     | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036988 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "min_data_in_leaf, val_score: 7.864465:  20%|#########                                    | 1/5 [00:02<00:09,  2.32s/it]\u001b[32m[I 2021-08-13 17:16:05,220]\u001b[0m Trial 60 finished with value: 7.864464562180641 and parameters: {'min_child_samples': 5}. Best is trial 60 with value: 7.864464562180641.\u001b[0m\n",
            "min_data_in_leaf, val_score: 7.864465:  20%|#########                                    | 1/5 [00:02<00:09,  2.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036704 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "min_data_in_leaf, val_score: 7.864465:  40%|##################                           | 2/5 [00:04<00:06,  2.01s/it]\u001b[32m[I 2021-08-13 17:16:07,003]\u001b[0m Trial 61 finished with value: 7.86518644402379 and parameters: {'min_child_samples': 50}. Best is trial 60 with value: 7.864464562180641.\u001b[0m\n",
            "min_data_in_leaf, val_score: 7.864465:  40%|##################                           | 2/5 [00:04<00:06,  2.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037370 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "min_data_in_leaf, val_score: 7.864465:  60%|###########################                  | 3/5 [00:05<00:03,  1.90s/it]\u001b[32m[I 2021-08-13 17:16:08,783]\u001b[0m Trial 62 finished with value: 7.867309453266853 and parameters: {'min_child_samples': 25}. Best is trial 60 with value: 7.864464562180641.\u001b[0m\n",
            "min_data_in_leaf, val_score: 7.864465:  60%|###########################                  | 3/5 [00:05<00:03,  1.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036611 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "min_data_in_leaf, val_score: 7.864012:  80%|####################################         | 4/5 [00:07<00:01,  1.91s/it]\u001b[32m[I 2021-08-13 17:16:10,694]\u001b[0m Trial 63 finished with value: 7.8640120603326 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 7.8640120603326.\u001b[0m\n",
            "min_data_in_leaf, val_score: 7.864012:  80%|####################################         | 4/5 [00:07<00:01,  1.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035957 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "min_data_in_leaf, val_score: 7.864012: 100%|#############################################| 5/5 [00:09<00:00,  1.91s/it]\u001b[32m[I 2021-08-13 17:16:12,619]\u001b[0m Trial 64 finished with value: 7.864771820781077 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 7.8640120603326.\u001b[0m\n",
            "min_data_in_leaf, val_score: 7.864012: 100%|#############################################| 5/5 [00:09<00:00,  1.95s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti9sUrY9CZ9D"
      },
      "source": [
        "best_lgb_params =model.params\n",
        "best_lgb_params[\"learning_rate\"] = 0.003\n",
        "best_lgb_params[\"early_stopping_round\"] = 1000\n",
        "best_lgb_params[\"num_iterations\"] = 80000"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3J-N3TsDm7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb108552-ea01-4e71-ceb4-282777bd2d1c"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "kfld=KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "\n",
        "models=[]\n",
        "\n",
        "for fold,(train_index,test_index) in enumerate(kfld.split(dataset[dependent_columns_lgb],dataset['loss'])):\n",
        "  lgb_train=lgb.Dataset(dataset[dependent_columns_lgb].loc[train_index],dataset['loss'].loc[train_index])\n",
        "  lgb_test=lgb.Dataset(dataset[dependent_columns_lgb].loc[test_index],dataset['loss'].loc[test_index])\n",
        "  _model=lgb.train(best_lgb_params,lgb_train,valid_sets=lgb_test,verbose_eval=100)\n",
        "  models.append(_model)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\venka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "c:\\users\\venka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037591 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.827720\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.90266\n",
            "[200]\tvalid_0's rmse: 7.89024\n",
            "[300]\tvalid_0's rmse: 7.88029\n",
            "[400]\tvalid_0's rmse: 7.87244\n",
            "[500]\tvalid_0's rmse: 7.86636\n",
            "[600]\tvalid_0's rmse: 7.86133\n",
            "[700]\tvalid_0's rmse: 7.85737\n",
            "[800]\tvalid_0's rmse: 7.85398\n",
            "[900]\tvalid_0's rmse: 7.8511\n",
            "[1000]\tvalid_0's rmse: 7.84867\n",
            "[1100]\tvalid_0's rmse: 7.84659\n",
            "[1200]\tvalid_0's rmse: 7.84476\n",
            "[1300]\tvalid_0's rmse: 7.84318\n",
            "[1400]\tvalid_0's rmse: 7.8418\n",
            "[1500]\tvalid_0's rmse: 7.84052\n",
            "[1600]\tvalid_0's rmse: 7.83928\n",
            "[1700]\tvalid_0's rmse: 7.83813\n",
            "[1800]\tvalid_0's rmse: 7.83694\n",
            "[1900]\tvalid_0's rmse: 7.83582\n",
            "[2000]\tvalid_0's rmse: 7.83481\n",
            "[2100]\tvalid_0's rmse: 7.83393\n",
            "[2200]\tvalid_0's rmse: 7.83308\n",
            "[2300]\tvalid_0's rmse: 7.83226\n",
            "[2400]\tvalid_0's rmse: 7.83147\n",
            "[2500]\tvalid_0's rmse: 7.83088\n",
            "[2600]\tvalid_0's rmse: 7.83022\n",
            "[2700]\tvalid_0's rmse: 7.82966\n",
            "[2800]\tvalid_0's rmse: 7.82929\n",
            "[2900]\tvalid_0's rmse: 7.82891\n",
            "[3000]\tvalid_0's rmse: 7.82853\n",
            "[3100]\tvalid_0's rmse: 7.82813\n",
            "[3200]\tvalid_0's rmse: 7.82771\n",
            "[3300]\tvalid_0's rmse: 7.82734\n",
            "[3400]\tvalid_0's rmse: 7.82711\n",
            "[3500]\tvalid_0's rmse: 7.82673\n",
            "[3600]\tvalid_0's rmse: 7.82652\n",
            "[3700]\tvalid_0's rmse: 7.82616\n",
            "[3800]\tvalid_0's rmse: 7.82585\n",
            "[3900]\tvalid_0's rmse: 7.82566\n",
            "[4000]\tvalid_0's rmse: 7.82539\n",
            "[4100]\tvalid_0's rmse: 7.82518\n",
            "[4200]\tvalid_0's rmse: 7.82484\n",
            "[4300]\tvalid_0's rmse: 7.82456\n",
            "[4400]\tvalid_0's rmse: 7.8244\n",
            "[4500]\tvalid_0's rmse: 7.82416\n",
            "[4600]\tvalid_0's rmse: 7.82389\n",
            "[4700]\tvalid_0's rmse: 7.82363\n",
            "[4800]\tvalid_0's rmse: 7.82345\n",
            "[4900]\tvalid_0's rmse: 7.82329\n",
            "[5000]\tvalid_0's rmse: 7.82313\n",
            "[5100]\tvalid_0's rmse: 7.82292\n",
            "[5200]\tvalid_0's rmse: 7.82273\n",
            "[5300]\tvalid_0's rmse: 7.8225\n",
            "[5400]\tvalid_0's rmse: 7.82232\n",
            "[5500]\tvalid_0's rmse: 7.82216\n",
            "[5600]\tvalid_0's rmse: 7.82201\n",
            "[5700]\tvalid_0's rmse: 7.82186\n",
            "[5800]\tvalid_0's rmse: 7.82167\n",
            "[5900]\tvalid_0's rmse: 7.82156\n",
            "[6000]\tvalid_0's rmse: 7.8215\n",
            "[6100]\tvalid_0's rmse: 7.82141\n",
            "[6200]\tvalid_0's rmse: 7.82127\n",
            "[6300]\tvalid_0's rmse: 7.8212\n",
            "[6400]\tvalid_0's rmse: 7.82113\n",
            "[6500]\tvalid_0's rmse: 7.82097\n",
            "[6600]\tvalid_0's rmse: 7.82088\n",
            "[6700]\tvalid_0's rmse: 7.8207\n",
            "[6800]\tvalid_0's rmse: 7.82058\n",
            "[6900]\tvalid_0's rmse: 7.82056\n",
            "[7000]\tvalid_0's rmse: 7.82046\n",
            "[7100]\tvalid_0's rmse: 7.82031\n",
            "[7200]\tvalid_0's rmse: 7.82015\n",
            "[7300]\tvalid_0's rmse: 7.81998\n",
            "[7400]\tvalid_0's rmse: 7.81984\n",
            "[7500]\tvalid_0's rmse: 7.81978\n",
            "[7600]\tvalid_0's rmse: 7.81974\n",
            "[7700]\tvalid_0's rmse: 7.81958\n",
            "[7800]\tvalid_0's rmse: 7.81954\n",
            "[7900]\tvalid_0's rmse: 7.81942\n",
            "[8000]\tvalid_0's rmse: 7.81936\n",
            "[8100]\tvalid_0's rmse: 7.81928\n",
            "[8200]\tvalid_0's rmse: 7.81912\n",
            "[8300]\tvalid_0's rmse: 7.81908\n",
            "[8400]\tvalid_0's rmse: 7.819\n",
            "[8500]\tvalid_0's rmse: 7.81885\n",
            "[8600]\tvalid_0's rmse: 7.81884\n",
            "[8700]\tvalid_0's rmse: 7.81875\n",
            "[8800]\tvalid_0's rmse: 7.81868\n",
            "[8900]\tvalid_0's rmse: 7.81866\n",
            "[9000]\tvalid_0's rmse: 7.81863\n",
            "[9100]\tvalid_0's rmse: 7.81854\n",
            "[9200]\tvalid_0's rmse: 7.81848\n",
            "[9300]\tvalid_0's rmse: 7.81836\n",
            "[9400]\tvalid_0's rmse: 7.81828\n",
            "[9500]\tvalid_0's rmse: 7.81825\n",
            "[9600]\tvalid_0's rmse: 7.81818\n",
            "[9700]\tvalid_0's rmse: 7.81817\n",
            "[9800]\tvalid_0's rmse: 7.81809\n",
            "[9900]\tvalid_0's rmse: 7.81803\n",
            "[10000]\tvalid_0's rmse: 7.81795\n",
            "[10100]\tvalid_0's rmse: 7.8179\n",
            "[10200]\tvalid_0's rmse: 7.81783\n",
            "[10300]\tvalid_0's rmse: 7.81775\n",
            "[10400]\tvalid_0's rmse: 7.81765\n",
            "[10500]\tvalid_0's rmse: 7.8175\n",
            "[10600]\tvalid_0's rmse: 7.8174\n",
            "[10700]\tvalid_0's rmse: 7.81734\n",
            "[10800]\tvalid_0's rmse: 7.8173\n",
            "[10900]\tvalid_0's rmse: 7.81728\n",
            "[11000]\tvalid_0's rmse: 7.81718\n",
            "[11100]\tvalid_0's rmse: 7.81718\n",
            "[11200]\tvalid_0's rmse: 7.81714\n",
            "[11300]\tvalid_0's rmse: 7.8171\n",
            "[11400]\tvalid_0's rmse: 7.81701\n",
            "[11500]\tvalid_0's rmse: 7.81697\n",
            "[11600]\tvalid_0's rmse: 7.81691\n",
            "[11700]\tvalid_0's rmse: 7.81687\n",
            "[11800]\tvalid_0's rmse: 7.8168\n",
            "[11900]\tvalid_0's rmse: 7.81683\n",
            "[12000]\tvalid_0's rmse: 7.81677\n",
            "[12100]\tvalid_0's rmse: 7.81675\n",
            "[12200]\tvalid_0's rmse: 7.81671\n",
            "[12300]\tvalid_0's rmse: 7.81666\n",
            "[12400]\tvalid_0's rmse: 7.81666\n",
            "[12500]\tvalid_0's rmse: 7.81668\n",
            "[12600]\tvalid_0's rmse: 7.81666\n",
            "[12700]\tvalid_0's rmse: 7.81675\n",
            "[12800]\tvalid_0's rmse: 7.81667\n",
            "[12900]\tvalid_0's rmse: 7.81668\n",
            "[13000]\tvalid_0's rmse: 7.81667\n",
            "[13100]\tvalid_0's rmse: 7.81665\n",
            "[13200]\tvalid_0's rmse: 7.81661\n",
            "[13300]\tvalid_0's rmse: 7.81654\n",
            "[13400]\tvalid_0's rmse: 7.81644\n",
            "[13500]\tvalid_0's rmse: 7.8164\n",
            "[13600]\tvalid_0's rmse: 7.81636\n",
            "[13700]\tvalid_0's rmse: 7.81636\n",
            "[13800]\tvalid_0's rmse: 7.81633\n",
            "[13900]\tvalid_0's rmse: 7.81626\n",
            "[14000]\tvalid_0's rmse: 7.81618\n",
            "[14100]\tvalid_0's rmse: 7.81619\n",
            "[14200]\tvalid_0's rmse: 7.81617\n",
            "[14300]\tvalid_0's rmse: 7.81617\n",
            "[14400]\tvalid_0's rmse: 7.81613\n",
            "[14500]\tvalid_0's rmse: 7.81613\n",
            "[14600]\tvalid_0's rmse: 7.81608\n",
            "[14700]\tvalid_0's rmse: 7.81607\n",
            "[14800]\tvalid_0's rmse: 7.81603\n",
            "[14900]\tvalid_0's rmse: 7.81597\n",
            "[15000]\tvalid_0's rmse: 7.81596\n",
            "[15100]\tvalid_0's rmse: 7.81595\n",
            "[15200]\tvalid_0's rmse: 7.81588\n",
            "[15300]\tvalid_0's rmse: 7.81585\n",
            "[15400]\tvalid_0's rmse: 7.81579\n",
            "[15500]\tvalid_0's rmse: 7.81571\n",
            "[15600]\tvalid_0's rmse: 7.81572\n",
            "[15700]\tvalid_0's rmse: 7.81576\n",
            "[15800]\tvalid_0's rmse: 7.81576\n",
            "[15900]\tvalid_0's rmse: 7.81574\n",
            "[16000]\tvalid_0's rmse: 7.81576\n",
            "[16100]\tvalid_0's rmse: 7.81573\n",
            "[16200]\tvalid_0's rmse: 7.81572\n",
            "[16300]\tvalid_0's rmse: 7.81568\n",
            "[16400]\tvalid_0's rmse: 7.81574\n",
            "[16500]\tvalid_0's rmse: 7.81573\n",
            "[16600]\tvalid_0's rmse: 7.81566\n",
            "[16700]\tvalid_0's rmse: 7.81566\n",
            "[16800]\tvalid_0's rmse: 7.81564\n",
            "[16900]\tvalid_0's rmse: 7.81564\n",
            "[17000]\tvalid_0's rmse: 7.81562\n",
            "[17100]\tvalid_0's rmse: 7.81555\n",
            "[17200]\tvalid_0's rmse: 7.81552\n",
            "[17300]\tvalid_0's rmse: 7.8155\n",
            "[17400]\tvalid_0's rmse: 7.81552\n",
            "[17500]\tvalid_0's rmse: 7.81554\n",
            "[17600]\tvalid_0's rmse: 7.81552\n",
            "[17700]\tvalid_0's rmse: 7.81551\n",
            "[17800]\tvalid_0's rmse: 7.81549\n",
            "[17900]\tvalid_0's rmse: 7.81543\n",
            "[18000]\tvalid_0's rmse: 7.81546\n",
            "[18100]\tvalid_0's rmse: 7.81544\n",
            "[18200]\tvalid_0's rmse: 7.81544\n",
            "[18300]\tvalid_0's rmse: 7.81538\n",
            "[18400]\tvalid_0's rmse: 7.8154\n",
            "[18500]\tvalid_0's rmse: 7.8154\n",
            "[18600]\tvalid_0's rmse: 7.81545\n",
            "[18700]\tvalid_0's rmse: 7.81542\n",
            "[18800]\tvalid_0's rmse: 7.81541\n",
            "[18900]\tvalid_0's rmse: 7.81535\n",
            "[19000]\tvalid_0's rmse: 7.81528\n",
            "[19100]\tvalid_0's rmse: 7.81529\n",
            "[19200]\tvalid_0's rmse: 7.8153\n",
            "[19300]\tvalid_0's rmse: 7.81526\n",
            "[19400]\tvalid_0's rmse: 7.81531\n",
            "[19500]\tvalid_0's rmse: 7.81532\n",
            "[19600]\tvalid_0's rmse: 7.81537\n",
            "[19700]\tvalid_0's rmse: 7.81542\n",
            "[19800]\tvalid_0's rmse: 7.81545\n",
            "[19900]\tvalid_0's rmse: 7.81543\n",
            "[20000]\tvalid_0's rmse: 7.81548\n",
            "[20100]\tvalid_0's rmse: 7.81552\n",
            "[20200]\tvalid_0's rmse: 7.8155\n",
            "[20300]\tvalid_0's rmse: 7.81546\n",
            "Early stopping, best iteration is:\n",
            "[19336]\tvalid_0's rmse: 7.81525\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084512 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.812385\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.94958\n",
            "[200]\tvalid_0's rmse: 7.93824\n",
            "[300]\tvalid_0's rmse: 7.92922\n",
            "[400]\tvalid_0's rmse: 7.92222\n",
            "[500]\tvalid_0's rmse: 7.91667\n",
            "[600]\tvalid_0's rmse: 7.9122\n",
            "[700]\tvalid_0's rmse: 7.90862\n",
            "[800]\tvalid_0's rmse: 7.90561\n",
            "[900]\tvalid_0's rmse: 7.90309\n",
            "[1000]\tvalid_0's rmse: 7.90102\n",
            "[1100]\tvalid_0's rmse: 7.89932\n",
            "[1200]\tvalid_0's rmse: 7.8977\n",
            "[1300]\tvalid_0's rmse: 7.89617\n",
            "[1400]\tvalid_0's rmse: 7.8947\n",
            "[1500]\tvalid_0's rmse: 7.89347\n",
            "[1600]\tvalid_0's rmse: 7.89236\n",
            "[1700]\tvalid_0's rmse: 7.89118\n",
            "[1800]\tvalid_0's rmse: 7.89011\n",
            "[1900]\tvalid_0's rmse: 7.88912\n",
            "[2000]\tvalid_0's rmse: 7.88827\n",
            "[2100]\tvalid_0's rmse: 7.8876\n",
            "[2200]\tvalid_0's rmse: 7.88694\n",
            "[2300]\tvalid_0's rmse: 7.88638\n",
            "[2400]\tvalid_0's rmse: 7.88581\n",
            "[2500]\tvalid_0's rmse: 7.88526\n",
            "[2600]\tvalid_0's rmse: 7.88478\n",
            "[2700]\tvalid_0's rmse: 7.88429\n",
            "[2800]\tvalid_0's rmse: 7.88387\n",
            "[2900]\tvalid_0's rmse: 7.88346\n",
            "[3000]\tvalid_0's rmse: 7.88304\n",
            "[3100]\tvalid_0's rmse: 7.88273\n",
            "[3200]\tvalid_0's rmse: 7.88239\n",
            "[3300]\tvalid_0's rmse: 7.882\n",
            "[3400]\tvalid_0's rmse: 7.88172\n",
            "[3500]\tvalid_0's rmse: 7.88137\n",
            "[3600]\tvalid_0's rmse: 7.88106\n",
            "[3700]\tvalid_0's rmse: 7.88081\n",
            "[3800]\tvalid_0's rmse: 7.88054\n",
            "[3900]\tvalid_0's rmse: 7.88024\n",
            "[4000]\tvalid_0's rmse: 7.87992\n",
            "[4100]\tvalid_0's rmse: 7.87953\n",
            "[4200]\tvalid_0's rmse: 7.87928\n",
            "[4300]\tvalid_0's rmse: 7.87912\n",
            "[4400]\tvalid_0's rmse: 7.8789\n",
            "[4500]\tvalid_0's rmse: 7.87865\n",
            "[4600]\tvalid_0's rmse: 7.87852\n",
            "[4700]\tvalid_0's rmse: 7.87836\n",
            "[4800]\tvalid_0's rmse: 7.87823\n",
            "[4900]\tvalid_0's rmse: 7.87799\n",
            "[5000]\tvalid_0's rmse: 7.87781\n",
            "[5100]\tvalid_0's rmse: 7.87762\n",
            "[5200]\tvalid_0's rmse: 7.87746\n",
            "[5300]\tvalid_0's rmse: 7.87729\n",
            "[5400]\tvalid_0's rmse: 7.87712\n",
            "[5500]\tvalid_0's rmse: 7.87692\n",
            "[5600]\tvalid_0's rmse: 7.87669\n",
            "[5700]\tvalid_0's rmse: 7.87656\n",
            "[5800]\tvalid_0's rmse: 7.87643\n",
            "[5900]\tvalid_0's rmse: 7.87629\n",
            "[6000]\tvalid_0's rmse: 7.87625\n",
            "[6100]\tvalid_0's rmse: 7.87608\n",
            "[6200]\tvalid_0's rmse: 7.87593\n",
            "[6300]\tvalid_0's rmse: 7.87583\n",
            "[6400]\tvalid_0's rmse: 7.87571\n",
            "[6500]\tvalid_0's rmse: 7.87563\n",
            "[6600]\tvalid_0's rmse: 7.87559\n",
            "[6700]\tvalid_0's rmse: 7.8755\n",
            "[6800]\tvalid_0's rmse: 7.8754\n",
            "[6900]\tvalid_0's rmse: 7.87536\n",
            "[7000]\tvalid_0's rmse: 7.8753\n",
            "[7100]\tvalid_0's rmse: 7.87518\n",
            "[7200]\tvalid_0's rmse: 7.87515\n",
            "[7300]\tvalid_0's rmse: 7.87501\n",
            "[7400]\tvalid_0's rmse: 7.87492\n",
            "[7500]\tvalid_0's rmse: 7.87489\n",
            "[7600]\tvalid_0's rmse: 7.87479\n",
            "[7700]\tvalid_0's rmse: 7.87468\n",
            "[7800]\tvalid_0's rmse: 7.87451\n",
            "[7900]\tvalid_0's rmse: 7.8745\n",
            "[8000]\tvalid_0's rmse: 7.87439\n",
            "[8100]\tvalid_0's rmse: 7.87426\n",
            "[8200]\tvalid_0's rmse: 7.87424\n",
            "[8300]\tvalid_0's rmse: 7.87419\n",
            "[8400]\tvalid_0's rmse: 7.87407\n",
            "[8500]\tvalid_0's rmse: 7.87403\n",
            "[8600]\tvalid_0's rmse: 7.87396\n",
            "[8700]\tvalid_0's rmse: 7.87387\n",
            "[8800]\tvalid_0's rmse: 7.87378\n",
            "[8900]\tvalid_0's rmse: 7.87368\n",
            "[9000]\tvalid_0's rmse: 7.87369\n",
            "[9100]\tvalid_0's rmse: 7.87361\n",
            "[9200]\tvalid_0's rmse: 7.87357\n",
            "[9300]\tvalid_0's rmse: 7.87352\n",
            "[9400]\tvalid_0's rmse: 7.87345\n",
            "[9500]\tvalid_0's rmse: 7.87342\n",
            "[9600]\tvalid_0's rmse: 7.87338\n",
            "[9700]\tvalid_0's rmse: 7.8733\n",
            "[9800]\tvalid_0's rmse: 7.8732\n",
            "[9900]\tvalid_0's rmse: 7.87319\n",
            "[10000]\tvalid_0's rmse: 7.87315\n",
            "[10100]\tvalid_0's rmse: 7.87311\n",
            "[10200]\tvalid_0's rmse: 7.87308\n",
            "[10300]\tvalid_0's rmse: 7.87303\n",
            "[10400]\tvalid_0's rmse: 7.87307\n",
            "[10500]\tvalid_0's rmse: 7.87311\n",
            "[10600]\tvalid_0's rmse: 7.8731\n",
            "[10700]\tvalid_0's rmse: 7.87312\n",
            "[10800]\tvalid_0's rmse: 7.87313\n",
            "[10900]\tvalid_0's rmse: 7.87307\n",
            "[11000]\tvalid_0's rmse: 7.87302\n",
            "[11100]\tvalid_0's rmse: 7.873\n",
            "[11200]\tvalid_0's rmse: 7.87293\n",
            "[11300]\tvalid_0's rmse: 7.87293\n",
            "[11400]\tvalid_0's rmse: 7.87289\n",
            "[11500]\tvalid_0's rmse: 7.87286\n",
            "[11600]\tvalid_0's rmse: 7.87281\n",
            "[11700]\tvalid_0's rmse: 7.87283\n",
            "[11800]\tvalid_0's rmse: 7.87281\n",
            "[11900]\tvalid_0's rmse: 7.87278\n",
            "[12000]\tvalid_0's rmse: 7.8727\n",
            "[12100]\tvalid_0's rmse: 7.87269\n",
            "[12200]\tvalid_0's rmse: 7.87265\n",
            "[12300]\tvalid_0's rmse: 7.87264\n",
            "[12400]\tvalid_0's rmse: 7.87259\n",
            "[12500]\tvalid_0's rmse: 7.87251\n",
            "[12600]\tvalid_0's rmse: 7.87243\n",
            "[12700]\tvalid_0's rmse: 7.87237\n",
            "[12800]\tvalid_0's rmse: 7.87229\n",
            "[12900]\tvalid_0's rmse: 7.8723\n",
            "[13000]\tvalid_0's rmse: 7.87224\n",
            "[13100]\tvalid_0's rmse: 7.87227\n",
            "[13200]\tvalid_0's rmse: 7.87228\n",
            "[13300]\tvalid_0's rmse: 7.87222\n",
            "[13400]\tvalid_0's rmse: 7.87223\n",
            "[13500]\tvalid_0's rmse: 7.87215\n",
            "[13600]\tvalid_0's rmse: 7.87215\n",
            "[13700]\tvalid_0's rmse: 7.87208\n",
            "[13800]\tvalid_0's rmse: 7.87209\n",
            "[13900]\tvalid_0's rmse: 7.87209\n",
            "[14000]\tvalid_0's rmse: 7.87206\n",
            "[14100]\tvalid_0's rmse: 7.87211\n",
            "[14200]\tvalid_0's rmse: 7.87206\n",
            "[14300]\tvalid_0's rmse: 7.87196\n",
            "[14400]\tvalid_0's rmse: 7.87194\n",
            "[14500]\tvalid_0's rmse: 7.87197\n",
            "[14600]\tvalid_0's rmse: 7.87191\n",
            "[14700]\tvalid_0's rmse: 7.87184\n",
            "[14800]\tvalid_0's rmse: 7.87178\n",
            "[14900]\tvalid_0's rmse: 7.87177\n",
            "[15000]\tvalid_0's rmse: 7.87172\n",
            "[15100]\tvalid_0's rmse: 7.87172\n",
            "[15200]\tvalid_0's rmse: 7.87164\n",
            "[15300]\tvalid_0's rmse: 7.87165\n",
            "[15400]\tvalid_0's rmse: 7.87163\n",
            "[15500]\tvalid_0's rmse: 7.87162\n",
            "[15600]\tvalid_0's rmse: 7.87152\n",
            "[15700]\tvalid_0's rmse: 7.87151\n",
            "[15800]\tvalid_0's rmse: 7.87153\n",
            "[15900]\tvalid_0's rmse: 7.87151\n",
            "[16000]\tvalid_0's rmse: 7.87146\n",
            "[16100]\tvalid_0's rmse: 7.87147\n",
            "[16200]\tvalid_0's rmse: 7.87147\n",
            "[16300]\tvalid_0's rmse: 7.8714\n",
            "[16400]\tvalid_0's rmse: 7.8714\n",
            "[16500]\tvalid_0's rmse: 7.87138\n",
            "[16600]\tvalid_0's rmse: 7.87131\n",
            "[16700]\tvalid_0's rmse: 7.87133\n",
            "[16800]\tvalid_0's rmse: 7.87129\n",
            "[16900]\tvalid_0's rmse: 7.87125\n",
            "[17000]\tvalid_0's rmse: 7.87123\n",
            "[17100]\tvalid_0's rmse: 7.87117\n",
            "[17200]\tvalid_0's rmse: 7.87119\n",
            "[17300]\tvalid_0's rmse: 7.87118\n",
            "[17400]\tvalid_0's rmse: 7.87113\n",
            "[17500]\tvalid_0's rmse: 7.87113\n",
            "[17600]\tvalid_0's rmse: 7.87106\n",
            "[17700]\tvalid_0's rmse: 7.87108\n",
            "[17800]\tvalid_0's rmse: 7.87107\n",
            "[17900]\tvalid_0's rmse: 7.87105\n",
            "[18000]\tvalid_0's rmse: 7.87105\n",
            "[18100]\tvalid_0's rmse: 7.87097\n",
            "[18200]\tvalid_0's rmse: 7.87094\n",
            "[18300]\tvalid_0's rmse: 7.87094\n",
            "[18400]\tvalid_0's rmse: 7.8709\n",
            "[18500]\tvalid_0's rmse: 7.87087\n",
            "[18600]\tvalid_0's rmse: 7.87085\n",
            "[18700]\tvalid_0's rmse: 7.87081\n",
            "[18800]\tvalid_0's rmse: 7.87084\n",
            "[18900]\tvalid_0's rmse: 7.87088\n",
            "[19000]\tvalid_0's rmse: 7.87086\n",
            "[19100]\tvalid_0's rmse: 7.87089\n",
            "[19200]\tvalid_0's rmse: 7.8709\n",
            "[19300]\tvalid_0's rmse: 7.8709\n",
            "[19400]\tvalid_0's rmse: 7.87093\n",
            "[19500]\tvalid_0's rmse: 7.87095\n",
            "[19600]\tvalid_0's rmse: 7.87085\n",
            "[19700]\tvalid_0's rmse: 7.87087\n",
            "Early stopping, best iteration is:\n",
            "[18705]\tvalid_0's rmse: 7.87079\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046505 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.809740\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.93865\n",
            "[200]\tvalid_0's rmse: 7.92622\n",
            "[300]\tvalid_0's rmse: 7.916\n",
            "[400]\tvalid_0's rmse: 7.90787\n",
            "[500]\tvalid_0's rmse: 7.90132\n",
            "[600]\tvalid_0's rmse: 7.89594\n",
            "[700]\tvalid_0's rmse: 7.89159\n",
            "[800]\tvalid_0's rmse: 7.88794\n",
            "[900]\tvalid_0's rmse: 7.88477\n",
            "[1000]\tvalid_0's rmse: 7.88207\n",
            "[1100]\tvalid_0's rmse: 7.87968\n",
            "[1200]\tvalid_0's rmse: 7.87741\n",
            "[1300]\tvalid_0's rmse: 7.87537\n",
            "[1400]\tvalid_0's rmse: 7.87362\n",
            "[1500]\tvalid_0's rmse: 7.87201\n",
            "[1600]\tvalid_0's rmse: 7.87064\n",
            "[1700]\tvalid_0's rmse: 7.86934\n",
            "[1800]\tvalid_0's rmse: 7.86801\n",
            "[1900]\tvalid_0's rmse: 7.86688\n",
            "[2000]\tvalid_0's rmse: 7.86575\n",
            "[2100]\tvalid_0's rmse: 7.86486\n",
            "[2200]\tvalid_0's rmse: 7.86394\n",
            "[2300]\tvalid_0's rmse: 7.86313\n",
            "[2400]\tvalid_0's rmse: 7.86234\n",
            "[2500]\tvalid_0's rmse: 7.86163\n",
            "[2600]\tvalid_0's rmse: 7.86092\n",
            "[2700]\tvalid_0's rmse: 7.86035\n",
            "[2800]\tvalid_0's rmse: 7.85983\n",
            "[2900]\tvalid_0's rmse: 7.85931\n",
            "[3000]\tvalid_0's rmse: 7.85883\n",
            "[3100]\tvalid_0's rmse: 7.85838\n",
            "[3200]\tvalid_0's rmse: 7.85797\n",
            "[3300]\tvalid_0's rmse: 7.85763\n",
            "[3400]\tvalid_0's rmse: 7.85726\n",
            "[3500]\tvalid_0's rmse: 7.85698\n",
            "[3600]\tvalid_0's rmse: 7.85675\n",
            "[3700]\tvalid_0's rmse: 7.85645\n",
            "[3800]\tvalid_0's rmse: 7.85611\n",
            "[3900]\tvalid_0's rmse: 7.85575\n",
            "[4000]\tvalid_0's rmse: 7.85552\n",
            "[4100]\tvalid_0's rmse: 7.85526\n",
            "[4200]\tvalid_0's rmse: 7.85506\n",
            "[4300]\tvalid_0's rmse: 7.85489\n",
            "[4400]\tvalid_0's rmse: 7.85469\n",
            "[4500]\tvalid_0's rmse: 7.85445\n",
            "[4600]\tvalid_0's rmse: 7.8542\n",
            "[4700]\tvalid_0's rmse: 7.85404\n",
            "[4800]\tvalid_0's rmse: 7.85383\n",
            "[4900]\tvalid_0's rmse: 7.85355\n",
            "[5000]\tvalid_0's rmse: 7.85335\n",
            "[5100]\tvalid_0's rmse: 7.85313\n",
            "[5200]\tvalid_0's rmse: 7.85292\n",
            "[5300]\tvalid_0's rmse: 7.85278\n",
            "[5400]\tvalid_0's rmse: 7.85263\n",
            "[5500]\tvalid_0's rmse: 7.85243\n",
            "[5600]\tvalid_0's rmse: 7.85224\n",
            "[5700]\tvalid_0's rmse: 7.85212\n",
            "[5800]\tvalid_0's rmse: 7.85196\n",
            "[5900]\tvalid_0's rmse: 7.85169\n",
            "[6000]\tvalid_0's rmse: 7.85154\n",
            "[6100]\tvalid_0's rmse: 7.8514\n",
            "[6200]\tvalid_0's rmse: 7.85132\n",
            "[6300]\tvalid_0's rmse: 7.85122\n",
            "[6400]\tvalid_0's rmse: 7.85111\n",
            "[6500]\tvalid_0's rmse: 7.85093\n",
            "[6600]\tvalid_0's rmse: 7.85087\n",
            "[6700]\tvalid_0's rmse: 7.85071\n",
            "[6800]\tvalid_0's rmse: 7.85063\n",
            "[6900]\tvalid_0's rmse: 7.85038\n",
            "[7000]\tvalid_0's rmse: 7.85022\n",
            "[7100]\tvalid_0's rmse: 7.85008\n",
            "[7200]\tvalid_0's rmse: 7.84996\n",
            "[7300]\tvalid_0's rmse: 7.84979\n",
            "[7400]\tvalid_0's rmse: 7.84973\n",
            "[7500]\tvalid_0's rmse: 7.84959\n",
            "[7600]\tvalid_0's rmse: 7.84948\n",
            "[7700]\tvalid_0's rmse: 7.84934\n",
            "[7800]\tvalid_0's rmse: 7.84924\n",
            "[7900]\tvalid_0's rmse: 7.84912\n",
            "[8000]\tvalid_0's rmse: 7.84899\n",
            "[8100]\tvalid_0's rmse: 7.84896\n",
            "[8200]\tvalid_0's rmse: 7.84885\n",
            "[8300]\tvalid_0's rmse: 7.84881\n",
            "[8400]\tvalid_0's rmse: 7.84864\n",
            "[8500]\tvalid_0's rmse: 7.84853\n",
            "[8600]\tvalid_0's rmse: 7.84844\n",
            "[8700]\tvalid_0's rmse: 7.84837\n",
            "[8800]\tvalid_0's rmse: 7.84827\n",
            "[8900]\tvalid_0's rmse: 7.84819\n",
            "[9000]\tvalid_0's rmse: 7.8481\n",
            "[9100]\tvalid_0's rmse: 7.84798\n",
            "[9200]\tvalid_0's rmse: 7.84785\n",
            "[9300]\tvalid_0's rmse: 7.84775\n",
            "[9400]\tvalid_0's rmse: 7.84768\n",
            "[9500]\tvalid_0's rmse: 7.84762\n",
            "[9600]\tvalid_0's rmse: 7.84749\n",
            "[9700]\tvalid_0's rmse: 7.84743\n",
            "[9800]\tvalid_0's rmse: 7.8473\n",
            "[9900]\tvalid_0's rmse: 7.84727\n",
            "[10000]\tvalid_0's rmse: 7.84725\n",
            "[10100]\tvalid_0's rmse: 7.84723\n",
            "[10200]\tvalid_0's rmse: 7.84718\n",
            "[10300]\tvalid_0's rmse: 7.84712\n",
            "[10400]\tvalid_0's rmse: 7.84701\n",
            "[10500]\tvalid_0's rmse: 7.84694\n",
            "[10600]\tvalid_0's rmse: 7.84693\n",
            "[10700]\tvalid_0's rmse: 7.84693\n",
            "[10800]\tvalid_0's rmse: 7.84686\n",
            "[10900]\tvalid_0's rmse: 7.84684\n",
            "[11000]\tvalid_0's rmse: 7.84682\n",
            "[11100]\tvalid_0's rmse: 7.84682\n",
            "[11200]\tvalid_0's rmse: 7.84672\n",
            "[11300]\tvalid_0's rmse: 7.8467\n",
            "[11400]\tvalid_0's rmse: 7.8467\n",
            "[11500]\tvalid_0's rmse: 7.84661\n",
            "[11600]\tvalid_0's rmse: 7.84654\n",
            "[11700]\tvalid_0's rmse: 7.84645\n",
            "[11800]\tvalid_0's rmse: 7.84642\n",
            "[11900]\tvalid_0's rmse: 7.84638\n",
            "[12000]\tvalid_0's rmse: 7.84635\n",
            "[12100]\tvalid_0's rmse: 7.8463\n",
            "[12200]\tvalid_0's rmse: 7.84633\n",
            "[12300]\tvalid_0's rmse: 7.84624\n",
            "[12400]\tvalid_0's rmse: 7.84619\n",
            "[12500]\tvalid_0's rmse: 7.84612\n",
            "[12600]\tvalid_0's rmse: 7.84605\n",
            "[12700]\tvalid_0's rmse: 7.84596\n",
            "[12800]\tvalid_0's rmse: 7.84589\n",
            "[12900]\tvalid_0's rmse: 7.84583\n",
            "[13000]\tvalid_0's rmse: 7.84581\n",
            "[13100]\tvalid_0's rmse: 7.84582\n",
            "[13200]\tvalid_0's rmse: 7.84578\n",
            "[13300]\tvalid_0's rmse: 7.84578\n",
            "[13400]\tvalid_0's rmse: 7.84568\n",
            "[13500]\tvalid_0's rmse: 7.84559\n",
            "[13600]\tvalid_0's rmse: 7.84553\n",
            "[13700]\tvalid_0's rmse: 7.8455\n",
            "[13800]\tvalid_0's rmse: 7.84545\n",
            "[13900]\tvalid_0's rmse: 7.84546\n",
            "[14000]\tvalid_0's rmse: 7.84542\n",
            "[14100]\tvalid_0's rmse: 7.84541\n",
            "[14200]\tvalid_0's rmse: 7.8454\n",
            "[14300]\tvalid_0's rmse: 7.84539\n",
            "[14400]\tvalid_0's rmse: 7.84546\n",
            "[14500]\tvalid_0's rmse: 7.84539\n",
            "[14600]\tvalid_0's rmse: 7.84541\n",
            "[14700]\tvalid_0's rmse: 7.84542\n",
            "[14800]\tvalid_0's rmse: 7.8454\n",
            "[14900]\tvalid_0's rmse: 7.84532\n",
            "[15000]\tvalid_0's rmse: 7.8453\n",
            "[15100]\tvalid_0's rmse: 7.84526\n",
            "[15200]\tvalid_0's rmse: 7.84527\n",
            "[15300]\tvalid_0's rmse: 7.8453\n",
            "[15400]\tvalid_0's rmse: 7.84526\n",
            "[15500]\tvalid_0's rmse: 7.84524\n",
            "[15600]\tvalid_0's rmse: 7.84516\n",
            "[15700]\tvalid_0's rmse: 7.84516\n",
            "[15800]\tvalid_0's rmse: 7.84509\n",
            "[15900]\tvalid_0's rmse: 7.84504\n",
            "[16000]\tvalid_0's rmse: 7.84503\n",
            "[16100]\tvalid_0's rmse: 7.84502\n",
            "[16200]\tvalid_0's rmse: 7.84493\n",
            "[16300]\tvalid_0's rmse: 7.84489\n",
            "[16400]\tvalid_0's rmse: 7.84481\n",
            "[16500]\tvalid_0's rmse: 7.84476\n",
            "[16600]\tvalid_0's rmse: 7.84473\n",
            "[16700]\tvalid_0's rmse: 7.84469\n",
            "[16800]\tvalid_0's rmse: 7.84473\n",
            "[16900]\tvalid_0's rmse: 7.84473\n",
            "[17000]\tvalid_0's rmse: 7.84469\n",
            "[17100]\tvalid_0's rmse: 7.84462\n",
            "[17200]\tvalid_0's rmse: 7.84469\n",
            "[17300]\tvalid_0's rmse: 7.84475\n",
            "[17400]\tvalid_0's rmse: 7.84471\n",
            "[17500]\tvalid_0's rmse: 7.8447\n",
            "[17600]\tvalid_0's rmse: 7.84466\n",
            "[17700]\tvalid_0's rmse: 7.84468\n",
            "[17800]\tvalid_0's rmse: 7.8447\n",
            "[17900]\tvalid_0's rmse: 7.84469\n",
            "[18000]\tvalid_0's rmse: 7.84466\n",
            "Early stopping, best iteration is:\n",
            "[17097]\tvalid_0's rmse: 7.8446\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091340 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.805665\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.91426\n",
            "[200]\tvalid_0's rmse: 7.90237\n",
            "[300]\tvalid_0's rmse: 7.89289\n",
            "[400]\tvalid_0's rmse: 7.88537\n",
            "[500]\tvalid_0's rmse: 7.87932\n",
            "[600]\tvalid_0's rmse: 7.87468\n",
            "[700]\tvalid_0's rmse: 7.87094\n",
            "[800]\tvalid_0's rmse: 7.86762\n",
            "[900]\tvalid_0's rmse: 7.86508\n",
            "[1000]\tvalid_0's rmse: 7.86274\n",
            "[1100]\tvalid_0's rmse: 7.86088\n",
            "[1200]\tvalid_0's rmse: 7.85912\n",
            "[1300]\tvalid_0's rmse: 7.85769\n",
            "[1400]\tvalid_0's rmse: 7.85647\n",
            "[1500]\tvalid_0's rmse: 7.85522\n",
            "[1600]\tvalid_0's rmse: 7.85414\n",
            "[1700]\tvalid_0's rmse: 7.85313\n",
            "[1800]\tvalid_0's rmse: 7.85227\n",
            "[1900]\tvalid_0's rmse: 7.85139\n",
            "[2000]\tvalid_0's rmse: 7.85071\n",
            "[2100]\tvalid_0's rmse: 7.85006\n",
            "[2200]\tvalid_0's rmse: 7.84944\n",
            "[2300]\tvalid_0's rmse: 7.84884\n",
            "[2400]\tvalid_0's rmse: 7.84832\n",
            "[2500]\tvalid_0's rmse: 7.84788\n",
            "[2600]\tvalid_0's rmse: 7.84742\n",
            "[2700]\tvalid_0's rmse: 7.84703\n",
            "[2800]\tvalid_0's rmse: 7.84663\n",
            "[2900]\tvalid_0's rmse: 7.84626\n",
            "[3000]\tvalid_0's rmse: 7.84596\n",
            "[3100]\tvalid_0's rmse: 7.84563\n",
            "[3200]\tvalid_0's rmse: 7.84536\n",
            "[3300]\tvalid_0's rmse: 7.84511\n",
            "[3400]\tvalid_0's rmse: 7.84487\n",
            "[3500]\tvalid_0's rmse: 7.84471\n",
            "[3600]\tvalid_0's rmse: 7.84453\n",
            "[3700]\tvalid_0's rmse: 7.84433\n",
            "[3800]\tvalid_0's rmse: 7.84412\n",
            "[3900]\tvalid_0's rmse: 7.84396\n",
            "[4000]\tvalid_0's rmse: 7.84385\n",
            "[4100]\tvalid_0's rmse: 7.84373\n",
            "[4200]\tvalid_0's rmse: 7.84358\n",
            "[4300]\tvalid_0's rmse: 7.84349\n",
            "[4400]\tvalid_0's rmse: 7.84327\n",
            "[4500]\tvalid_0's rmse: 7.84312\n",
            "[4600]\tvalid_0's rmse: 7.84302\n",
            "[4700]\tvalid_0's rmse: 7.84283\n",
            "[4800]\tvalid_0's rmse: 7.84268\n",
            "[4900]\tvalid_0's rmse: 7.8426\n",
            "[5000]\tvalid_0's rmse: 7.84245\n",
            "[5100]\tvalid_0's rmse: 7.84234\n",
            "[5200]\tvalid_0's rmse: 7.84221\n",
            "[5300]\tvalid_0's rmse: 7.84216\n",
            "[5400]\tvalid_0's rmse: 7.842\n",
            "[5500]\tvalid_0's rmse: 7.84198\n",
            "[5600]\tvalid_0's rmse: 7.8419\n",
            "[5700]\tvalid_0's rmse: 7.84178\n",
            "[5800]\tvalid_0's rmse: 7.84162\n",
            "[5900]\tvalid_0's rmse: 7.84158\n",
            "[6000]\tvalid_0's rmse: 7.84142\n",
            "[6100]\tvalid_0's rmse: 7.84134\n",
            "[6200]\tvalid_0's rmse: 7.84131\n",
            "[6300]\tvalid_0's rmse: 7.84115\n",
            "[6400]\tvalid_0's rmse: 7.84106\n",
            "[6500]\tvalid_0's rmse: 7.84095\n",
            "[6600]\tvalid_0's rmse: 7.84075\n",
            "[6700]\tvalid_0's rmse: 7.84066\n",
            "[6800]\tvalid_0's rmse: 7.84053\n",
            "[6900]\tvalid_0's rmse: 7.84049\n",
            "[7000]\tvalid_0's rmse: 7.84043\n",
            "[7100]\tvalid_0's rmse: 7.84036\n",
            "[7200]\tvalid_0's rmse: 7.84023\n",
            "[7300]\tvalid_0's rmse: 7.84011\n",
            "[7400]\tvalid_0's rmse: 7.84007\n",
            "[7500]\tvalid_0's rmse: 7.84003\n",
            "[7600]\tvalid_0's rmse: 7.83995\n",
            "[7700]\tvalid_0's rmse: 7.83993\n",
            "[7800]\tvalid_0's rmse: 7.83993\n",
            "[7900]\tvalid_0's rmse: 7.83978\n",
            "[8000]\tvalid_0's rmse: 7.83981\n",
            "[8100]\tvalid_0's rmse: 7.83978\n",
            "[8200]\tvalid_0's rmse: 7.83969\n",
            "[8300]\tvalid_0's rmse: 7.8396\n",
            "[8400]\tvalid_0's rmse: 7.83954\n",
            "[8500]\tvalid_0's rmse: 7.83938\n",
            "[8600]\tvalid_0's rmse: 7.8394\n",
            "[8700]\tvalid_0's rmse: 7.8393\n",
            "[8800]\tvalid_0's rmse: 7.83924\n",
            "[8900]\tvalid_0's rmse: 7.83922\n",
            "[9000]\tvalid_0's rmse: 7.83922\n",
            "[9100]\tvalid_0's rmse: 7.83931\n",
            "[9200]\tvalid_0's rmse: 7.83924\n",
            "[9300]\tvalid_0's rmse: 7.83922\n",
            "[9400]\tvalid_0's rmse: 7.83913\n",
            "[9500]\tvalid_0's rmse: 7.83911\n",
            "[9600]\tvalid_0's rmse: 7.83898\n",
            "[9700]\tvalid_0's rmse: 7.83892\n",
            "[9800]\tvalid_0's rmse: 7.83887\n",
            "[9900]\tvalid_0's rmse: 7.83881\n",
            "[10000]\tvalid_0's rmse: 7.83881\n",
            "[10100]\tvalid_0's rmse: 7.83874\n",
            "[10200]\tvalid_0's rmse: 7.8387\n",
            "[10300]\tvalid_0's rmse: 7.83861\n",
            "[10400]\tvalid_0's rmse: 7.83859\n",
            "[10500]\tvalid_0's rmse: 7.83859\n",
            "[10600]\tvalid_0's rmse: 7.83855\n",
            "[10700]\tvalid_0's rmse: 7.8385\n",
            "[10800]\tvalid_0's rmse: 7.8385\n",
            "[10900]\tvalid_0's rmse: 7.83839\n",
            "[11000]\tvalid_0's rmse: 7.83837\n",
            "[11100]\tvalid_0's rmse: 7.83836\n",
            "[11200]\tvalid_0's rmse: 7.83836\n",
            "[11300]\tvalid_0's rmse: 7.8383\n",
            "[11400]\tvalid_0's rmse: 7.83823\n",
            "[11500]\tvalid_0's rmse: 7.83827\n",
            "[11600]\tvalid_0's rmse: 7.83827\n",
            "[11700]\tvalid_0's rmse: 7.83822\n",
            "[11800]\tvalid_0's rmse: 7.83818\n",
            "[11900]\tvalid_0's rmse: 7.83807\n",
            "[12000]\tvalid_0's rmse: 7.83802\n",
            "[12100]\tvalid_0's rmse: 7.8379\n",
            "[12200]\tvalid_0's rmse: 7.83797\n",
            "[12300]\tvalid_0's rmse: 7.83791\n",
            "[12400]\tvalid_0's rmse: 7.83792\n",
            "[12500]\tvalid_0's rmse: 7.83786\n",
            "[12600]\tvalid_0's rmse: 7.83786\n",
            "[12700]\tvalid_0's rmse: 7.83793\n",
            "[12800]\tvalid_0's rmse: 7.83793\n",
            "[12900]\tvalid_0's rmse: 7.83788\n",
            "[13000]\tvalid_0's rmse: 7.83792\n",
            "[13100]\tvalid_0's rmse: 7.83799\n",
            "[13200]\tvalid_0's rmse: 7.83799\n",
            "[13300]\tvalid_0's rmse: 7.83788\n",
            "[13400]\tvalid_0's rmse: 7.83776\n",
            "[13500]\tvalid_0's rmse: 7.83779\n",
            "[13600]\tvalid_0's rmse: 7.83772\n",
            "[13700]\tvalid_0's rmse: 7.83763\n",
            "[13800]\tvalid_0's rmse: 7.83762\n",
            "[13900]\tvalid_0's rmse: 7.83755\n",
            "[14000]\tvalid_0's rmse: 7.8375\n",
            "[14100]\tvalid_0's rmse: 7.83753\n",
            "[14200]\tvalid_0's rmse: 7.83754\n",
            "[14300]\tvalid_0's rmse: 7.83749\n",
            "[14400]\tvalid_0's rmse: 7.8374\n",
            "[14500]\tvalid_0's rmse: 7.83747\n",
            "[14600]\tvalid_0's rmse: 7.83744\n",
            "[14700]\tvalid_0's rmse: 7.83749\n",
            "[14800]\tvalid_0's rmse: 7.8375\n",
            "[14900]\tvalid_0's rmse: 7.83745\n",
            "[15000]\tvalid_0's rmse: 7.83744\n",
            "[15100]\tvalid_0's rmse: 7.83744\n",
            "[15200]\tvalid_0's rmse: 7.83743\n",
            "[15300]\tvalid_0's rmse: 7.83737\n",
            "[15400]\tvalid_0's rmse: 7.83739\n",
            "[15500]\tvalid_0's rmse: 7.83742\n",
            "[15600]\tvalid_0's rmse: 7.83746\n",
            "[15700]\tvalid_0's rmse: 7.83749\n",
            "[15800]\tvalid_0's rmse: 7.83745\n",
            "[15900]\tvalid_0's rmse: 7.83744\n",
            "[16000]\tvalid_0's rmse: 7.83737\n",
            "[16100]\tvalid_0's rmse: 7.83738\n",
            "[16200]\tvalid_0's rmse: 7.83736\n",
            "[16300]\tvalid_0's rmse: 7.83731\n",
            "[16400]\tvalid_0's rmse: 7.83733\n",
            "[16500]\tvalid_0's rmse: 7.83726\n",
            "[16600]\tvalid_0's rmse: 7.8372\n",
            "[16700]\tvalid_0's rmse: 7.83723\n",
            "[16800]\tvalid_0's rmse: 7.83722\n",
            "[16900]\tvalid_0's rmse: 7.83719\n",
            "[17000]\tvalid_0's rmse: 7.83714\n",
            "[17100]\tvalid_0's rmse: 7.8372\n",
            "[17200]\tvalid_0's rmse: 7.83723\n",
            "[17300]\tvalid_0's rmse: 7.83723\n",
            "[17400]\tvalid_0's rmse: 7.83724\n",
            "[17500]\tvalid_0's rmse: 7.83723\n",
            "[17600]\tvalid_0's rmse: 7.83713\n",
            "[17700]\tvalid_0's rmse: 7.83713\n",
            "[17800]\tvalid_0's rmse: 7.83709\n",
            "[17900]\tvalid_0's rmse: 7.83707\n",
            "[18000]\tvalid_0's rmse: 7.83704\n",
            "[18100]\tvalid_0's rmse: 7.83695\n",
            "[18200]\tvalid_0's rmse: 7.83702\n",
            "[18300]\tvalid_0's rmse: 7.83701\n",
            "[18400]\tvalid_0's rmse: 7.83701\n",
            "[18500]\tvalid_0's rmse: 7.83694\n",
            "[18600]\tvalid_0's rmse: 7.83696\n",
            "[18700]\tvalid_0's rmse: 7.83693\n",
            "[18800]\tvalid_0's rmse: 7.83691\n",
            "[18900]\tvalid_0's rmse: 7.83696\n",
            "[19000]\tvalid_0's rmse: 7.83694\n",
            "[19100]\tvalid_0's rmse: 7.83698\n",
            "[19200]\tvalid_0's rmse: 7.83689\n",
            "[19300]\tvalid_0's rmse: 7.83698\n",
            "[19400]\tvalid_0's rmse: 7.83698\n",
            "[19500]\tvalid_0's rmse: 7.83693\n",
            "[19600]\tvalid_0's rmse: 7.83697\n",
            "[19700]\tvalid_0's rmse: 7.83696\n",
            "[19800]\tvalid_0's rmse: 7.83699\n",
            "[19900]\tvalid_0's rmse: 7.83706\n",
            "[20000]\tvalid_0's rmse: 7.83706\n",
            "[20100]\tvalid_0's rmse: 7.83708\n",
            "[20200]\tvalid_0's rmse: 7.83715\n",
            "Early stopping, best iteration is:\n",
            "[19201]\tvalid_0's rmse: 7.83688\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082571 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25755\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 101\n",
            "[LightGBM] [Info] Start training from score 6.814090\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[100]\tvalid_0's rmse: 7.91291\n",
            "[200]\tvalid_0's rmse: 7.90095\n",
            "[300]\tvalid_0's rmse: 7.89132\n",
            "[400]\tvalid_0's rmse: 7.8838\n",
            "[500]\tvalid_0's rmse: 7.87804\n",
            "[600]\tvalid_0's rmse: 7.87347\n",
            "[700]\tvalid_0's rmse: 7.86968\n",
            "[800]\tvalid_0's rmse: 7.86666\n",
            "[900]\tvalid_0's rmse: 7.86409\n",
            "[1000]\tvalid_0's rmse: 7.86192\n",
            "[1100]\tvalid_0's rmse: 7.85995\n",
            "[1200]\tvalid_0's rmse: 7.85819\n",
            "[1300]\tvalid_0's rmse: 7.85688\n",
            "[1400]\tvalid_0's rmse: 7.85548\n",
            "[1500]\tvalid_0's rmse: 7.85425\n",
            "[1600]\tvalid_0's rmse: 7.85315\n",
            "[1700]\tvalid_0's rmse: 7.85217\n",
            "[1800]\tvalid_0's rmse: 7.85108\n",
            "[1900]\tvalid_0's rmse: 7.85009\n",
            "[2000]\tvalid_0's rmse: 7.8493\n",
            "[2100]\tvalid_0's rmse: 7.84852\n",
            "[2200]\tvalid_0's rmse: 7.84781\n",
            "[2300]\tvalid_0's rmse: 7.84701\n",
            "[2400]\tvalid_0's rmse: 7.84635\n",
            "[2500]\tvalid_0's rmse: 7.84577\n",
            "[2600]\tvalid_0's rmse: 7.84531\n",
            "[2700]\tvalid_0's rmse: 7.84484\n",
            "[2800]\tvalid_0's rmse: 7.84451\n",
            "[2900]\tvalid_0's rmse: 7.84407\n",
            "[3000]\tvalid_0's rmse: 7.84366\n",
            "[3100]\tvalid_0's rmse: 7.84335\n",
            "[3200]\tvalid_0's rmse: 7.84302\n",
            "[3300]\tvalid_0's rmse: 7.84278\n",
            "[3400]\tvalid_0's rmse: 7.84242\n",
            "[3500]\tvalid_0's rmse: 7.84219\n",
            "[3600]\tvalid_0's rmse: 7.84199\n",
            "[3700]\tvalid_0's rmse: 7.84177\n",
            "[3800]\tvalid_0's rmse: 7.84159\n",
            "[3900]\tvalid_0's rmse: 7.84135\n",
            "[4000]\tvalid_0's rmse: 7.84114\n",
            "[4100]\tvalid_0's rmse: 7.84087\n",
            "[4200]\tvalid_0's rmse: 7.84078\n",
            "[4300]\tvalid_0's rmse: 7.84069\n",
            "[4400]\tvalid_0's rmse: 7.84055\n",
            "[4500]\tvalid_0's rmse: 7.84031\n",
            "[4600]\tvalid_0's rmse: 7.84018\n",
            "[4700]\tvalid_0's rmse: 7.84008\n",
            "[4800]\tvalid_0's rmse: 7.83997\n",
            "[4900]\tvalid_0's rmse: 7.83971\n",
            "[5000]\tvalid_0's rmse: 7.83966\n",
            "[5100]\tvalid_0's rmse: 7.83944\n",
            "[5200]\tvalid_0's rmse: 7.83928\n",
            "[5300]\tvalid_0's rmse: 7.83911\n",
            "[5400]\tvalid_0's rmse: 7.83899\n",
            "[5500]\tvalid_0's rmse: 7.83882\n",
            "[5600]\tvalid_0's rmse: 7.83865\n",
            "[5700]\tvalid_0's rmse: 7.83851\n",
            "[5800]\tvalid_0's rmse: 7.83847\n",
            "[5900]\tvalid_0's rmse: 7.8383\n",
            "[6000]\tvalid_0's rmse: 7.8382\n",
            "[6100]\tvalid_0's rmse: 7.8381\n",
            "[6200]\tvalid_0's rmse: 7.83803\n",
            "[6300]\tvalid_0's rmse: 7.83793\n",
            "[6400]\tvalid_0's rmse: 7.83776\n",
            "[6500]\tvalid_0's rmse: 7.83767\n",
            "[6600]\tvalid_0's rmse: 7.83756\n",
            "[6700]\tvalid_0's rmse: 7.83743\n",
            "[6800]\tvalid_0's rmse: 7.83729\n",
            "[6900]\tvalid_0's rmse: 7.83711\n",
            "[7000]\tvalid_0's rmse: 7.83695\n",
            "[7100]\tvalid_0's rmse: 7.8369\n",
            "[7200]\tvalid_0's rmse: 7.83683\n",
            "[7300]\tvalid_0's rmse: 7.83674\n",
            "[7400]\tvalid_0's rmse: 7.83658\n",
            "[7500]\tvalid_0's rmse: 7.83645\n",
            "[7600]\tvalid_0's rmse: 7.83631\n",
            "[7700]\tvalid_0's rmse: 7.83624\n",
            "[7800]\tvalid_0's rmse: 7.83616\n",
            "[7900]\tvalid_0's rmse: 7.836\n",
            "[8000]\tvalid_0's rmse: 7.83595\n",
            "[8100]\tvalid_0's rmse: 7.83598\n",
            "[8200]\tvalid_0's rmse: 7.83596\n",
            "[8300]\tvalid_0's rmse: 7.8359\n",
            "[8400]\tvalid_0's rmse: 7.83586\n",
            "[8500]\tvalid_0's rmse: 7.83579\n",
            "[8600]\tvalid_0's rmse: 7.83568\n",
            "[8700]\tvalid_0's rmse: 7.83556\n",
            "[8800]\tvalid_0's rmse: 7.83555\n",
            "[8900]\tvalid_0's rmse: 7.83543\n",
            "[9000]\tvalid_0's rmse: 7.83536\n",
            "[9100]\tvalid_0's rmse: 7.83531\n",
            "[9200]\tvalid_0's rmse: 7.83526\n",
            "[9300]\tvalid_0's rmse: 7.83524\n",
            "[9400]\tvalid_0's rmse: 7.83521\n",
            "[9500]\tvalid_0's rmse: 7.83522\n",
            "[9600]\tvalid_0's rmse: 7.83523\n",
            "[9700]\tvalid_0's rmse: 7.83518\n",
            "[9800]\tvalid_0's rmse: 7.83516\n",
            "[9900]\tvalid_0's rmse: 7.83502\n",
            "[10000]\tvalid_0's rmse: 7.83494\n",
            "[10100]\tvalid_0's rmse: 7.83481\n",
            "[10200]\tvalid_0's rmse: 7.83472\n",
            "[10300]\tvalid_0's rmse: 7.83466\n",
            "[10400]\tvalid_0's rmse: 7.8346\n",
            "[10500]\tvalid_0's rmse: 7.83464\n",
            "[10600]\tvalid_0's rmse: 7.83455\n",
            "[10700]\tvalid_0's rmse: 7.83451\n",
            "[10800]\tvalid_0's rmse: 7.83439\n",
            "[10900]\tvalid_0's rmse: 7.83432\n",
            "[11000]\tvalid_0's rmse: 7.83437\n",
            "[11100]\tvalid_0's rmse: 7.83432\n",
            "[11200]\tvalid_0's rmse: 7.83425\n",
            "[11300]\tvalid_0's rmse: 7.83422\n",
            "[11400]\tvalid_0's rmse: 7.83419\n",
            "[11500]\tvalid_0's rmse: 7.83419\n",
            "[11600]\tvalid_0's rmse: 7.83417\n",
            "[11700]\tvalid_0's rmse: 7.83414\n",
            "[11800]\tvalid_0's rmse: 7.83414\n",
            "[11900]\tvalid_0's rmse: 7.83412\n",
            "[12000]\tvalid_0's rmse: 7.83411\n",
            "[12100]\tvalid_0's rmse: 7.83412\n",
            "[12200]\tvalid_0's rmse: 7.83409\n",
            "[12300]\tvalid_0's rmse: 7.83401\n",
            "[12400]\tvalid_0's rmse: 7.83391\n",
            "[12500]\tvalid_0's rmse: 7.83385\n",
            "[12600]\tvalid_0's rmse: 7.83381\n",
            "[12700]\tvalid_0's rmse: 7.83372\n",
            "[12800]\tvalid_0's rmse: 7.83373\n",
            "[12900]\tvalid_0's rmse: 7.83369\n",
            "[13000]\tvalid_0's rmse: 7.83367\n",
            "[13100]\tvalid_0's rmse: 7.8336\n",
            "[13200]\tvalid_0's rmse: 7.83357\n",
            "[13300]\tvalid_0's rmse: 7.83362\n",
            "[13400]\tvalid_0's rmse: 7.8336\n",
            "[13500]\tvalid_0's rmse: 7.83357\n",
            "[13600]\tvalid_0's rmse: 7.83348\n",
            "[13700]\tvalid_0's rmse: 7.83349\n",
            "[13800]\tvalid_0's rmse: 7.83349\n",
            "[13900]\tvalid_0's rmse: 7.83352\n",
            "[14000]\tvalid_0's rmse: 7.83343\n",
            "[14100]\tvalid_0's rmse: 7.83338\n",
            "[14200]\tvalid_0's rmse: 7.83345\n",
            "[14300]\tvalid_0's rmse: 7.8334\n",
            "[14400]\tvalid_0's rmse: 7.83329\n",
            "[14500]\tvalid_0's rmse: 7.83324\n",
            "[14600]\tvalid_0's rmse: 7.83322\n",
            "[14700]\tvalid_0's rmse: 7.83323\n",
            "[14800]\tvalid_0's rmse: 7.83324\n",
            "[14900]\tvalid_0's rmse: 7.83324\n",
            "[15000]\tvalid_0's rmse: 7.8332\n",
            "[15100]\tvalid_0's rmse: 7.83323\n",
            "[15200]\tvalid_0's rmse: 7.83317\n",
            "[15300]\tvalid_0's rmse: 7.83314\n",
            "[15400]\tvalid_0's rmse: 7.83321\n",
            "[15500]\tvalid_0's rmse: 7.83317\n",
            "[15600]\tvalid_0's rmse: 7.83315\n",
            "[15700]\tvalid_0's rmse: 7.8331\n",
            "[15800]\tvalid_0's rmse: 7.83305\n",
            "[15900]\tvalid_0's rmse: 7.83301\n",
            "[16000]\tvalid_0's rmse: 7.83298\n",
            "[16100]\tvalid_0's rmse: 7.83287\n",
            "[16200]\tvalid_0's rmse: 7.8328\n",
            "[16300]\tvalid_0's rmse: 7.83278\n",
            "[16400]\tvalid_0's rmse: 7.83279\n",
            "[16500]\tvalid_0's rmse: 7.83269\n",
            "[16600]\tvalid_0's rmse: 7.83266\n",
            "[16700]\tvalid_0's rmse: 7.83267\n",
            "[16800]\tvalid_0's rmse: 7.83264\n",
            "[16900]\tvalid_0's rmse: 7.8326\n",
            "[17000]\tvalid_0's rmse: 7.83259\n",
            "[17100]\tvalid_0's rmse: 7.83256\n",
            "[17200]\tvalid_0's rmse: 7.83249\n",
            "[17300]\tvalid_0's rmse: 7.83241\n",
            "[17400]\tvalid_0's rmse: 7.83241\n",
            "[17500]\tvalid_0's rmse: 7.83246\n",
            "[17600]\tvalid_0's rmse: 7.83244\n",
            "[17700]\tvalid_0's rmse: 7.83247\n",
            "[17800]\tvalid_0's rmse: 7.83244\n",
            "[17900]\tvalid_0's rmse: 7.8324\n",
            "[18000]\tvalid_0's rmse: 7.8324\n",
            "[18100]\tvalid_0's rmse: 7.83232\n",
            "[18200]\tvalid_0's rmse: 7.83228\n",
            "[18300]\tvalid_0's rmse: 7.83222\n",
            "[18400]\tvalid_0's rmse: 7.8322\n",
            "[18500]\tvalid_0's rmse: 7.8322\n",
            "[18600]\tvalid_0's rmse: 7.83222\n",
            "[18700]\tvalid_0's rmse: 7.8322\n",
            "[18800]\tvalid_0's rmse: 7.83217\n",
            "[18900]\tvalid_0's rmse: 7.83213\n",
            "[19000]\tvalid_0's rmse: 7.83212\n",
            "[19100]\tvalid_0's rmse: 7.83212\n",
            "[19200]\tvalid_0's rmse: 7.83213\n",
            "[19300]\tvalid_0's rmse: 7.83213\n",
            "[19400]\tvalid_0's rmse: 7.8321\n",
            "[19500]\tvalid_0's rmse: 7.83216\n",
            "[19600]\tvalid_0's rmse: 7.83225\n",
            "[19700]\tvalid_0's rmse: 7.83223\n",
            "[19800]\tvalid_0's rmse: 7.83224\n",
            "[19900]\tvalid_0's rmse: 7.83221\n",
            "[20000]\tvalid_0's rmse: 7.83223\n",
            "[20100]\tvalid_0's rmse: 7.83227\n",
            "[20200]\tvalid_0's rmse: 7.83222\n",
            "[20300]\tvalid_0's rmse: 7.83226\n",
            "[20400]\tvalid_0's rmse: 7.83229\n",
            "Early stopping, best iteration is:\n",
            "[19420]\tvalid_0's rmse: 7.83209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GR3URbtFEt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53296fd-1b69-4dc3-e4d1-2aeda39ba381"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "for model1 in models:\n",
        "  print(mean_squared_error(y_test,model1.predict(X_test_lgb))**0.5)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.224783318066234\n",
            "7.240225593647312\n",
            "7.293873071695133\n",
            "7.2268603340133835\n",
            "7.221476197912943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znuxvA3bFNd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "5e31032a-bc40-48b4-f6f3-6e32859ed2c2"
      },
      "source": [
        "test_final=pd.read_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\test.csv')\n",
        "id_index=test_final['id']\n",
        "test_final.drop('id',inplace=True,axis=1)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-75-6afea43476f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mid_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdependent_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdependent_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\users\\venka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         X = self._validate_data(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[0;32m    435\u001b[0m                                 force_all_finite=\"allow-nan\", reset=False)\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\venka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ensure_2d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\venka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
            "\u001b[1;31mValueError\u001b[0m: X has 100 features, but MinMaxScaler is expecting 1 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIJ-gTplInFK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "f0d8f504-c3ab-45f4-e1e9-f787c2cf4735"
      },
      "source": [
        "pred_dataset"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              f0        f1        f2        f3        f4        f5        f6  \\\n",
              "0       0.772720  0.110345  0.376834  0.184523  0.173535  0.499419  0.064781   \n",
              "1       0.227467  0.510345  0.418600  0.721692  0.168458  0.166490  0.390414   \n",
              "2       0.866474  0.124138  0.443734  0.753222  0.430279  0.513110  0.317988   \n",
              "3       0.815056  0.124138  0.415874  0.617083  0.332129  0.158931  0.113397   \n",
              "4       0.335133  0.365517  0.480299  0.520721  0.231542  0.199143  0.043051   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "149995  0.718738  0.324138  0.484727  0.117018  0.638746  0.517924  0.165717   \n",
              "149996  0.704383  0.200000  0.481585  0.702127  0.173504  0.365400  0.066795   \n",
              "149997  0.426323  0.548276  0.426724  0.202462  0.193775  0.362679  0.265731   \n",
              "149998  0.957795  0.117241  0.467765  0.170763  0.207358  0.307747  0.307419   \n",
              "149999  0.279176  0.227586  0.466495  0.288108  0.217450  0.627955  0.396323   \n",
              "\n",
              "              f7        f8        f9  ...       f91       f92       f93  \\\n",
              "0       0.623703  0.549884  0.231115  ...  0.183102  0.031116  0.305497   \n",
              "1       0.607138  0.558683  0.209289  ...  0.343843  0.299171  0.334076   \n",
              "2       0.507733  0.558186  0.280292  ...  0.237500  0.021511  0.279352   \n",
              "3       0.605291  0.519431  0.343690  ...  0.333960  0.224251  0.253703   \n",
              "4       0.645294  0.527027  0.300654  ...  0.229583  0.574476  0.284815   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "149995  0.468975  0.434149  0.263514  ...  0.191246  0.214853  0.428160   \n",
              "149996  0.587191  0.504573  0.230486  ...  0.294597  0.153844  0.267596   \n",
              "149997  0.615467  0.476152  0.604092  ...  0.351704  0.206957  0.385754   \n",
              "149998  0.450003  0.526416  0.375948  ...  0.226907  0.509885  0.269716   \n",
              "149999  0.633061  0.528837  0.378993  ...  0.369813  0.249765  0.298401   \n",
              "\n",
              "             f94       f95       f96       f97       f98       f99     extra  \n",
              "0       0.775579  0.320757  0.458946  0.476499  0.395735  0.276237  0.484618  \n",
              "1       0.570809  0.431019  0.452454  0.221138  0.293963  0.362470  0.286629  \n",
              "2       0.365115  0.629283  0.442132  0.842200  0.580560  0.283877  0.532077  \n",
              "3       0.590433  0.377182  0.501562  0.807252  0.390389  0.163238  0.413293  \n",
              "4       0.458479  0.753800  0.568627  0.324205  0.424113  0.130487  0.501549  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "149995  0.395748  0.327021  0.074967  0.320961  0.405083  0.269552  0.604612  \n",
              "149996  0.882559  0.369859  0.506035  0.507415  0.490268  0.384148  0.465247  \n",
              "149997  0.639840  0.694837  0.527072  0.257226  0.577989  0.378089  0.407385  \n",
              "149998  0.458858  0.478258  0.547498  0.407763  0.447528  0.918093  0.380684  \n",
              "149999  0.313268  0.540254  0.500405  0.347515  0.429655  0.187573  0.405743  \n",
              "\n",
              "[150000 rows x 101 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>extra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.772720</td>\n",
              "      <td>0.110345</td>\n",
              "      <td>0.376834</td>\n",
              "      <td>0.184523</td>\n",
              "      <td>0.173535</td>\n",
              "      <td>0.499419</td>\n",
              "      <td>0.064781</td>\n",
              "      <td>0.623703</td>\n",
              "      <td>0.549884</td>\n",
              "      <td>0.231115</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183102</td>\n",
              "      <td>0.031116</td>\n",
              "      <td>0.305497</td>\n",
              "      <td>0.775579</td>\n",
              "      <td>0.320757</td>\n",
              "      <td>0.458946</td>\n",
              "      <td>0.476499</td>\n",
              "      <td>0.395735</td>\n",
              "      <td>0.276237</td>\n",
              "      <td>0.484618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.227467</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.418600</td>\n",
              "      <td>0.721692</td>\n",
              "      <td>0.168458</td>\n",
              "      <td>0.166490</td>\n",
              "      <td>0.390414</td>\n",
              "      <td>0.607138</td>\n",
              "      <td>0.558683</td>\n",
              "      <td>0.209289</td>\n",
              "      <td>...</td>\n",
              "      <td>0.343843</td>\n",
              "      <td>0.299171</td>\n",
              "      <td>0.334076</td>\n",
              "      <td>0.570809</td>\n",
              "      <td>0.431019</td>\n",
              "      <td>0.452454</td>\n",
              "      <td>0.221138</td>\n",
              "      <td>0.293963</td>\n",
              "      <td>0.362470</td>\n",
              "      <td>0.286629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.866474</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.443734</td>\n",
              "      <td>0.753222</td>\n",
              "      <td>0.430279</td>\n",
              "      <td>0.513110</td>\n",
              "      <td>0.317988</td>\n",
              "      <td>0.507733</td>\n",
              "      <td>0.558186</td>\n",
              "      <td>0.280292</td>\n",
              "      <td>...</td>\n",
              "      <td>0.237500</td>\n",
              "      <td>0.021511</td>\n",
              "      <td>0.279352</td>\n",
              "      <td>0.365115</td>\n",
              "      <td>0.629283</td>\n",
              "      <td>0.442132</td>\n",
              "      <td>0.842200</td>\n",
              "      <td>0.580560</td>\n",
              "      <td>0.283877</td>\n",
              "      <td>0.532077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.815056</td>\n",
              "      <td>0.124138</td>\n",
              "      <td>0.415874</td>\n",
              "      <td>0.617083</td>\n",
              "      <td>0.332129</td>\n",
              "      <td>0.158931</td>\n",
              "      <td>0.113397</td>\n",
              "      <td>0.605291</td>\n",
              "      <td>0.519431</td>\n",
              "      <td>0.343690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.333960</td>\n",
              "      <td>0.224251</td>\n",
              "      <td>0.253703</td>\n",
              "      <td>0.590433</td>\n",
              "      <td>0.377182</td>\n",
              "      <td>0.501562</td>\n",
              "      <td>0.807252</td>\n",
              "      <td>0.390389</td>\n",
              "      <td>0.163238</td>\n",
              "      <td>0.413293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.335133</td>\n",
              "      <td>0.365517</td>\n",
              "      <td>0.480299</td>\n",
              "      <td>0.520721</td>\n",
              "      <td>0.231542</td>\n",
              "      <td>0.199143</td>\n",
              "      <td>0.043051</td>\n",
              "      <td>0.645294</td>\n",
              "      <td>0.527027</td>\n",
              "      <td>0.300654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229583</td>\n",
              "      <td>0.574476</td>\n",
              "      <td>0.284815</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.753800</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.324205</td>\n",
              "      <td>0.424113</td>\n",
              "      <td>0.130487</td>\n",
              "      <td>0.501549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>0.718738</td>\n",
              "      <td>0.324138</td>\n",
              "      <td>0.484727</td>\n",
              "      <td>0.117018</td>\n",
              "      <td>0.638746</td>\n",
              "      <td>0.517924</td>\n",
              "      <td>0.165717</td>\n",
              "      <td>0.468975</td>\n",
              "      <td>0.434149</td>\n",
              "      <td>0.263514</td>\n",
              "      <td>...</td>\n",
              "      <td>0.191246</td>\n",
              "      <td>0.214853</td>\n",
              "      <td>0.428160</td>\n",
              "      <td>0.395748</td>\n",
              "      <td>0.327021</td>\n",
              "      <td>0.074967</td>\n",
              "      <td>0.320961</td>\n",
              "      <td>0.405083</td>\n",
              "      <td>0.269552</td>\n",
              "      <td>0.604612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>0.704383</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.481585</td>\n",
              "      <td>0.702127</td>\n",
              "      <td>0.173504</td>\n",
              "      <td>0.365400</td>\n",
              "      <td>0.066795</td>\n",
              "      <td>0.587191</td>\n",
              "      <td>0.504573</td>\n",
              "      <td>0.230486</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294597</td>\n",
              "      <td>0.153844</td>\n",
              "      <td>0.267596</td>\n",
              "      <td>0.882559</td>\n",
              "      <td>0.369859</td>\n",
              "      <td>0.506035</td>\n",
              "      <td>0.507415</td>\n",
              "      <td>0.490268</td>\n",
              "      <td>0.384148</td>\n",
              "      <td>0.465247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>0.426323</td>\n",
              "      <td>0.548276</td>\n",
              "      <td>0.426724</td>\n",
              "      <td>0.202462</td>\n",
              "      <td>0.193775</td>\n",
              "      <td>0.362679</td>\n",
              "      <td>0.265731</td>\n",
              "      <td>0.615467</td>\n",
              "      <td>0.476152</td>\n",
              "      <td>0.604092</td>\n",
              "      <td>...</td>\n",
              "      <td>0.351704</td>\n",
              "      <td>0.206957</td>\n",
              "      <td>0.385754</td>\n",
              "      <td>0.639840</td>\n",
              "      <td>0.694837</td>\n",
              "      <td>0.527072</td>\n",
              "      <td>0.257226</td>\n",
              "      <td>0.577989</td>\n",
              "      <td>0.378089</td>\n",
              "      <td>0.407385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>0.957795</td>\n",
              "      <td>0.117241</td>\n",
              "      <td>0.467765</td>\n",
              "      <td>0.170763</td>\n",
              "      <td>0.207358</td>\n",
              "      <td>0.307747</td>\n",
              "      <td>0.307419</td>\n",
              "      <td>0.450003</td>\n",
              "      <td>0.526416</td>\n",
              "      <td>0.375948</td>\n",
              "      <td>...</td>\n",
              "      <td>0.226907</td>\n",
              "      <td>0.509885</td>\n",
              "      <td>0.269716</td>\n",
              "      <td>0.458858</td>\n",
              "      <td>0.478258</td>\n",
              "      <td>0.547498</td>\n",
              "      <td>0.407763</td>\n",
              "      <td>0.447528</td>\n",
              "      <td>0.918093</td>\n",
              "      <td>0.380684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>0.279176</td>\n",
              "      <td>0.227586</td>\n",
              "      <td>0.466495</td>\n",
              "      <td>0.288108</td>\n",
              "      <td>0.217450</td>\n",
              "      <td>0.627955</td>\n",
              "      <td>0.396323</td>\n",
              "      <td>0.633061</td>\n",
              "      <td>0.528837</td>\n",
              "      <td>0.378993</td>\n",
              "      <td>...</td>\n",
              "      <td>0.369813</td>\n",
              "      <td>0.249765</td>\n",
              "      <td>0.298401</td>\n",
              "      <td>0.313268</td>\n",
              "      <td>0.540254</td>\n",
              "      <td>0.500405</td>\n",
              "      <td>0.347515</td>\n",
              "      <td>0.429655</td>\n",
              "      <td>0.187573</td>\n",
              "      <td>0.405743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows × 101 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7HaqMbkDvMR"
      },
      "source": [
        "lpred=[]\n",
        "spl=5\n",
        "for model in models:\n",
        "   lpred.append(model.predict(pred_dataset))\n",
        "pred=lpred[0]\n",
        "for i in range(1,spl):\n",
        "   pred += lpred[i]\n",
        "test_pred = pred /spl"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c9WkjbkD29e"
      },
      "source": [
        "test_pred=pd.DataFrame(test_pred)\n",
        "test_pred.columns=['loss']"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMvy4MEHEB0e"
      },
      "source": [
        "test_pred['id']=id_index"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npK_4IBnEE9-"
      },
      "source": [
        "test_pred=test_pred[['id','loss']]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC96uS-5EJA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc47ce2c-3182-45bb-db80-507b67f12a5d"
      },
      "source": [
        "%%time\n",
        "test_pred.to_csv(r'C:\\AI Practice\\Tabular Playground Series - Aug 2021\\submit.csv',index=False)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 357 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAO9HkRIExSE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}